{
  "class": "Workflow",
  "cwlVersion": "v1.2",
  "doc": "This is the workflow developed for the Purdue CGC seminar series. It takes fastQ gene reads, genome reference, and transcript files with GTF gene annotation to feed into Salmon for pseudoalignment then DESeq2 for differential expression.\n\nThe Salmon workflow infers maximum likelihood estimates of transcript abundances from RNA-Seq data using a process called Quasi-mapping.\n\nQuasi-mapping is a process of assigning reads to transcripts without doing an exact base-to-base alignment. The Salmon tool implements a procedure geared towards knowing the transcript from which a read originates rather than the actual mapping coordinates, since the former is crucial to estimating transcript abundances [1, 2].\n\nThe result is a software running at speeds orders of magnitude faster than other tools which utilize the full likelihood model while obtaining near-optimal probabilistic RNA-seq quantification results [1, 2, 3].\n\nDESeq2 performs differential gene expression analysis by use of negative binomial generalized linear models. It analyzes estimated read counts from several samples, each belonging to one of two or more conditions under study, searching for systematic changes between conditions, as compared to within-condition variability.\n\nThe Bioconductor/R package DESeq2 provides a set of functions for importing data, performing exploratory analysis and finally testing for differential expression. This CWL tool is a wrapper around the script based on the standard workflow for this type of analysis [1].\n\nDESeq2 offers two kinds of hypothesis tests: the Wald test, where we use the estimated standard error of a log2 fold change to test if it is equal to zero, and the likelihood ratio test (LRT). The LRT examines two models for the counts, a full model with a certain number of terms and a reduced model, in which some of the terms of the full model are removed. The test determines if the increased likelihood of the data using the extra terms in the full model is more than expected if those extra terms are truly zero. The LRT is therefore useful for testing multiple terms at once, for example testing 3 or more levels of a factor at once, or all interactions between two variables [2].",
  "label": "Differential Expression - Salmon + DESeq2",
  "$namespaces": {
    "sbg": "https://sevenbridges.com"
  },
  "inputs": [
    {
      "id": "in_transcriptome_or_index",
      "sbg:fileTypes": "FA, FASTA, FA.GZ, FASTA.GZ, TAR",
      "type": "File",
      "label": "Transcript FASTA or Salmon Index",
      "doc": "Transcript FASTA file, or an already generated Salmon index archive.",
      "sbg:x": -957.6114501953125,
      "sbg:y": -610.02099609375
    },
    {
      "id": "in_reads",
      "sbg:fileTypes": "FASTQ, FQ, FASTQ.GZ, FQ.GZ",
      "type": "File[]",
      "label": "FASTQ read files",
      "doc": "List of the FASTQ files with properly set metadata fileds.",
      "sbg:x": -973.0126342773438,
      "sbg:y": -174.49200439453125
    },
    {
      "id": "factor",
      "type": "string",
      "label": "Covariate of interest",
      "doc": "The samples will be grouped according to the chosen variable of interest. This needs to match either a column name in the provided phenotype data CSV file or a metadata key. If the latter is true, then all the input files need to have this metadata field populated.",
      "sbg:exposed": true
    },
    {
      "id": "in_reference_genome",
      "sbg:fileTypes": "FA, FASTA, FA.GZ, FASTA.GZ, TSV",
      "type": "File?",
      "label": "Genome FASTA",
      "doc": "Provide genome FASTA file to generate decoy sequences and combine genome and transcriptome reference used for selective alignment.",
      "sbg:x": -970.4965209960938,
      "sbg:y": -482.0785217285156
    },
    {
      "id": "in_annotation",
      "sbg:fileTypes": "GTF, GTF.GZ",
      "type": "File",
      "label": "GTF annotation",
      "doc": "GTF annotation file used for mapping transcripts to genes.",
      "sbg:x": -979.6732177734375,
      "sbg:y": -354.8807678222656
    },
    {
      "id": "cond1",
      "type": "string?",
      "label": "Factor level - test",
      "doc": "The numerator for the log fold change calculation.",
      "sbg:exposed": true
    },
    {
      "id": "cond2",
      "type": "string?",
      "label": "Factor level - reference",
      "doc": "The denominator for the log fold change calculation.",
      "sbg:exposed": true
    }
  ],
  "outputs": [
    {
      "id": "out_html_report",
      "outputSource": [
        "fastqc_0_11_9/out_html_report"
      ],
      "sbg:fileTypes": "HTML",
      "type": "File[]?",
      "label": "HTML reports",
      "doc": "FastQC reports in HTML format.",
      "sbg:x": -426.5964050292969,
      "sbg:y": 22.2568359375
    },
    {
      "id": "out_zip",
      "outputSource": [
        "fastqc_0_11_9/out_zip"
      ],
      "sbg:fileTypes": "ZIP",
      "type": "File[]?",
      "label": "Report zip",
      "doc": "Zip archive of the report.",
      "sbg:x": -411.010009765625,
      "sbg:y": -103.49200439453125
    },
    {
      "id": "out_quant_sf",
      "outputSource": [
        "salmon_workflow_1_2_0/out_quant_sf"
      ],
      "sbg:fileTypes": "SF",
      "type": "File?",
      "label": "Transcript-level quantification",
      "doc": "Salmon transcript-level quantification file.",
      "sbg:x": -463,
      "sbg:y": -730.001708984375
    },
    {
      "id": "out_quant_log",
      "outputSource": [
        "salmon_workflow_1_2_0/out_quant_log"
      ],
      "sbg:fileTypes": "LOG",
      "type": "File?",
      "label": "Salmon quant log",
      "doc": "Salmon quant log file.",
      "sbg:x": -344,
      "sbg:y": -643
    },
    {
      "id": "out_quant_genes_sf",
      "outputSource": [
        "salmon_workflow_1_2_0/out_quant_genes_sf"
      ],
      "sbg:fileTypes": "SF",
      "type": "File?",
      "label": "Gene-level quantification",
      "doc": "Salmon gene-level quantification file.",
      "sbg:x": -182,
      "sbg:y": -592
    },
    {
      "id": "out_quant_archive",
      "outputSource": [
        "salmon_workflow_1_2_0/out_quant_archive"
      ],
      "sbg:fileTypes": "TAR",
      "type": "File?",
      "label": "Salmon Quant archive",
      "doc": "Salmon quant archive.",
      "sbg:x": -58,
      "sbg:y": -519
    },
    {
      "id": "expression_matrix_tx",
      "outputSource": [
        "salmon_workflow_1_2_0/expression_matrix_tx"
      ],
      "sbg:fileTypes": "TXT",
      "type": "File[]?",
      "label": "Expression matrix transcripts",
      "doc": "A matrix of raw counts for transcript expression data, aggregated across all samples.",
      "sbg:x": -39.694252014160156,
      "sbg:y": -402.98175048828125
    },
    {
      "id": "results",
      "outputSource": [
        "deseq2_1_26_0/results"
      ],
      "sbg:fileTypes": "CSV",
      "type": "File?",
      "label": "DESeq2 analysis results.",
      "doc": "Output CSV file.",
      "sbg:x": 319,
      "sbg:y": -470
    },
    {
      "id": "rdata",
      "outputSource": [
        "deseq2_1_26_0/rdata"
      ],
      "sbg:fileTypes": "RDATA",
      "type": "File[]?",
      "label": "RData file",
      "doc": "Workspace image.",
      "sbg:x": 347.98870849609375,
      "sbg:y": -348.4939880371094
    },
    {
      "id": "pheno_out",
      "outputSource": [
        "deseq2_1_26_0/pheno_out"
      ],
      "type": "File?",
      "sbg:x": 377.98870849609375,
      "sbg:y": -213.49398803710938
    },
    {
      "id": "normalized_counts",
      "outputSource": [
        "deseq2_1_26_0/normalized_counts"
      ],
      "sbg:fileTypes": "TXT",
      "type": "File?",
      "label": "Normalized counts",
      "doc": "Counts normalized using estimated sample-specific normalization factors.",
      "sbg:x": 406.98870849609375,
      "sbg:y": -72.49398803710938
    },
    {
      "id": "html_report",
      "outputSource": [
        "deseq2_1_26_0/html_report"
      ],
      "sbg:fileTypes": "HTML",
      "type": "File?",
      "label": "HTML report",
      "doc": "HTML report.",
      "sbg:x": 395.98870849609375,
      "sbg:y": 94.5163803100586
    },
    {
      "id": "expression_matrix_genes",
      "outputSource": [
        "salmon_workflow_1_2_0/expression_matrix_genes"
      ],
      "sbg:fileTypes": "TXT",
      "type": "File[]?",
      "label": "Expression matrix genes",
      "doc": "A matrix of raw counts for gene expression data, aggregated across all samples.",
      "sbg:x": -34.19074249267578,
      "sbg:y": -290.1150207519531
    }
  ],
  "steps": [
    {
      "id": "salmon_workflow_1_2_0",
      "in": [
        {
          "id": "in_reads",
          "source": [
            "in_reads"
          ]
        },
        {
          "id": "in_transcriptome_or_index",
          "source": "in_transcriptome_or_index"
        },
        {
          "id": "in_reference_genome",
          "source": "in_reference_genome"
        },
        {
          "id": "in_annotation",
          "source": "in_annotation"
        }
      ],
      "out": [
        {
          "id": "out_quant_sf"
        },
        {
          "id": "out_quant_genes_sf"
        },
        {
          "id": "out_quant_archive"
        },
        {
          "id": "out_quant_log"
        },
        {
          "id": "expression_matrix_tx"
        },
        {
          "id": "expression_matrix_genes"
        }
      ],
      "run": {
        "class": "Workflow",
        "cwlVersion": "v1.0",
        "id": "admin/sbg-public-data/salmon-workflow-1-2-0/2",
        "doc": "The **Salmon Workflow** estimates transcript abundances from RNA-Seq data using an improved mapping algorithm named **Selective Alignment (SA)**. SA is designed to remain fast as quasi-mapping while simultaneously eliminating many of its mapping errors. It relies upon alignment scoring to help differentiate between mapping loci that would otherwise be indistinguishable due to the multiple exact matches along the reference. Also, the improved mapping algorithm addresses the failure of direct alignment against the transcriptome, compared to spliced alignment to the genome by identifying and extracting sequence-similar decoy regions or using the entire genome as a decoy. The Salmon index is then augmented with these decoy sequences, which are handled in a special manner during mapping and alignment scoring, leading to a reduction of false mappings [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\n- The workflow consists of three steps: **Salmon Index**, **Salmon Quant**, and **SBG Create Expression Matrix**.\n- **FASTQ read files** is the required input port that accepts raw sequencing reads. \n- **Transcript FASTA or Salmon Index** is also required and accepts a transcriptome reference file or a pre-built salmon index file.\n- **Genome FASTA** accepts the reference genome file used for extracting decoy sequences.\n- **Gene annotation** file to be used for creating transcripts to genes mapping file required for gene-level aggregation of quantification results.\n- The workflow will generate transcript abundance estimates in plaintext format (**Transcript-level quantifications**), and an optional file containing **Gene-level quantifications** if the **Gene annotation** input is provided. \n- In addition to the quantification outputs, additional outputs can be produced if the proper options are set. These files will be accessible in the TAR archive on the **Salmon Quant archive** output port. \n- A **Transcript expression matrix** and a **Gene expression matrix** will be generated if more than one sample is provided.\n- The workflow is optimized to run in scatter mode. To run it successfully, supply multiple samples (paired-end or single-end, with adequately filled out **Sample ID** and **Paired End** metadata). \n- The **Salmon Quant archive** output can be used for downstream differential expression analysis tools, like DESeq2. \n\n### Changes Introduced by Seven Bridges\n\n- All output files produced with **Salmon Quant - Reads** will be prefixed by the input sample ID (inferred from the **Sample ID** metadata if existent, or from filename otherwise), instead of having identical names between runs.\n\n### Common Issues and Important Notes\n\n- For paired-end read files, it is important to correctly set the **Paired End** metadata field on your read files.\n- The input FASTA file (if provided instead of the already generated Salmon index archive) should be a transcriptome FASTA, not a genomic FASTA.\n- For FASTQ reads in multi-file format (i.e., two FASTQ files for paired-end 1 and two FASTQ files for paired-end 2), proper metadata needs to be set (the following hierarchy is valid: **Sample ID/Library ID/Platform Unit ID/File Segment Number)**.\n- The GTF and FASTA files need to have compatible transcript IDs.\n- If the location of the project where the workflow is running is set to Google US West, to run the task successfully, please reduce the default number of CPUs by setting the **Number of CPUs** parameter of **SBG Pair FASTQs** to 32.\n\n### Performance Benchmarking\n\nThe main advantage of the Salmon software is that it is not computationally challenging, as alignment in the traditional sense is not performed. Therefore, it is optimized to be run in scatter mode, so a c4.8xlarge instance (AWS) is used by default. \nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes:\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost |  Instance (AWS) |\n|:---------------:|:-----------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     RNA-Seq     |  4 x 4.5 GB |     Yes    |     20M     |     101     |   18min   | $0.44| c4.8xlarge |\n|     RNA-Seq     | 2 x 17.4 GB, 2 x 19 GB |     Yes    |     76M & 84M    |     101     |   46min  | $1.20 | c4.8xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform, visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\n# Enter api credentials\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n\n# Get file names from files in your project. The file names below are just as an example.\ninputs = {\n        'in_reads': list(api.files.query(project=project_id, names=['sample_pe1.fq', 'sample_pe2.fq'])),\n        'in_annotation': list(api.files.query(project=project_id, names=['gtf_file.gtf'])),\n        'in_transcriptome_or_index': list(api.files.query(project=project_id, names=['transcriptome_fasta_file.fa']))\n        }\n\n# Run the task\ntask = api.tasks.create(name='Salmon 1.0.0 workflow - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients, please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [Salmon paper](https://www.biorxiv.org/content/10.1101/657874v2)   \n[2] [Towards selective-alignment](https://www.biorxiv.org/content/10.1101/138800v2)",
        "label": "Salmon workflow 1.2.0",
        "$namespaces": {
          "sbg": "https://sevenbridges.com"
        },
        "inputs": [
          {
            "id": "in_reads",
            "sbg:fileTypes": "FASTQ, FQ, FASTQ.GZ, FQ.GZ",
            "type": "File[]",
            "label": "FASTQ read files",
            "doc": "List of the FASTQ files with properly set metadata fileds.",
            "sbg:y": 320.3984375,
            "sbg:x": 0
          },
          {
            "id": "in_transcriptome_or_index",
            "sbg:fileTypes": "FA, FASTA, FA.GZ, FASTA.GZ, TAR",
            "type": "File",
            "label": "Transcript FASTA or Salmon Index",
            "doc": "Transcript FASTA file, or an already generated Salmon index archive.",
            "sbg:y": 106.83595275878906,
            "sbg:x": 0
          },
          {
            "id": "in_reference_genome",
            "sbg:fileTypes": "FA, FASTA, FA.GZ, FASTA.GZ, TSV",
            "type": "File?",
            "label": "Genome FASTA",
            "doc": "Provide genome FASTA file to generate decoy sequences and combine genome and transcriptome reference used for selective alignment.",
            "sbg:y": 213.61721801757812,
            "sbg:x": 0
          },
          {
            "id": "in_annotation",
            "sbg:fileTypes": "GTF, GTF.GZ",
            "type": "File?",
            "label": "GTF annotation",
            "doc": "GTF annotation file used for mapping transcripts to genes.",
            "sbg:y": 427.1796875,
            "sbg:x": 0
          },
          {
            "id": "max_number_of_parallel_jobs",
            "type": "int?",
            "label": "Maximum number of parallel jobs",
            "doc": "Maximum number of parallel jobs to allow in the tool downstream of this one.",
            "sbg:exposed": true
          },
          {
            "id": "number_of_cpus",
            "type": "int?",
            "label": "Number of CPUs",
            "doc": "Number of CPUs available in the workflow that uses this tool. This number will be used to determine the optimal number of threads to use in the tool downstream of this one.",
            "sbg:exposed": true
          },
          {
            "id": "alternative_init_mode",
            "type": "boolean?",
            "label": "Alternative initialization mode",
            "doc": "Use an alternative strategy (rather than simple interpolation) between the online and uniform abundance estimates to initialize the EM/VBEM algorithm.",
            "sbg:exposed": true
          },
          {
            "id": "bias_speed_samp",
            "type": "int?",
            "label": "Bias speed sample",
            "doc": "The value at which the fragment length PMF is down-sampled when evaluating sequence-specific and & GC fragment bias. Larger values speed up effective length correction, but may decrease the fidelity of bias modeling results.",
            "sbg:exposed": true
          },
          {
            "id": "bootstrap_reproject",
            "type": "boolean?",
            "label": "Bootstrap reproject",
            "doc": "This switch will learn the parameter distribution from the bootstrapped counts for each sample but will reproject those parameters onto the original equivalence class counts.",
            "sbg:exposed": true
          },
          {
            "id": "consensus_slack",
            "type": "int?",
            "label": "Consensus slack",
            "doc": "The amount of slack allowed in the quasi-mapping consensus mechanism.  Normally, a transcript must cover all hits to be considered for mapping.  If this is set to a fraction, X, greater than 0 (and in [0,1)), then a transcript can fail to cover up to (100 * X)% of the hits before it is discounted as a mapping candidate.  The default value of this option is 0.2 if --validateMappings is given and 0 otherwise.",
            "sbg:exposed": true
          },
          {
            "id": "discard_orphans_quasi",
            "type": "boolean?",
            "label": "Discard orphans in Quasi-mapping mode",
            "doc": "Discard orphans mapping in quasi-mapping mode. If this flag is passed then only paired mappings will be considered towards quantification estimates. The default behaviour is to consider orphan mappings if no valid paired mappings exist. This flag is independent of the option to write the oprhaned mappings to file (--writeOprhanLinks).",
            "sbg:exposed": true
          },
          {
            "id": "dump_eq",
            "type": "boolean?",
            "label": "Dump equivalence class counts",
            "doc": "Dump conditional probabilities associated with transcripts when  equivalence class information is being dumped to file. Note, this will  dump the factorization that is actually used by salmon's offline phase  for inference.  If you are using range-factorized equivalence classes  (the default) then the same transcript set may appear multiple times  with different associated conditional probabilities.",
            "sbg:exposed": true
          },
          {
            "id": "dump_eq_weights",
            "type": "boolean?",
            "label": "Dump equivalence class weights",
            "doc": "Includes 'rich' equivalence class weights in the output when equivalence class information is being dumpes to file.",
            "sbg:exposed": true
          },
          {
            "id": "fld_max",
            "type": "int?",
            "label": "Maximum fragment length",
            "doc": "The maximum fragment length to consider when building the empirical distribution.",
            "sbg:exposed": true
          },
          {
            "id": "fld_mean",
            "type": "int?",
            "label": "Mean fragment length",
            "doc": "The mean used in the fragment lenght distribution prior.",
            "sbg:exposed": true
          },
          {
            "id": "fld_sd",
            "type": "int?",
            "label": "Fragment length standard deviation",
            "doc": "The standard deviation used in the fragment length distribution prior.",
            "sbg:exposed": true
          },
          {
            "id": "forgetting_factor",
            "type": "float?",
            "label": "Forgetting factor",
            "doc": "The forgetting factor used in the online learning schedule. A smaller value results in quicker learning, but higher variance and may be unstable. A larger value results in slower learning but may be more stable. The input value should be in the interva (0.5, 1.0].",
            "sbg:exposed": true
          },
          {
            "id": "gap_extension",
            "type": "int?",
            "label": "The value given to a gap extension in an alignment",
            "doc": "The value given to a gap extension in an alignment.",
            "sbg:exposed": true
          },
          {
            "id": "gap_opening",
            "type": "int?",
            "label": "The value given to a gap opening in an alignment",
            "doc": "The value given to a gap opening in an alignment.",
            "sbg:exposed": true
          },
          {
            "id": "gc_bias",
            "type": "boolean?",
            "label": "GC bias correction",
            "doc": "[Biasl] Perform fragment GC bias correction.",
            "sbg:exposed": true
          },
          {
            "id": "gc_size_samp",
            "type": "int?",
            "label": "GC size sample",
            "doc": "The value by which to downsample transcripts when representing the GC content. Larger values will reduce memory usage, but may decrease the fidelity of bias modeling results.",
            "sbg:exposed": true
          },
          {
            "id": "incompatible_prior",
            "type": "float?",
            "label": "Incompatible prior probability",
            "doc": "This option sets the prior probability that an alignment that disagrees with the specified library type (--libType) results from the true fragment origin. Setting this to 0 specifies that alignments that disagree with the library type should be \"impossible\", while setting it to 1 says that alignments that disagree with the library type are no less likely than those that do.",
            "sbg:exposed": true
          },
          {
            "id": "init_uniform",
            "type": "boolean?",
            "label": "Initialize uniform parameters",
            "doc": "Initialize the offline inference with uniform parameters, rather than seeding with online parameters.",
            "sbg:exposed": true
          },
          {
            "id": "lib_type",
            "type": "string?",
            "label": "Library type",
            "doc": "Format string describing the library type. As of version 0.7.0, Salmon also has the ability to automatically infer (i.e. guess) the library type based on how the first few thousand reads map to the transcriptome. To allow Salmon to automatically infer the library type, simply provide the letter 'A' as an input to this parameter (also the default behaviour). Otherwise, the input string should be in the format of maximum three uppercase letters ('SSS'). The first part of the library string (relative orientation) is only provided if the library is paired-end. The possible options are: I=inward, O=outword, M=matching. The second part of the read library string specifies whether the protocol is stranded or unstranded; the options are: S=stranded, U=unstranded. If the protocol is unstranded, then we\u2019re done. The final part of the library string specifies the strand from which the read originates in a strand-specific protocol \u2014 it is only provided if the library is stranded (i.e. if the library format string is of the form S). The possible values are: F=read 1 (or single-end read) comes from the forward strand, R=read 1 (or single-end read) comes from the reverse strand. Examples: IU, SF, OSR.",
            "sbg:exposed": true
          },
          {
            "id": "max_read_occ",
            "type": "int?",
            "label": "Maximum read occurence",
            "doc": "Reads mapping to more than this many places won't be considered.",
            "sbg:exposed": true
          },
          {
            "id": "meta",
            "type": "boolean?",
            "label": "Meta",
            "doc": "If you're using Salmon on a metagenomic dataset, consider setting this flag to disable parts of the abundance estimation model that make less sense for metagenomic data.",
            "sbg:exposed": true
          },
          {
            "id": "min_assigned_frags",
            "type": "int?",
            "label": "Minimum assigned fragments",
            "doc": "The minimum number of fragments that must be assigned to the transcriptome.",
            "sbg:exposed": true
          },
          {
            "id": "min_core_fraction",
            "type": "float?",
            "label": "Minimum score fraction",
            "doc": "The fraction of the optimal possible alignment score that a mapping must achieve in order to be considered \"valid\", should be in the range [0,1].",
            "sbg:exposed": true
          },
          {
            "id": "no_bias_length_threshold",
            "type": "boolean?",
            "label": "No bias length threshold",
            "doc": "[Experimental] If this option is enabled, then no (lower) threshold will be set on how short bias correction can make effective lengths. This can increase precision of bias correction, but harm robustness. The default correction applies a threshold.",
            "sbg:exposed": true
          },
          {
            "id": "no_effective_length_correction",
            "type": "boolean?",
            "label": "No effective length correction",
            "doc": "Disables effective lenght correction when computing the probability that a fragment was generated from a transcript. If this flag is passed in, the fragment lenght distribution is not taken into account when computing this probability.",
            "sbg:exposed": true
          },
          {
            "id": "no_fragment_length_distribution",
            "type": "boolean?",
            "label": "No fragment length distribution",
            "doc": "[Experimental] Do not consider concordance with the learned fragment lenght distribution when trying to determine the probability that a fragment has originated from a specific location. Normally, fragments with unlikely lengths will be assigned a smaller relative probability than those with more likely lenghts. When this flag is passed in, the observed fragment length has no effect on that fragment's a priori probability.",
            "sbg:exposed": true
          },
          {
            "id": "no_gamma_draw",
            "type": "boolean?",
            "label": "No gamma draw",
            "doc": "This switch will disable drawing transcript fractions from a Gamma distribution during Gibbs sampling.  In this case, the sampler does not account for shot-noise, but only assignment ambiguity.",
            "sbg:exposed": true
          },
          {
            "id": "no_length_correction",
            "type": "boolean?",
            "label": "No length correction",
            "doc": "Entirely disables length correction when estimating abundance of transcripts. This option can be used with protocols where one expects that fragments derive from their underlying targets without regard to that target's length, e.g. QuantSeq (EXPERIMENTAL).",
            "sbg:exposed": true
          },
          {
            "id": "num_aux_model_samples",
            "type": "int?",
            "label": "Number of auxiliary model samples",
            "doc": "The first this many numbers are used to train the auxiliary model parameters (e.g. fragment length distribution, bias, etc.). After their first that many observations, the auxiliary model parameters will be assumed to have converged and will be fixed.",
            "sbg:exposed": true
          },
          {
            "id": "num_bias_samples",
            "type": "int?",
            "label": "Number of bias samples",
            "doc": "Number of fragment mappings to use when learning the sequence-specific bias model.",
            "sbg:exposed": true
          },
          {
            "id": "num_bootstraps",
            "type": "int?",
            "label": "Number of bootstraps",
            "doc": "The number of bootstrap samples to generate. Note: this is mutually exclusive with Gibbs sampling.",
            "sbg:exposed": true
          },
          {
            "id": "num_gibbs_samples",
            "type": "int?",
            "label": "Number of Gibbs samples",
            "doc": "The number of Gibbs sampling rounds to perform.",
            "sbg:exposed": true
          },
          {
            "id": "num_pre_aux_model_samples",
            "type": "int?",
            "label": "Number of pre auxiliary model samples",
            "doc": "The first this many samples will have their assignment likelihoods and contributions to the transcript abundances computed without applying any auxiliary models. The purpose of ignoring the auxiliary models for the first that many observations is to avoid applying these models before their parameters have been learned sufficiently well.",
            "sbg:exposed": true
          },
          {
            "id": "per_transcript_prior",
            "type": "boolean?",
            "label": "Per transcript prior",
            "doc": "The prior (either the default, or the argument provided via --vbPrior) will be interpreted as a transcript-level prior (i.e. each transcript will be given a prior read count of this value).",
            "sbg:exposed": true
          },
          {
            "id": "pos_bias",
            "type": "boolean?",
            "label": "Position bias",
            "doc": "Perform positional bias correction.",
            "sbg:exposed": true
          },
          {
            "id": "range_factorization_bins",
            "type": "int?",
            "label": "Range factorization bins",
            "doc": "Factorizes the likelihood used in quantification by adopting a new notion  of equivalence classes based on the conditional probabilities with which  fragments are generated from different transcripts.  This is a more  fine-grained factorization than the normal rich equivalence classes.   The default value (4) corresponds to the default used in Zakeri et al.  2017 (doi: 10.1093/bioinformatics/btx262), and larger values imply a more  fine-grained factorization. If range factorization is enabled, a common  value to select for this parameter is 4. A value of 0 signifies the use  of basic rich equivalence classes.",
            "sbg:exposed": true
          },
          {
            "id": "read_ref_match",
            "type": "int?",
            "label": "The value given to a match between read and reference nucleotides in an alignment",
            "doc": "The value given to a match between read and reference nucleotides in an alignment.",
            "sbg:exposed": true
          },
          {
            "id": "read_ref_mismatch",
            "type": "int?",
            "label": "The value given to a mismatch between read and reference nucleotides in an alignment",
            "doc": "The value given to a mismatch between read and reference nucleotides in an alignment.",
            "sbg:exposed": true
          },
          {
            "id": "reduce_GC_memory",
            "type": "boolean?",
            "label": "Reduce GC memory",
            "doc": "If this option is selected, a more memory efficient (but slightly slower) representation is used to compute fragment GC content. Enabling this will reduce memory usage, but can also reduce speed. However, the results themselves will remain the same.",
            "sbg:exposed": true
          },
          {
            "id": "seq_bias",
            "type": "boolean?",
            "label": "Sequence-specific bias correction",
            "doc": "Perform sequence-specific bias correction.",
            "sbg:exposed": true
          },
          {
            "id": "sig_digits",
            "type": "int?",
            "label": "Significant digits",
            "doc": "The number of significant digits to write when outputting the EffectiveLength and NumReads columns.",
            "sbg:exposed": true
          },
          {
            "id": "use_em",
            "type": "boolean?",
            "label": "Use the EM algorithm",
            "doc": "Use the traditional EM algorithm for optimization in the batch passes.",
            "sbg:exposed": true
          },
          {
            "id": "use_vbopt",
            "type": "boolean?",
            "label": "Use Variational Bayesian optimization",
            "doc": "Use the Variational Bayesian EM rather than the traditional EM algorithm for optimization in the batch passes.",
            "sbg:exposed": true
          },
          {
            "id": "vb_prior",
            "type": "float?",
            "label": "VBEM prior",
            "doc": "The prior that will be used in the VBEM algorithm.  This is interpreted  as a per-transcript prior, unless the --perNucleotidePrior flag is also  given. If the --perNucleotidePrior flag is given, this is used as a  nucleotide-level prior.  If the default is used, it will be divided by  1000 before being used as a nucleotide-level prior, i.e. the default  per-nucleotide prior will be 1e-5.",
            "sbg:exposed": true
          },
          {
            "id": "write_mappings",
            "type": "string?",
            "label": "Write Mappings",
            "doc": "If this option is provided, then the quasi-mapping results will be written out in SAM-compatible format. By default, output will be directed to stdout, but an alternative file name can be provided instead.",
            "sbg:exposed": true
          },
          {
            "id": "bandwidth",
            "type": "int?",
            "label": "Bandwidth",
            "doc": "The value used for the bandwidth passed to ksw2.  A smaller bandwidth can make the alignment verification run more quickly, but could possibly miss valid alignments.",
            "sbg:exposed": true
          },
          {
            "id": "allow_dovetail",
            "type": "boolean?",
            "label": "Allow Dovetail",
            "doc": "Allow dovetailing mappings.",
            "sbg:exposed": true
          },
          {
            "id": "recover_orphans",
            "type": "boolean?",
            "label": "Recover Orphans",
            "doc": "Attempt to recover the mates of orphaned reads. This uses edlib for orphan recovery, and so introduces some computational overhead, but it can improve sensitivity.",
            "sbg:exposed": true
          },
          {
            "id": "mimic_bt2",
            "type": "boolean?",
            "label": "Mimic BT2",
            "doc": "Set flags to mimic parameters similar to Bowtie2 with --no-discordant and --no-mixed flags.This increases disallows dovetailing reads, and discards orphans. Note, this does not impose the very strict parameters assumed by RSEM+Bowtie2, like gapless alignments.  For that behavior, use the --mimiStrictBT2 flag below.",
            "sbg:exposed": true
          },
          {
            "id": "mimic_strict_bt2",
            "type": "boolean?",
            "label": "Mimic Strict BT2",
            "doc": "Set flags to mimic the very strict parameters used by RSEM+Bowtie2.  This increases --minScoreFraction to 0.8, disallows dovetailing reads, discards orphans, and disallows gaps in alignments.",
            "sbg:exposed": true
          },
          {
            "id": "hard_filter",
            "type": "boolean?",
            "label": "Hard Filter",
            "doc": "Instead of weighting mappings by their alignment score, this flag will discard any mappings with sub-optimal alignment score.  The default option of soft-filtering (i.e. weighting mappings by their alignment score) usually yields slightly more accurate abundance estimates but this flag may be desirable if you want more accurate 'naive' equivalence classes, rather than range factorized equivalence classes.",
            "sbg:exposed": true
          },
          {
            "id": "skip_quant",
            "type": "boolean?",
            "label": "Skip Quant",
            "doc": "Skip performing the actual transcript quantification (including any Gibbs sampling or bootstrapping).",
            "sbg:exposed": true
          },
          {
            "id": "write_orphan_links",
            "type": "boolean?",
            "label": "Write Orphan Links",
            "doc": "Write the transcripts that are linked by orphaned reads.",
            "sbg:exposed": true
          },
          {
            "id": "write_unmapped_names",
            "type": "boolean?",
            "label": "Write Unmapped Names",
            "doc": "Write the names of un-mapped reads to the file unmapped_names.txt in the auxiliary directory.",
            "sbg:exposed": true
          },
          {
            "id": "thinning_factor",
            "type": "int?",
            "label": "Thinning Factor",
            "doc": "Number of steps to discard for every sample kept from the Gibbs chain. The larger this number, the less chance that subsequent samples are auto-correlated, but the slower sampling becomes.",
            "sbg:exposed": true
          },
          {
            "id": "softclip_overhangs",
            "type": "boolean?",
            "label": "Softclip overhangs",
            "doc": "Allow soft-clipping of reads that overhang the beginning or ends of the transcript.  In this case, the overhaning section of the read will simply be unaligned, and will not contribute or detract from the alignment score. The default policy is to force an end-to-end alignemnt of the entire read, so that overhanings will result in some deletion of nucleotides from the read.",
            "sbg:exposed": true
          },
          {
            "id": "full_length_alignment",
            "type": "boolean?",
            "label": "Full length alignment [selective-alignment mode only]",
            "doc": "Perform selective alignment over the full length of the read, beginning from the (approximate) initial mapping location and using extension alignment. This is in contrast with the default behavior which is to only perform alignment between the MEMs in the optimal chain (and before the first and after the last MEM if applicable). The default strategy forces the MEMs to belong to the alignment, but has the benefit that it can discover indels prior to the first hit shared between the read and reference.",
            "sbg:exposed": true
          },
          {
            "id": "hit_filter_policy",
            "type": [
              "null",
              {
                "type": "enum",
                "symbols": [
                  "BEFORE",
                  "AFTER",
                  "BOTH",
                  "NONE"
                ],
                "name": "hit_filter_policy"
              }
            ],
            "label": "Hit filter policy",
            "doc": "Determines the policy by which hits are filtered in selective alignment. Filtering hits after chaining (the default) is more sensitive, but more computationally intensive, because it performs the chaining dynamic program for all hits.  Filtering before chaining is faster, but some true hits may be missed.  The options are BEFORE, AFTER, BOTH and NONE.",
            "sbg:exposed": true
          },
          {
            "id": "max_occs_per_hit",
            "type": "int?",
            "label": "Maximum occurrences per hit",
            "doc": "When collecting \"hits\" (MEMs), hits having more than maxOccsPerHit occurrences won't be considered.",
            "sbg:exposed": true
          },
          {
            "id": "no_single_frag_prob",
            "type": "boolean?",
            "label": "Skip fragment length estimate for SE reads",
            "doc": "Disables the estimation of an associated fragment length probability for single-end reads or for orphaned mappings in paired-end libraries.  The default behavior is to consider the probability of all possible fragment lengths associated with the retained mapping.  Enabling this flag (i.e. turning this default behavior off) will simply not attempt to estimate a fragment length probability in such cases.",
            "sbg:exposed": true
          },
          {
            "id": "per_nucleotide_prior",
            "type": "boolean?",
            "label": "Nucleotide level prior",
            "doc": "The prior (either the default or the argument provided via --vbPrior) will be interpreted as a nucleotide-level prior (i.e. each nucleotide will be given a prior read count of this value).",
            "sbg:exposed": true
          },
          {
            "id": "column_name",
            "type": [
              "null",
              {
                "type": "array",
                "items": "string",
                "inputBinding": {
                  "separate": true
                }
              }
            ],
            "label": "Column name",
            "doc": "Column name chose to aggregate results over.",
            "sbg:exposed": true
          },
          {
            "id": "output_name",
            "type": "string?",
            "label": "Output file name",
            "doc": "Name of the outputted counts matrix file.",
            "sbg:exposed": true
          },
          {
            "id": "column_name_1",
            "type": [
              "null",
              {
                "type": "array",
                "items": "string",
                "inputBinding": {
                  "separate": true
                }
              }
            ],
            "label": "Column name",
            "doc": "Column name chose to aggregate results over.",
            "sbg:exposed": true
          },
          {
            "id": "output_name_1",
            "type": "string?",
            "label": "Output file name",
            "doc": "Name of the outputted counts matrix file.",
            "sbg:exposed": true
          },
          {
            "id": "kmer_len",
            "type": "int?",
            "label": "K-mer length",
            "doc": "The size of k-mers that should be used for the quasi index. K-mer length should be an odd number.",
            "sbg:exposed": true
          },
          {
            "id": "gencode",
            "type": "boolean?",
            "label": "Gencode FASTA",
            "doc": "This flag will expect the input transcript FASTA to be in GENCODE format and will split the transcript name at the first '|' character. These reduced names will be used in the output and when looking for these transcripts in a gene to transcript GTF.",
            "sbg:exposed": true
          },
          {
            "id": "keep_duplicates",
            "type": "boolean?",
            "label": "Keep duplicates",
            "doc": "This flag will disable the default indexing behavior of discarding sequence-identical duplicate transcripts. If this flag is passed, then duplicate transcripts that appear in the input will be retained and quantified separately.",
            "sbg:exposed": true
          }
        ],
        "outputs": [
          {
            "id": "out_quant_sf",
            "outputSource": [
              "salmon_quant_reads_1_2_0/out_quant_sf"
            ],
            "sbg:fileTypes": "SF",
            "type": "File?",
            "label": "Transcript-level quantification",
            "doc": "Salmon transcript-level quantification file.",
            "description": "Salmon Quant output file, containing quantification results.",
            "sbg:y": 213.5625,
            "sbg:x": 1123.3009033203125
          },
          {
            "id": "out_quant_genes_sf",
            "outputSource": [
              "salmon_quant_reads_1_2_0/out_quant_genes_sf"
            ],
            "sbg:fileTypes": "SF",
            "type": "File?",
            "label": "Gene-level quantification",
            "doc": "Salmon gene-level quantification file.",
            "description": "File containing aggregated gene-level abundance estimates.",
            "sbg:y": 427.2343444824219,
            "sbg:x": 1123.3009033203125
          },
          {
            "id": "out_quant_archive",
            "outputSource": [
              "salmon_quant_reads_1_2_0/out_quant_archive"
            ],
            "sbg:fileTypes": "TAR",
            "type": "File?",
            "label": "Salmon Quant archive",
            "doc": "Salmon quant archive.",
            "description": "All files outputed by Salmon Quant tool. Contains quantification files.",
            "sbg:y": 534.015625,
            "sbg:x": 1123.3009033203125
          },
          {
            "id": "out_quant_log",
            "outputSource": [
              "salmon_quant_reads_1_2_0/out_quant_log"
            ],
            "sbg:fileTypes": "LOG",
            "type": "File?",
            "label": "Salmon quant log",
            "doc": "Salmon quant log file.",
            "description": "Salmon quant log file.",
            "sbg:y": 320.3984375,
            "sbg:x": 1123.3009033203125
          },
          {
            "id": "expression_matrix_tx",
            "outputSource": [
              "sbg_create_rsem_tpm_counts_matrix_tx/expression_matrix"
            ],
            "sbg:fileTypes": "TXT",
            "type": "File[]?",
            "label": "Expression matrix transcripts",
            "doc": "A matrix of raw counts for transcript expression data, aggregated across all samples.",
            "description": "A single file containing expression values across all genes/transcripts for multiple provided inputs.",
            "sbg:y": 213.56251525878906,
            "sbg:x": 1422.816650390625
          },
          {
            "id": "expression_matrix_genes",
            "outputSource": [
              "sbg_create_rsem_tpm_counts_matrix_genes/expression_matrix"
            ],
            "sbg:fileTypes": "TXT",
            "type": "File[]?",
            "label": "Expression matrix genes",
            "doc": "A matrix of raw counts for gene expression data, aggregated across all samples.",
            "description": "A single file containing expression values across all genes/transcripts for multiple provided inputs.",
            "sbg:y": 320.3984375,
            "sbg:x": 1422.816650390625
          }
        ],
        "steps": [
          {
            "id": "salmon_quant_reads_1_2_0",
            "in": [
              {
                "id": "alternative_init_mode",
                "source": "alternative_init_mode"
              },
              {
                "id": "bias_speed_samp",
                "source": "bias_speed_samp"
              },
              {
                "id": "bootstrap_reproject",
                "source": "bootstrap_reproject"
              },
              {
                "id": "consensus_slack",
                "source": "consensus_slack"
              },
              {
                "id": "discard_orphans_quasi",
                "source": "discard_orphans_quasi"
              },
              {
                "id": "dump_eq",
                "source": "dump_eq"
              },
              {
                "id": "dump_eq_weights",
                "source": "dump_eq_weights"
              },
              {
                "id": "fld_max",
                "source": "fld_max"
              },
              {
                "id": "fld_mean",
                "source": "fld_mean"
              },
              {
                "id": "fld_sd",
                "source": "fld_sd"
              },
              {
                "id": "forgetting_factor",
                "source": "forgetting_factor"
              },
              {
                "id": "gap_extension",
                "source": "gap_extension"
              },
              {
                "id": "gap_opening",
                "source": "gap_opening"
              },
              {
                "id": "gc_bias",
                "source": "gc_bias"
              },
              {
                "id": "gc_size_samp",
                "source": "gc_size_samp"
              },
              {
                "id": "in_annotation",
                "source": "salmon_index_1_2_0/out_tx2gene"
              },
              {
                "id": "incompatible_prior",
                "source": "incompatible_prior"
              },
              {
                "id": "init_uniform",
                "source": "init_uniform"
              },
              {
                "id": "lib_type",
                "source": "lib_type"
              },
              {
                "id": "max_read_occ",
                "source": "max_read_occ"
              },
              {
                "id": "meta",
                "source": "meta"
              },
              {
                "id": "min_assigned_frags",
                "source": "min_assigned_frags"
              },
              {
                "id": "min_core_fraction",
                "source": "min_core_fraction"
              },
              {
                "id": "no_bias_length_threshold",
                "source": "no_bias_length_threshold"
              },
              {
                "id": "no_effective_length_correction",
                "source": "no_effective_length_correction"
              },
              {
                "id": "no_fragment_length_distribution",
                "source": "no_fragment_length_distribution"
              },
              {
                "id": "no_gamma_draw",
                "source": "no_gamma_draw"
              },
              {
                "id": "no_length_correction",
                "source": "no_length_correction"
              },
              {
                "id": "num_aux_model_samples",
                "source": "num_aux_model_samples"
              },
              {
                "id": "num_bias_samples",
                "source": "num_bias_samples"
              },
              {
                "id": "num_bootstraps",
                "source": "num_bootstraps"
              },
              {
                "id": "num_gibbs_samples",
                "source": "num_gibbs_samples"
              },
              {
                "id": "num_pre_aux_model_samples",
                "source": "num_pre_aux_model_samples"
              },
              {
                "id": "per_transcript_prior",
                "source": "per_transcript_prior"
              },
              {
                "id": "pos_bias",
                "source": "pos_bias"
              },
              {
                "id": "range_factorization_bins",
                "source": "range_factorization_bins"
              },
              {
                "id": "in_reads",
                "source": [
                  "sbg_pair_fastqs_by_metadata/tuple_list"
                ]
              },
              {
                "id": "read_ref_match",
                "source": "read_ref_match"
              },
              {
                "id": "read_ref_mismatch",
                "source": "read_ref_mismatch"
              },
              {
                "id": "reduce_GC_memory",
                "source": "reduce_GC_memory"
              },
              {
                "id": "in_archive",
                "source": "salmon_index_1_2_0/out_index"
              },
              {
                "id": "seq_bias",
                "source": "seq_bias"
              },
              {
                "id": "sig_digits",
                "source": "sig_digits"
              },
              {
                "id": "use_em",
                "source": "use_em"
              },
              {
                "id": "use_vbopt",
                "source": "use_vbopt"
              },
              {
                "id": "vb_prior",
                "source": "vb_prior"
              },
              {
                "id": "write_mappings",
                "source": "write_mappings"
              },
              {
                "id": "bandwidth",
                "source": "bandwidth"
              },
              {
                "id": "allow_dovetail",
                "source": "allow_dovetail"
              },
              {
                "id": "recover_orphans",
                "source": "recover_orphans"
              },
              {
                "id": "mimic_bt2",
                "source": "mimic_bt2"
              },
              {
                "id": "mimic_strict_bt2",
                "source": "mimic_strict_bt2"
              },
              {
                "id": "hard_filter",
                "source": "hard_filter"
              },
              {
                "id": "skip_quant",
                "source": "skip_quant"
              },
              {
                "id": "write_orphan_links",
                "source": "write_orphan_links"
              },
              {
                "id": "write_unmapped_names",
                "source": "write_unmapped_names"
              },
              {
                "id": "thinning_factor",
                "source": "thinning_factor"
              },
              {
                "id": "cpu_per_job",
                "source": "sbg_pair_fastqs_by_metadata/number_of_elements"
              },
              {
                "id": "threads",
                "source": "sbg_pair_fastqs_by_metadata/number_of_elements"
              },
              {
                "id": "softclip_overhangs",
                "source": "softclip_overhangs"
              },
              {
                "id": "full_length_alignment",
                "source": "full_length_alignment"
              },
              {
                "id": "hit_filter_policy",
                "source": "hit_filter_policy"
              },
              {
                "id": "max_occs_per_hit",
                "source": "max_occs_per_hit"
              },
              {
                "id": "no_single_frag_prob",
                "source": "no_single_frag_prob"
              },
              {
                "id": "per_nucleotide_prior",
                "source": "per_nucleotide_prior"
              }
            ],
            "out": [
              {
                "id": "out_quant_genes_sf"
              },
              {
                "id": "out_quant_sf"
              },
              {
                "id": "out_quant_archive"
              },
              {
                "id": "out_quant_log"
              },
              {
                "id": "out_meta_info"
              },
              {
                "id": "out_fraglen_dist"
              }
            ],
            "run": {
              "class": "CommandLineTool",
              "cwlVersion": "v1.0",
              "$namespaces": {
                "sbg": "https://sevenbridges.com"
              },
              "id": "h-d0b74b8c/h-581ac68c/h-74d1bb25/0",
              "baseCommand": [],
              "inputs": [
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "Use an alternative strategy (rather than simple interpolation) between the online and uniform abundance estimates to initialize the EM/VBEM algorithm.",
                  "sbg:category": "Advanced options",
                  "id": "alternative_init_mode",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--alternativeInitMode",
                    "shellQuote": false,
                    "position": 45
                  },
                  "label": "Alternative initialization mode",
                  "doc": "Use an alternative strategy (rather than simple interpolation) between the online and uniform abundance estimates to initialize the EM/VBEM algorithm."
                },
                {
                  "sbg:toolDefaultValue": "5",
                  "description": "The value at which the fragment length PMF is down-sampled when evaluating sequence-specific and & GC fragment bias. Larger values speed up effective length correction, but may decrease the fidelity of bias modeling results.",
                  "sbg:category": "Advanced options",
                  "id": "bias_speed_samp",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "--biasSpeedSamp",
                    "shellQuote": false,
                    "position": 17
                  },
                  "label": "Bias speed sample",
                  "doc": "The value at which the fragment length PMF is down-sampled when evaluating sequence-specific and & GC fragment bias. Larger values speed up effective length correction, but may decrease the fidelity of bias modeling results."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "This switch will learn the parameter distribution from the bootstrapped counts for each sample but will reproject those parameters onto the original equivalence class counts.",
                  "sbg:category": "Options specific to mapping mode",
                  "id": "bootstrap_reproject",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--bootstrapReproject",
                    "separate": false,
                    "shellQuote": false,
                    "position": 60
                  },
                  "label": "Bootstrap reproject",
                  "doc": "This switch will learn the parameter distribution from the bootstrapped counts for each sample but will reproject those parameters onto the original equivalence class counts."
                },
                {
                  "sbg:toolDefaultValue": "0",
                  "description": "The amount of slack allowed in the quasi-mapping consensus mechanism.  Normally, a transcript must cover all hits to be considered for mapping.  If this is set to a fraction, X, greater than 0 (and in [0,1)), then a transcript can fail to cover up to (100 * X)% of the hits before it is discounted as a mapping candidate.  The default value of this option is 0.2 if --validateMappings is given and 0 otherwise.",
                  "sbg:category": "Selective-alignment mode",
                  "id": "consensus_slack",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "--consensusSlack",
                    "shellQuote": false,
                    "position": 52
                  },
                  "label": "Consensus slack",
                  "doc": "The amount of slack allowed in the quasi-mapping consensus mechanism.  Normally, a transcript must cover all hits to be considered for mapping.  If this is set to a fraction, X, greater than 0 (and in [0,1)), then a transcript can fail to cover up to (100 * X)% of the hits before it is discounted as a mapping candidate.  The default value of this option is 0.2 if --validateMappings is given and 0 otherwise."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "Discard orphans mapping in quasi-mapping mode. If this flag is passed then only paired mappings will be considered towards quantification estimates. The default behaviour is to consider orphan mappings if no valid paired mappings exist. This flag is independent of the option to write the oprhaned mappings to file (--writeOprhanLinks).",
                  "sbg:category": "Selective-alignment mode",
                  "id": "discard_orphans_quasi",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--discardOrphansQuasi",
                    "shellQuote": false,
                    "position": 9
                  },
                  "label": "Discard orphans in Quasi-mapping mode",
                  "doc": "Discard orphans mapping in quasi-mapping mode. If this flag is passed then only paired mappings will be considered towards quantification estimates. The default behaviour is to consider orphan mappings if no valid paired mappings exist. This flag is independent of the option to write the oprhaned mappings to file (--writeOprhanLinks)."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "Dump conditional probabilities associated with transcripts when  equivalence class information is being dumped to file. Note, this will  dump the factorization that is actually used by salmon's offline phase  for inference.  If you are using range-factorized equivalence classes  (the default) then the same transcript set may appear multiple times  with different associated conditional probabilities.",
                  "sbg:category": "Options specific to mapping mode",
                  "id": "dump_eq",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--dumpEq",
                    "shellQuote": false,
                    "position": 15
                  },
                  "label": "Dump equivalence class counts",
                  "doc": "Dump conditional probabilities associated with transcripts when  equivalence class information is being dumped to file. Note, this will  dump the factorization that is actually used by salmon's offline phase  for inference.  If you are using range-factorized equivalence classes  (the default) then the same transcript set may appear multiple times  with different associated conditional probabilities."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "sbg:altPrefix": "--dumpEqWeights",
                  "description": "Includes 'rich' equivalence class weights in the output when equivalence class information is being dumpes to file.",
                  "sbg:category": "Advanced options",
                  "id": "dump_eq_weights",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "-d",
                    "shellQuote": false,
                    "position": 42
                  },
                  "label": "Dump equivalence class weights",
                  "doc": "Includes 'rich' equivalence class weights in the output when equivalence class information is being dumpes to file."
                },
                {
                  "sbg:toolDefaultValue": "1000",
                  "description": "The maximum fragment length to consider when building the empirical distribution.",
                  "sbg:category": "Advanced options",
                  "id": "fld_max",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "--fldMax",
                    "shellQuote": false,
                    "position": 18
                  },
                  "label": "Maximum fragment length",
                  "doc": "The maximum fragment length to consider when building the empirical distribution."
                },
                {
                  "sbg:toolDefaultValue": "200",
                  "description": "The mean used in the fragment lenght distribution prior.",
                  "sbg:category": "Advanced options",
                  "id": "fld_mean",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "--fldMean",
                    "shellQuote": false,
                    "position": 19
                  },
                  "label": "Mean fragment length",
                  "doc": "The mean used in the fragment lenght distribution prior."
                },
                {
                  "sbg:toolDefaultValue": "80",
                  "description": "The standard deviation used in the fragment length distribution prior.",
                  "sbg:category": "Advanced options",
                  "id": "fld_sd",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "--fldSD",
                    "shellQuote": false,
                    "position": 20
                  },
                  "label": "Fragment length standard deviation",
                  "doc": "The standard deviation used in the fragment length distribution prior."
                },
                {
                  "sbg:toolDefaultValue": "0.65000000000000002",
                  "description": "The forgetting factor used in the online learning schedule. A smaller value results in quicker learning, but higher variance and may be unstable. A larger value results in slower learning but may be more stable. The input value should be in the interva (0.5, 1.0].",
                  "sbg:category": "Advanced options",
                  "id": "forgetting_factor",
                  "type": "float?",
                  "inputBinding": {
                    "prefix": "-f",
                    "shellQuote": false,
                    "position": 21
                  },
                  "label": "Forgetting factor",
                  "doc": "The forgetting factor used in the online learning schedule. A smaller value results in quicker learning, but higher variance and may be unstable. A larger value results in slower learning but may be more stable. The input value should be in the interva (0.5, 1.0]."
                },
                {
                  "sbg:toolDefaultValue": "2",
                  "description": "The value given to a gap extension in an alignment.",
                  "sbg:category": "Selective-alignment mode",
                  "id": "gap_extension",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "--ge",
                    "shellQuote": false,
                    "position": 57
                  },
                  "label": "The value given to a gap extension in an alignment",
                  "doc": "The value given to a gap extension in an alignment."
                },
                {
                  "sbg:toolDefaultValue": "4",
                  "description": "The value given to a gap opening in an alignment.",
                  "sbg:category": "Selective-alignment mode",
                  "id": "gap_opening",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "--go",
                    "shellQuote": false,
                    "position": 56
                  },
                  "label": "The value given to a gap opening in an alignment",
                  "doc": "The value given to a gap opening in an alignment."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "[Biasl] Perform fragment GC bias correction.",
                  "sbg:category": "Basic options",
                  "id": "gc_bias",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--gcBias",
                    "shellQuote": false,
                    "position": 11
                  },
                  "label": "GC bias correction",
                  "doc": "[Biasl] Perform fragment GC bias correction."
                },
                {
                  "sbg:toolDefaultValue": "1",
                  "description": "The value by which to downsample transcripts when representing the GC content. Larger values will reduce memory usage, but may decrease the fidelity of bias modeling results.",
                  "sbg:category": "Advanced options",
                  "id": "gc_size_samp",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "--gcSizeSamp",
                    "shellQuote": false,
                    "position": 16
                  },
                  "label": "GC size sample",
                  "doc": "The value by which to downsample transcripts when representing the GC content. Larger values will reduce memory usage, but may decrease the fidelity of bias modeling results."
                },
                {
                  "description": "File containing a mapping of transcripts to genes.  If this file is provided Salmon will output both quant.sf and quant.genes.sf files, where the latter contains aggregated gene-level abundance estimates.  The transcript to gene mapping should be provided as either a GTF file, or a in a simple tab-delimited format where each line contains the name of a transcript and the gene to which it belongs separated by a tab.  The extension of the file is used to determine how the file should be parsed.  Files ending in '.gtf\u2019, \u2018.gff\u2019 or '.gff3\u2019 are assumed to be in GTF format; files with any other extension are assumed to be in the simple format. In GTF/GFF format, the \u2018transcript_id\u2019 is assumed to contain the transcript identifier and the \u2018gene_id\u2019 is assumed to contain the corresponding gene identifier.",
                  "sbg:category": "Inputs",
                  "id": "in_annotation",
                  "type": "File?",
                  "inputBinding": {
                    "prefix": "-g",
                    "shellQuote": false,
                    "position": 13
                  },
                  "label": "Gene map",
                  "doc": "File containing a mapping of transcripts to genes.  If this file is provided Salmon will output both quant.sf and quant.genes.sf files, where the latter contains aggregated gene-level abundance estimates.  The transcript to gene mapping should be provided as either a GTF file, or a in a simple tab-delimited format where each line contains the name of a transcript and the gene to which it belongs separated by a tab.  The extension of the file is used to determine how the file should be parsed.  Files ending in '.gtf\u2019, \u2018.gff\u2019 or '.gff3\u2019 are assumed to be in GTF format; files with any other extension are assumed to be in the simple format. In GTF/GFF format, the \u2018transcript_id\u2019 is assumed to contain the transcript identifier and the \u2018gene_id\u2019 is assumed to contain the corresponding gene identifier.",
                  "sbg:fileTypes": "GTF, GFF, GFF3, GTF.GZ, TSV"
                },
                {
                  "sbg:toolDefaultValue": "0",
                  "description": "This option sets the prior probability that an alignment that disagrees with the specified library type (--libType) results from the true fragment origin. Setting this to 0 specifies that alignments that disagree with the library type should be \"impossible\", while setting it to 1 says that alignments that disagree with the library type are no less likely than those that do.",
                  "sbg:category": "Basic options",
                  "id": "incompatible_prior",
                  "type": "float?",
                  "inputBinding": {
                    "prefix": "--incompatPrior",
                    "shellQuote": false,
                    "position": 12
                  },
                  "label": "Incompatible prior probability",
                  "doc": "This option sets the prior probability that an alignment that disagrees with the specified library type (--libType) results from the true fragment origin. Setting this to 0 specifies that alignments that disagree with the library type should be \"impossible\", while setting it to 1 says that alignments that disagree with the library type are no less likely than those that do."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "Initialize the offline inference with uniform parameters, rather than seeding with online parameters.",
                  "sbg:category": "Advanced options",
                  "id": "init_uniform",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--initUniform",
                    "shellQuote": false,
                    "position": 22
                  },
                  "label": "Initialize uniform parameters",
                  "doc": "Initialize the offline inference with uniform parameters, rather than seeding with online parameters."
                },
                {
                  "sbg:toolDefaultValue": "A",
                  "description": "Format string describing the library type. As of version 0.7.0, Salmon also has the ability to automatically infer (i.e. guess) the library type based on how the first few thousand reads map to the transcriptome. To allow Salmon to automatically infer the library type, simply provide the letter 'A' as an input to this parameter (also the default behaviour). Otherwise, the input string should be in the format of maximum three uppercase letters ('SSS'). The first part of the library string (relative orientation) is only provided if the library is paired-end. The possible options are: I=inward, O=outword, M=matching. The second part of the read library string specifies whether the protocol is stranded or unstranded; the options are: S=stranded, U=unstranded. If the protocol is unstranded, then we\u2019re done. The final part of the library string specifies the strand from which the read originates in a strand-specific protocol \u2014 it is only provided if the library is stranded (i.e. if the library format string is of the form S). The possible values are: F=read 1 (or single-end read) comes from the forward strand, R=read 1 (or single-end read) comes from the reverse strand. Examples: IU, SF, OSR.",
                  "sbg:category": "Basic options",
                  "id": "lib_type",
                  "type": "string?",
                  "inputBinding": {
                    "prefix": "-l",
                    "shellQuote": false,
                    "position": 6
                  },
                  "label": "Library type",
                  "doc": "Format string describing the library type. As of version 0.7.0, Salmon also has the ability to automatically infer (i.e. guess) the library type based on how the first few thousand reads map to the transcriptome. To allow Salmon to automatically infer the library type, simply provide the letter 'A' as an input to this parameter (also the default behaviour). Otherwise, the input string should be in the format of maximum three uppercase letters ('SSS'). The first part of the library string (relative orientation) is only provided if the library is paired-end. The possible options are: I=inward, O=outword, M=matching. The second part of the read library string specifies whether the protocol is stranded or unstranded; the options are: S=stranded, U=unstranded. If the protocol is unstranded, then we\u2019re done. The final part of the library string specifies the strand from which the read originates in a strand-specific protocol \u2014 it is only provided if the library is stranded (i.e. if the library format string is of the form S). The possible values are: F=read 1 (or single-end read) comes from the forward strand, R=read 1 (or single-end read) comes from the reverse strand. Examples: IU, SF, OSR.",
                  "default": "A"
                },
                {
                  "sbg:toolDefaultValue": "200",
                  "description": "Reads mapping to more than this many places won't be considered.",
                  "sbg:category": "Advanced options",
                  "id": "max_read_occ",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "-w",
                    "shellQuote": false,
                    "position": 23
                  },
                  "label": "Maximum read occurence",
                  "doc": "Reads mapping to more than this many places won't be considered."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "If you're using Salmon on a metagenomic dataset, consider setting this flag to disable parts of the abundance estimation model that make less sense for metagenomic data.",
                  "sbg:category": "Basic options",
                  "id": "meta",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--meta",
                    "shellQuote": false,
                    "position": 41
                  },
                  "label": "Meta",
                  "doc": "If you're using Salmon on a metagenomic dataset, consider setting this flag to disable parts of the abundance estimation model that make less sense for metagenomic data."
                },
                {
                  "sbg:toolDefaultValue": "10",
                  "description": "The minimum number of fragments that must be assigned to the transcriptome.",
                  "sbg:category": "Advanced options",
                  "id": "min_assigned_frags",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "--minAssignedFrags",
                    "shellQuote": false,
                    "position": 46
                  },
                  "label": "Minimum assigned fragments",
                  "doc": "The minimum number of fragments that must be assigned to the transcriptome."
                },
                {
                  "sbg:toolDefaultValue": "0.65",
                  "description": "The fraction of the optimal possible alignment score that a mapping must achieve in order to be considered \"valid\", should be in the range [0,1].",
                  "sbg:category": "Selective-alignment mode",
                  "id": "min_core_fraction",
                  "type": "float?",
                  "inputBinding": {
                    "prefix": "--minScoreFraction",
                    "shellQuote": false,
                    "position": 53
                  },
                  "label": "Minimum score fraction",
                  "doc": "The fraction of the optimal possible alignment score that a mapping must achieve in order to be considered \"valid\", should be in the range [0,1]."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "[Experimental] If this option is enabled, then no (lower) threshold will be set on how short bias correction can make effective lengths. This can increase precision of bias correction, but harm robustness. The default correction applies a threshold.",
                  "sbg:category": "Advanced options",
                  "id": "no_bias_length_threshold",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--noBiasLengthThreshold",
                    "shellQuote": false,
                    "position": 26
                  },
                  "label": "No bias length threshold",
                  "doc": "[Experimental] If this option is enabled, then no (lower) threshold will be set on how short bias correction can make effective lengths. This can increase precision of bias correction, but harm robustness. The default correction applies a threshold."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "Disables effective lenght correction when computing the probability that a fragment was generated from a transcript. If this flag is passed in, the fragment lenght distribution is not taken into account when computing this probability.",
                  "sbg:category": "Advanced options",
                  "id": "no_effective_length_correction",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--noEffectiveLengthCorrection",
                    "shellQuote": false,
                    "position": 24
                  },
                  "label": "No effective length correction",
                  "doc": "Disables effective lenght correction when computing the probability that a fragment was generated from a transcript. If this flag is passed in, the fragment lenght distribution is not taken into account when computing this probability."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "[Experimental] Do not consider concordance with the learned fragment lenght distribution when trying to determine the probability that a fragment has originated from a specific location. Normally, fragments with unlikely lengths will be assigned a smaller relative probability than those with more likely lenghts. When this flag is passed in, the observed fragment length has no effect on that fragment's a priori probability.",
                  "sbg:category": "Advanced options",
                  "id": "no_fragment_length_distribution",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--noFragLengthDist",
                    "shellQuote": false,
                    "position": 25
                  },
                  "label": "No fragment length distribution",
                  "doc": "[Experimental] Do not consider concordance with the learned fragment lenght distribution when trying to determine the probability that a fragment has originated from a specific location. Normally, fragments with unlikely lengths will be assigned a smaller relative probability than those with more likely lenghts. When this flag is passed in, the observed fragment length has no effect on that fragment's a priori probability."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "This switch will disable drawing transcript fractions from a Gamma distribution during Gibbs sampling.  In this case, the sampler does not account for shot-noise, but only assignment ambiguity.",
                  "sbg:category": "Options specific to mapping mode",
                  "id": "no_gamma_draw",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--noGammaDraw",
                    "separate": false,
                    "shellQuote": false,
                    "position": 59
                  },
                  "label": "No gamma draw",
                  "doc": "This switch will disable drawing transcript fractions from a Gamma distribution during Gibbs sampling.  In this case, the sampler does not account for shot-noise, but only assignment ambiguity."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "Entirely disables length correction when estimating abundance of transcripts. This option can be used with protocols where one expects that fragments derive from their underlying targets without regard to that target's length, e.g. QuantSeq (EXPERIMENTAL).",
                  "sbg:category": "Advanced options",
                  "id": "no_length_correction",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--noLengthCorrection",
                    "shellQuote": false,
                    "position": 43
                  },
                  "label": "No length correction",
                  "doc": "Entirely disables length correction when estimating abundance of transcripts. This option can be used with protocols where one expects that fragments derive from their underlying targets without regard to that target's length, e.g. QuantSeq (EXPERIMENTAL)."
                },
                {
                  "sbg:toolDefaultValue": "5000000",
                  "description": "The first this many numbers are used to train the auxiliary model parameters (e.g. fragment length distribution, bias, etc.). After their first that many observations, the auxiliary model parameters will be assumed to have converged and will be fixed.",
                  "sbg:category": "Advanced options",
                  "id": "num_aux_model_samples",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "--numAuxModelSamples",
                    "shellQuote": false,
                    "position": 28
                  },
                  "label": "Number of auxiliary model samples",
                  "doc": "The first this many numbers are used to train the auxiliary model parameters (e.g. fragment length distribution, bias, etc.). After their first that many observations, the auxiliary model parameters will be assumed to have converged and will be fixed."
                },
                {
                  "sbg:toolDefaultValue": "2000000",
                  "description": "Number of fragment mappings to use when learning the sequence-specific bias model.",
                  "sbg:category": "Advanced options",
                  "id": "num_bias_samples",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "--numBiasSamples",
                    "shellQuote": false,
                    "position": 27
                  },
                  "label": "Number of bias samples",
                  "doc": "Number of fragment mappings to use when learning the sequence-specific bias model."
                },
                {
                  "sbg:toolDefaultValue": "0",
                  "description": "The number of bootstrap samples to generate. Note: this is mutually exclusive with Gibbs sampling.",
                  "sbg:category": "Advanced options",
                  "id": "num_bootstraps",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "--numBootstraps",
                    "shellQuote": false,
                    "position": 34
                  },
                  "label": "Number of bootstraps",
                  "doc": "The number of bootstrap samples to generate. Note: this is mutually exclusive with Gibbs sampling."
                },
                {
                  "sbg:toolDefaultValue": "0",
                  "description": "The number of Gibbs sampling rounds to perform.",
                  "sbg:category": "Advanced options",
                  "id": "num_gibbs_samples",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "--numGibbsSamples",
                    "shellQuote": false,
                    "position": 33
                  },
                  "label": "Number of Gibbs samples",
                  "doc": "The number of Gibbs sampling rounds to perform."
                },
                {
                  "sbg:toolDefaultValue": "5000",
                  "description": "The first this many samples will have their assignment likelihoods and contributions to the transcript abundances computed without applying any auxiliary models. The purpose of ignoring the auxiliary models for the first that many observations is to avoid applying these models before their parameters have been learned sufficiently well.",
                  "sbg:category": "Advanced options",
                  "id": "num_pre_aux_model_samples",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "--numPreAuxModelSamples",
                    "shellQuote": false,
                    "position": 29
                  },
                  "label": "Number of pre auxiliary model samples",
                  "doc": "The first this many samples will have their assignment likelihoods and contributions to the transcript abundances computed without applying any auxiliary models. The purpose of ignoring the auxiliary models for the first that many observations is to avoid applying these models before their parameters have been learned sufficiently well."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "The prior (either the default, or the argument provided via --vbPrior) will be interpreted as a transcript-level prior (i.e. each transcript will be given a prior read count of this value).",
                  "sbg:category": "Advanced options",
                  "id": "per_transcript_prior",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--perTranscriptPrior",
                    "shellQuote": false,
                    "position": 30
                  },
                  "label": "Per transcript prior",
                  "doc": "The prior (either the default, or the argument provided via --vbPrior) will be interpreted as a transcript-level prior (i.e. each transcript will be given a prior read count of this value)."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "Enable modeling of a position-specific fragment start distribution. This is meant to model non-uniform coverage biases that are sometimes present in RNA-seq data (e.g. 5' or 3' positional bias). Currently, a small and fixed number of models are learned for different length classes of transcripts.",
                  "sbg:category": "Basic options",
                  "id": "pos_bias",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--posBias",
                    "shellQuote": false,
                    "position": 50
                  },
                  "label": "Position bias",
                  "doc": "Perform positional bias correction."
                },
                {
                  "sbg:toolDefaultValue": "4",
                  "description": "Factorizes the likelihood used in quantification by adopting a new notion  of equivalence classes based on the conditional probabilities with which  fragments are generated from different transcripts.  This is a more  fine-grained factorization than the normal rich equivalence classes.   The default value (4) corresponds to the default used in Zakeri et al.  2017 (doi: 10.1093/bioinformatics/btx262), and larger values imply a more  fine-grained factorization. If range factorization is enabled, a common  value to select for this parameter is 4. A value of 0 signifies the use  of basic rich equivalence classes.",
                  "sbg:category": "Advanced options",
                  "id": "range_factorization_bins",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "--rangeFactorizationBins",
                    "shellQuote": false,
                    "position": 49
                  },
                  "label": "Range factorization bins",
                  "doc": "Factorizes the likelihood used in quantification by adopting a new notion  of equivalence classes based on the conditional probabilities with which  fragments are generated from different transcripts.  This is a more  fine-grained factorization than the normal rich equivalence classes.   The default value (4) corresponds to the default used in Zakeri et al.  2017 (doi: 10.1093/bioinformatics/btx262), and larger values imply a more  fine-grained factorization. If range factorization is enabled, a common  value to select for this parameter is 4. A value of 0 signifies the use  of basic rich equivalence classes."
                },
                {
                  "description": "Input FASTQ read files.",
                  "sbg:category": "Inputs",
                  "id": "in_reads",
                  "type": "File[]?",
                  "inputBinding": {
                    "itemSeparator": " ",
                    "shellQuote": false,
                    "position": 7,
                    "valueFrom": "${\n    function get_meta_map(m, file, meta) {\n        if (meta in file.metadata) {\n            return m[file.metadata[meta]]\n        } else {\n            return m['Undefined']\n        }\n    }\n\n    function create_new_map(map, file, meta) {\n        if (meta in file.metadata) {\n            map[file.metadata[meta]] = {}\n            return map[file.metadata[meta]]\n        } else {\n            map['Undefined'] = {}\n            return map['Undefined']\n        }\n    }\n\n    var arr = [].concat(inputs.in_reads);\n    var map = {};\n    var sm_map;\n    var lb_map;\n    var pu_map;\n\n\n    if (arr.length == 1) {\n        return \"-r \" + arr[0].path\n    }\n\n    for (i in arr) {\n\n        sm_map = get_meta_map(map, arr[i], 'sample_id')\n        if (!sm_map) sm_map = create_new_map(map, arr[i], 'sample_id')\n\n        lb_map = get_meta_map(sm_map, arr[i], 'library_id')\n        if (!lb_map) lb_map = create_new_map(sm_map, arr[i], 'library_id')\n\n        pu_map = get_meta_map(lb_map, arr[i], 'platform_unit_id')\n        if (!pu_map) pu_map = create_new_map(lb_map, arr[i], 'platform_unit_id')\n\n        if ('file_segment_number' in arr[i].metadata) {\n            if (pu_map[arr[i].metadata['file_segment_number']]) {\n                a = pu_map[arr[i].metadata['file_segment_number']]\n                ar = [].concat(a)\n                ar = ar.concat(arr[i])\n                pu_map[arr[i].metadata['file_segment_number']] = ar\n            } else pu_map[arr[i].metadata['file_segment_number']] = [].concat(arr[i])\n        } else {\n            var a;\n            var ar;\n            if (pu_map['Undefined']) {\n                a = pu_map['Undefined']\n                ar = [].concat(a)\n                ar = ar.concat(arr[i])\n                pu_map['Undefined'] = ar\n            } else {\n                pu_map['Undefined'] = [].concat(arr[i])\n            }\n        }\n    }\n    var tuple_list = [];\n    for (sm in map)\n        for (lb in map[sm])\n            for (pu in map[sm][lb]) {\n                var list = [];\n                for (fsm in map[sm][lb][pu]) {\n                    list = map[sm][lb][pu][fsm]\n                    tuple_list.push(list)\n                }\n            }\n    //return tuple_list[0][0].path\n\n    var pe_1 = [];\n    var pe_2 = [];\n    var se = [];\n    if (tuple_list[0].length == 1) {\n        for (i = 0; i < tuple_list.length; i++) {\n            se = se.concat(tuple_list[i][0].path)\n        }\n    }\n    for (i = 0; i < tuple_list.length; i++) {\n        for (j = 0; j < tuple_list[i].length; j++) {\n            if (tuple_list[i][j].metadata.paired_end == 1) {\n                pe_1 = pe_1.concat(tuple_list[i][j].path)\n            } else if (tuple_list[i][j].metadata.paired_end == 2) {\n                pe_2 = pe_2.concat(tuple_list[i][j].path)\n            }\n        }\n    }\n\n    var cmd;\n    var tmp;\n    if (pe_2.length == 0) {\n        cmd = \"\"\n        if (se.length > 0) {\n            tmp = se\n        } else if (pe_1.length > 0) {\n            tmp = pe_1\n        }\n        for (i = 0; i < tmp.length; i++) {\n            cmd += tmp[i] + \" \"\n        }\n        return \"-r \" + cmd\n    } else if (pe_2.length > 0) {\n        var cmd1 = \"\";\n        var cmd2 = \"\";\n        for (i = 0; i < pe_1.length; i++) {\n            cmd1 += pe_1[i] + \" \"\n            cmd2 += pe_2[i] + \" \"\n        }\n        return \"-1 \" + cmd1 + \" -2 \" + cmd2\n    } else {\n        return \"\"\n    }\n\n}"
                  },
                  "label": "FASTQ Read files",
                  "doc": "Input FASTQ read files.",
                  "sbg:fileTypes": "FASTQ, FASTQ.GZ"
                },
                {
                  "sbg:toolDefaultValue": "2",
                  "description": "The value given to a match between read and reference nucleotides in an alignment.",
                  "sbg:category": "Selective-alignment mode",
                  "id": "read_ref_match",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "--ma",
                    "shellQuote": false,
                    "position": 54
                  },
                  "label": "The value given to a match between read and reference nucleotides in an alignment",
                  "doc": "The value given to a match between read and reference nucleotides in an alignment."
                },
                {
                  "sbg:toolDefaultValue": "-4",
                  "description": "The value given to a mismatch between read and reference nucleotides in an alignment.",
                  "sbg:category": "Selective-alignment mode",
                  "id": "read_ref_mismatch",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "--mp",
                    "shellQuote": false,
                    "position": 55
                  },
                  "label": "The value given to a mismatch between read and reference nucleotides in an alignment",
                  "doc": "The value given to a mismatch between read and reference nucleotides in an alignment."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "If this option is selected, a more memory efficient (but slightly slower) representation is used to compute fragment GC content. Enabling this will reduce memory usage, but can also reduce speed. However, the results themselves will remain the same.",
                  "sbg:category": "Advanced options",
                  "id": "reduce_GC_memory",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--reduceGCMemory",
                    "shellQuote": false,
                    "position": 47
                  },
                  "label": "Reduce GC memory",
                  "doc": "If this option is selected, a more memory efficient (but slightly slower) representation is used to compute fragment GC content. Enabling this will reduce memory usage, but can also reduce speed. However, the results themselves will remain the same."
                },
                {
                  "description": "Archive outputed by Salmon Index tool.",
                  "sbg:category": "Inputs",
                  "id": "in_archive",
                  "type": "File?",
                  "label": "Salmon index archive",
                  "doc": "Archive outputed by Salmon Index tool.",
                  "sbg:fileTypes": "TAR"
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "Perform sequence-specific bias correction.",
                  "sbg:category": "Basic options",
                  "id": "seq_bias",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--seqBias",
                    "shellQuote": false,
                    "position": 10
                  },
                  "label": "Sequence-specific bias correction",
                  "doc": "Perform sequence-specific bias correction."
                },
                {
                  "sbg:toolDefaultValue": "3",
                  "description": "The number of significant digits to write when outputting the EffectiveLength and NumReads columns.",
                  "sbg:category": "Options specific to mapping mode",
                  "id": "sig_digits",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "--sigDigits",
                    "shellQuote": false,
                    "position": 61
                  },
                  "label": "Significant digits",
                  "doc": "The number of significant digits to write when outputting the EffectiveLength and NumReads columns."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "Use the traditional EM algorithm for optimization in the batch passes.",
                  "sbg:category": "Options specific to mapping mode",
                  "id": "use_em",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--useEM",
                    "separate": false,
                    "shellQuote": false,
                    "position": 58
                  },
                  "label": "Use the EM algorithm",
                  "doc": "Use the traditional EM algorithm for optimization in the batch passes."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "Use the Variational Bayesian EM rather than the traditional EM algorithm for optimization in the batch passes.",
                  "sbg:category": "Advanced options",
                  "id": "use_vbopt",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--useVBOpt",
                    "shellQuote": false,
                    "position": 32
                  },
                  "label": "Use Variational Bayesian optimization",
                  "doc": "Use the Variational Bayesian EM rather than the traditional EM algorithm for optimization in the batch passes."
                },
                {
                  "sbg:toolDefaultValue": "0.01",
                  "description": "The prior that will be used in the VBEM algorithm.  This is interpreted  as a per-transcript prior, unless the --perNucleotidePrior flag is also  given. If the --perNucleotidePrior flag is given, this is used as a  nucleotide-level prior.  If the default is used, it will be divided by  1000 before being used as a nucleotide-level prior, i.e. the default  per-nucleotide prior will be 1e-5.",
                  "sbg:category": "Options specific to mapping mode",
                  "id": "vb_prior",
                  "type": "float?",
                  "inputBinding": {
                    "prefix": "--vbPrior",
                    "shellQuote": false,
                    "position": 31
                  },
                  "label": "VBEM prior",
                  "doc": "The prior that will be used in the VBEM algorithm.  This is interpreted  as a per-transcript prior, unless the --perNucleotidePrior flag is also  given. If the --perNucleotidePrior flag is given, this is used as a  nucleotide-level prior.  If the default is used, it will be divided by  1000 before being used as a nucleotide-level prior, i.e. the default  per-nucleotide prior will be 1e-5."
                },
                {
                  "sbg:toolDefaultValue": "aux_info",
                  "description": "The sub-directory of the quantification directory where auxiliary information e.g. bootstraps, bias parameters, etc. will be written.",
                  "sbg:category": "Advanced options",
                  "id": "aux_dir",
                  "type": "string?",
                  "inputBinding": {
                    "prefix": "--auxDir",
                    "shellQuote": false,
                    "position": 38
                  },
                  "label": "Aux dir",
                  "doc": "The sub-directory of the quantification directory where auxiliary information e.g. bootstraps, bias parameters, etc. will be written.",
                  "default": "aux_info"
                },
                {
                  "sbg:toolDefaultValue": "mapping_info.sam",
                  "description": "If this option is provided, then the quasi-mapping results will be written out in SAM-compatible format. By default, output will be directed to stdout, but an alternative file name can be provided instead.",
                  "sbg:category": "Options specific to mapping mode",
                  "id": "write_mappings",
                  "type": "string?",
                  "inputBinding": {
                    "prefix": "--writeMappings=",
                    "separate": false,
                    "shellQuote": false,
                    "position": 38
                  },
                  "label": "Write Mappings",
                  "doc": "If this option is provided, then the quasi-mapping results will be written out in SAM-compatible format. By default, output will be directed to stdout, but an alternative file name can be provided instead."
                },
                {
                  "sbg:toolDefaultValue": "15",
                  "description": "The value used for the bandwidth passed to ksw2.  A smaller bandwidth can make the alignment verification run more quickly, but could possibly miss valid alignments.",
                  "sbg:category": "Selective-alignment mode",
                  "id": "bandwidth",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "--bandwidth",
                    "shellQuote": false,
                    "position": 40
                  },
                  "label": "Bandwidth",
                  "doc": "The value used for the bandwidth passed to ksw2.  A smaller bandwidth can make the alignment verification run more quickly, but could possibly miss valid alignments."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "Allow dovetailing mappings.",
                  "sbg:category": "Selective-alignment mode",
                  "id": "allow_dovetail",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--allowDovetail",
                    "shellQuote": false,
                    "position": 41
                  },
                  "label": "Allow Dovetail",
                  "doc": "Allow dovetailing mappings."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "Attempt to recover the mates of orphaned reads. This uses edlib for orphan recovery, and so introduces some computational overhead, but it can improve sensitivity.",
                  "sbg:category": "Selective-alignment mode",
                  "id": "recover_orphans",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--recoverOrphans",
                    "shellQuote": false,
                    "position": 42
                  },
                  "label": "Recover Orphans",
                  "doc": "Attempt to recover the mates of orphaned reads. This uses edlib for orphan recovery, and so introduces some computational overhead, but it can improve sensitivity."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "Set flags to mimic parameters similar to Bowtie2 with --no-discordant and --no-mixed flags.This increases disallows dovetailing reads, and discards orphans. Note, this does not impose the very strict parameters assumed by RSEM+Bowtie2, like gapless alignments.  For that behavior, use the --mimiStrictBT2 flag below.",
                  "sbg:category": "Selective-alignment mode",
                  "id": "mimic_bt2",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--mimicBT2",
                    "shellQuote": false,
                    "position": 43
                  },
                  "label": "Mimic BT2",
                  "doc": "Set flags to mimic parameters similar to Bowtie2 with --no-discordant and --no-mixed flags.This increases disallows dovetailing reads, and discards orphans. Note, this does not impose the very strict parameters assumed by RSEM+Bowtie2, like gapless alignments.  For that behavior, use the --mimiStrictBT2 flag below."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "Set flags to mimic the very strict parameters used by RSEM+Bowtie2.  This increases --minScoreFraction to 0.8, disallows dovetailing reads, discards orphans, and disallows gaps in alignments.",
                  "sbg:category": "Selective-alignment mode",
                  "id": "mimic_strict_bt2",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--mimicStrictBT2",
                    "shellQuote": false,
                    "position": 44
                  },
                  "label": "Mimic Strict BT2",
                  "doc": "Set flags to mimic the very strict parameters used by RSEM+Bowtie2.  This increases --minScoreFraction to 0.8, disallows dovetailing reads, discards orphans, and disallows gaps in alignments."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "Instead of weighting mappings by their alignment score, this flag will discard any mappings with sub-optimal alignment score.  The default option of soft-filtering (i.e. weighting mappings by their alignment score) usually yields slightly more accurate abundance estimates but this flag may be desirable if you want more accurate 'naive' equivalence classes, rather than range factorized equivalence classes.",
                  "sbg:category": "Selective-alignment mode",
                  "id": "hard_filter",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--hardFilter",
                    "shellQuote": false,
                    "position": 45
                  },
                  "label": "Hard Filter",
                  "doc": "Instead of weighting mappings by their alignment score, this flag will discard any mappings with sub-optimal alignment score.  The default option of soft-filtering (i.e. weighting mappings by their alignment score) usually yields slightly more accurate abundance estimates but this flag may be desirable if you want more accurate 'naive' equivalence classes, rather than range factorized equivalence classes."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "Skip performing the actual transcript quantification (including any Gibbs sampling or bootstrapping).",
                  "sbg:category": "Options specific to mapping mode",
                  "id": "skip_quant",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--skipQuant",
                    "shellQuote": false,
                    "position": 46
                  },
                  "label": "Skip Quant",
                  "doc": "Skip performing the actual transcript quantification (including any Gibbs sampling or bootstrapping)."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "Write the transcripts that are linked by orphaned reads.",
                  "sbg:category": "Advanced options",
                  "id": "write_orphan_links",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--writeOrphanLinks",
                    "shellQuote": false,
                    "position": 47
                  },
                  "label": "Write Orphan Links",
                  "doc": "Write the transcripts that are linked by orphaned reads."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "Write the names of un-mapped reads to the file unmapped_names.txt in the auxiliary directory.",
                  "sbg:category": "Advanced options",
                  "id": "write_unmapped_names",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--writeUnmappedNames",
                    "shellQuote": false,
                    "position": 48
                  },
                  "label": "Write Unmapped Names",
                  "doc": "Write the names of un-mapped reads to the file unmapped_names.txt in the auxiliary directory."
                },
                {
                  "sbg:toolDefaultValue": "16",
                  "description": "Number of steps to discard for every sample kept from the Gibbs chain. The larger this number, the less chance that subsequent samples are auto-correlated, but the slower sampling becomes.",
                  "sbg:category": "Advanced options",
                  "id": "thinning_factor",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "--thinningFactor",
                    "shellQuote": false,
                    "position": 49
                  },
                  "label": "Thinning Factor",
                  "doc": "Number of steps to discard for every sample kept from the Gibbs chain. The larger this number, the less chance that subsequent samples are auto-correlated, but the slower sampling becomes."
                },
                {
                  "sbg:toolDefaultValue": "1000",
                  "description": "Amount of RAM memory to be used per job. Defaults to 1000MB for Single threaded jobs,and all of the available memory on the instance for multi-threaded jobs.",
                  "sbg:category": "Platform options",
                  "id": "mem_per_job",
                  "type": "int?",
                  "label": "Memory per job",
                  "doc": "Amount of RAM memory to be used per job. Defaults to 1000MB for Single threaded jobs,and all of the available memory on the instance for multi-threaded jobs."
                },
                {
                  "sbg:toolDefaultValue": "1",
                  "description": "Number of CPUs to be used per job.",
                  "sbg:category": "Platform options",
                  "id": "cpu_per_job",
                  "type": "int?",
                  "label": "CPU per job",
                  "doc": "Number of CPUs to be used per job."
                },
                {
                  "sbg:toolDefaultValue": "1",
                  "sbg:altPrefix": "--threads",
                  "description": "The number of threads to use concurrently.",
                  "sbg:category": "Basic options",
                  "id": "threads",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "-p",
                    "shellQuote": false,
                    "position": 8
                  },
                  "label": "Number of threads",
                  "doc": "The number of threads to use concurrently."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "Allow soft-clipping of reads that overhang the beginning or ends of the transcript.  In this case, the overhaning section of the read will simply be unaligned, and will not contribute or detract from the alignment score. The default policy is to force an end-to-end alignemnt of the entire read, so that overhanings will result in some deletion of nucleotides from the read.",
                  "sbg:category": "Selective-alignment mode",
                  "id": "softclip_overhangs",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "",
                    "shellQuote": false,
                    "position": 50,
                    "valueFrom": "${\n    if (self && !inputs.no_sa) {\n        return \"--softclipOverhangs\"\n    }\n}\n"
                  },
                  "label": "Softclip overhangs",
                  "doc": "Allow soft-clipping of reads that overhang the beginning or ends of the transcript.  In this case, the overhaning section of the read will simply be unaligned, and will not contribute or detract from the alignment score. The default policy is to force an end-to-end alignemnt of the entire read, so that overhanings will result in some deletion of nucleotides from the read."
                },
                {
                  "sbg:toolDefaultValue": "false",
                  "description": "Perform selective alignment over the full length of the read, beginning from the (approximate) initial mapping location and using extension alignment. This is in contrast with the default behavior which is to only perform alignment between the MEMs in the optimal chain (and before the first and after the last MEM if applicable). The default strategy forces the MEMs to belong to the alignment, but has the benefit that it can discover indels prior to the first hit shared between the read and reference.",
                  "sbg:category": "Options specific to mapping mode",
                  "id": "full_length_alignment",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "",
                    "shellQuote": false,
                    "position": 51,
                    "valueFrom": "${\n    if (self && !inputs.no_sa) {\n        return \"--fullLengthAlignment\"\n    }\n}\n"
                  },
                  "label": "Full length alignment [selective-alignment mode only]",
                  "doc": "Perform selective alignment over the full length of the read, beginning from the (approximate) initial mapping location and using extension alignment. This is in contrast with the default behavior which is to only perform alignment between the MEMs in the optimal chain (and before the first and after the last MEM if applicable). The default strategy forces the MEMs to belong to the alignment, but has the benefit that it can discover indels prior to the first hit shared between the read and reference."
                },
                {
                  "sbg:toolDefaultValue": "AFTER",
                  "description": "Determines the policy by which hits are filtered in selective alignment. Filtering hits after chaining (the default) is more sensitive, but more computationally intensive, because it performs the chaining dynamic program for all hits.  Filtering before chaining is faster, but some true hits may be missed.  The options are BEFORE, AFTER, BOTH and NONE.",
                  "sbg:category": "Selective-alignment mode",
                  "id": "hit_filter_policy",
                  "type": [
                    "null",
                    {
                      "type": "enum",
                      "symbols": [
                        "BEFORE",
                        "AFTER",
                        "BOTH",
                        "NONE"
                      ],
                      "name": "hit_filter_policy"
                    }
                  ],
                  "inputBinding": {
                    "prefix": "--hitFilterPolicy",
                    "shellQuote": false,
                    "position": 52
                  },
                  "label": "Hit filter policy",
                  "doc": "Determines the policy by which hits are filtered in selective alignment. Filtering hits after chaining (the default) is more sensitive, but more computationally intensive, because it performs the chaining dynamic program for all hits.  Filtering before chaining is faster, but some true hits may be missed.  The options are BEFORE, AFTER, BOTH and NONE."
                },
                {
                  "sbg:toolDefaultValue": "1000",
                  "description": "When collecting \"hits\" (MEMs), hits having more than maxOccsPerHit occurrences won't be considered.",
                  "sbg:category": "Advanced options",
                  "id": "max_occs_per_hit",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "--maxOccsPerHit",
                    "shellQuote": false,
                    "position": 54
                  },
                  "label": "Maximum occurrences per hit",
                  "doc": "When collecting \"hits\" (MEMs), hits having more than maxOccsPerHit occurrences won't be considered."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "Disables the estimation of an associated fragment length probability for single-end reads or for orphaned mappings in paired-end libraries.  The default behavior is to consider the probability of all possible fragment lengths associated with the retained mapping.  Enabling this flag (i.e. turning this default behavior off) will simply not attempt to estimate a fragment length probability in such cases.",
                  "sbg:category": "Advanced options",
                  "id": "no_single_frag_prob",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--noSingleFragProb",
                    "shellQuote": false,
                    "position": 55
                  },
                  "label": "Skip fragment length estimate for SE reads",
                  "doc": "Disables the estimation of an associated fragment length probability for single-end reads or for orphaned mappings in paired-end libraries.  The default behavior is to consider the probability of all possible fragment lengths associated with the retained mapping.  Enabling this flag (i.e. turning this default behavior off) will simply not attempt to estimate a fragment length probability in such cases."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "The prior (either the default or the argument provided via --vbPrior) will be interpreted as a nucleotide-level prior (i.e. each nucleotide will be given a prior read count of this value).",
                  "sbg:category": "Advanced options",
                  "id": "per_nucleotide_prior",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--perNucleotidePrior",
                    "shellQuote": false,
                    "position": 56
                  },
                  "label": "Nucleotide level prior",
                  "doc": "The prior (either the default or the argument provided via --vbPrior) will be interpreted as a nucleotide-level prior (i.e. each nucleotide will be given a prior read count of this value)."
                },
                {
                  "sbg:category": "Basic options",
                  "id": "in_aux_target",
                  "type": "File?",
                  "inputBinding": {
                    "prefix": "--auxTargetFile",
                    "shellQuote": false,
                    "position": 62
                  },
                  "label": "Auxiliary targets",
                  "doc": "A file containing a list of \"auxiliary\" targets.  These are valid targets (i.e., not decoys) to which fragments are allowed to map and be assigned, and which will be quantified, but for which auxiliary models like sequence-specific and fragment-GC bias correction should not be applied.",
                  "sbg:fileTypes": "TXT"
                },
                {
                  "sbg:toolDefaultValue": "1",
                  "sbg:category": "Selective-alignment mode",
                  "id": "score_exp",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "--scoreExp",
                    "shellQuote": false,
                    "position": 52
                  },
                  "label": "Downgrade sub-optimal alignments factor",
                  "doc": "The factor by which sub-optimal alignment scores are downweighted to produce a probability.  If the best alignment score for the current read is S, and the score for a particular alignment is w, then the probability will be computed porportional to exp( -scoreExp * (S-w) )."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "sbg:category": "Selective-alignment mode",
                  "id": "disable_chaining_heuristic",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--disableChainingHeuristic",
                    "shellQuote": false,
                    "position": 52
                  },
                  "label": "Disable chaining heuristic",
                  "doc": "By default, the heuristic of (Li 2018) is implemented, which terminates the chaining DP once a given number of valid backpointers are found.  This speeds up the seed (MEM) chaining step, but may result in sub-optimal chains in complex situations (e.g. sequences with many repeats and overlapping repeats). Passing this flag will disable the chaining heuristic, and perform the full chaining dynamic program, guaranteeing the optimal chain is found in this step."
                },
                {
                  "sbg:toolDefaultValue": "1",
                  "sbg:category": "Selective-alignment mode",
                  "id": "decoy_threshold",
                  "type": "float?",
                  "inputBinding": {
                    "prefix": "--decoyThreshold",
                    "shellQuote": false,
                    "position": 52
                  },
                  "label": "Decoy threshold",
                  "doc": "For an alignemnt to an annotated transcript to be considered invalid, it must have an alignment score < (decoyThreshold * bestDecoyScore).  A value of 1.0 means that any alignment strictly worse than the best decoy alignment will be discarded.  A smaller value will allow reads to be allocated to transcripts even if they strictly align better to the decoy sequence."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "sbg:category": "Selective-alignment mode",
                  "id": "softclip",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--softclip",
                    "shellQuote": false,
                    "position": 52
                  },
                  "label": "Enable soft-clipping (experimental)",
                  "doc": "Allows soft-clipping of reads during selective-alignment. If this option is provided, then regions at the beginning or end of the read can be withheld from alignment without any effect on the resulting score (i.e. neither adding nor removing from the score).  This will drastically reduce the penalty if there are mismatches at the beginning or end of the read due to e.g. low-quality bases or adapters. NOTE: Even with soft-clipping enabled, the read must still achieve a score of at least minScoreFraction * maximum achievable score, where the maximum achievable score is computed based on the full (un-clipped) read length."
                },
                {
                  "sbg:toolDefaultValue": "1.0000000000000001e-05",
                  "sbg:category": "Selective-alignment mode",
                  "id": "min_aln_prob",
                  "type": "float?",
                  "inputBinding": {
                    "prefix": "--minAlnProb",
                    "shellQuote": false,
                    "position": 52
                  },
                  "label": "Filter low probability alignments",
                  "doc": "Any mapping whose alignment probability (as computed by P(aln) = exp(-scoreExp * difference from best mapping score) is less than minAlnProb will not be considered as a valid alignment for this read.  The goal of this flag is to remove very low probability alignments that are unlikely to have any non-trivial effect on the final quantifications.  Filtering such alignments reduces the number of variables that need to be considered and can result in slightly faster inference and 'cleaner' equivalence classes."
                }
              ],
              "outputs": [
                {
                  "id": "out_quant_genes_sf",
                  "doc": "File containing aggregated gene-level abundance estimates.",
                  "label": "Gene-level quantifications",
                  "type": "File?",
                  "outputBinding": {
                    "glob": "${\n    if(inputs.in_archive && inputs.in_reads && [].concat(inputs.in_reads).length > 0){\n        function sharedStart(array){\n            var A= array.concat().sort(), \n            a1= A[0], a2= A[A.length-1], L= a1.length, i= 0;\n            while(i<L && a1.charAt(i)=== a2.charAt(i)) i++;\n            return a1.substring(0, i);\n        }\n        \n        var arr = [].concat(inputs.in_reads);\n        var sample_ids = [];\n        var file_paths = [];\n        \n        for (i = 0; i < arr.length; i++) {\n            if(arr[i].metadata && arr[i].metadata.sample_id){\n                sample_ids.push(arr[i].metadata.sample_id)\n            } else {\n                file_paths.push(arr[i].nameroot)\n            }\n        }\n        \n        if(sample_ids.length >= 1){\n            var basename = sharedStart(sample_ids).replace(\"-\",\"_\");\n        }\n        else if(file_paths.length >= 1){\n            var basename = sharedStart(file_paths).replace(\"-\",\"_\");\n        } else {\n            var basename = arr[0].nameroot;\n        }\n        if(basename.endsWith(\"_\")) {\n            var basename = basename.slice(0,-1);\n        }\n    \n        x = basename + \".salmon_quant\"\n        return x + \"/\" + x + \".genes.sf\";\n    }\n}",
                    "outputEval": "${\n    if(inputs.in_archive && inputs.in_reads && [].concat(inputs.in_reads).length > 0){\n        return inheritMetadata(self, inputs.in_reads)\n    }\n}"
                  },
                  "sbg:fileTypes": "SF",
                  "description": "File containing aggregated gene-level abundance estimates."
                },
                {
                  "id": "out_quant_sf",
                  "doc": "Salmon Quant output file, containing quantification results.",
                  "label": "Transcript-level quantifications",
                  "type": "File?",
                  "outputBinding": {
                    "glob": "${\n    if(inputs.in_archive && inputs.in_reads && [].concat(inputs.in_reads).length > 0){\n        function sharedStart(array){\n            var A= array.concat().sort(), \n            a1= A[0], a2= A[A.length-1], L= a1.length, i= 0;\n            while(i<L && a1.charAt(i)=== a2.charAt(i)) i++;\n            return a1.substring(0, i);\n        }\n        \n        var arr = [].concat(inputs.in_reads);\n        var sample_ids = [];\n        var file_paths = [];\n        \n        for (i = 0; i < arr.length; i++) {\n            if(arr[i].metadata && arr[i].metadata.sample_id){\n                sample_ids.push(arr[i].metadata.sample_id)\n            } else {\n                file_paths.push(arr[i].nameroot)\n            }\n        }\n        \n        if(sample_ids.length >= 1){\n            var basename = sharedStart(sample_ids).replace(\"-\",\"_\");\n        }\n        else if(file_paths.length >= 1){\n            var basename = sharedStart(file_paths).replace(\"-\",\"_\");\n        } else {\n            var basename = arr[0].nameroot;\n        }\n        if(basename.endsWith(\"_\")) {\n            var basename = basename.slice(0,-1);\n        }\n        \n        x = basename + \".salmon_quant\"\n        return x + \"/\" + x + \".sf\";\n    \n    }\n}",
                    "outputEval": "${\n    if(inputs.in_archive && inputs.in_reads && [].concat(inputs.in_reads).length > 0){\n        return inheritMetadata(self, inputs.in_reads)\n    }\n}"
                  },
                  "sbg:fileTypes": "SF",
                  "description": "Salmon Quant output file, containing quantification results."
                },
                {
                  "id": "out_quant_archive",
                  "doc": "All files outputed by Salmon Quant tool. Contains quantification files.",
                  "label": "Salmon Quant archive",
                  "type": "File?",
                  "outputBinding": {
                    "glob": "*salmon_quant_archive.tar",
                    "outputEval": "${\n    if(inputs.in_archive && inputs.in_reads && [].concat(inputs.in_reads).length > 0){\n        var setMetadata = function(file, metadata) {\n        if (!('metadata' in file))\n            file.metadata = metadata;\n        else {\n            for (var key in metadata) {\n                file.metadata[key] = metadata[key];\n            }\n        }\n        return file\n        };\n        \n        var arr = [].concat(inputs.in_reads);\n        var basename;\n        var meta_val;\n        if (arr[0].metadata && arr[0].metadata.sample_id) {\n            basename = arr[0].metadata.sample_id\n        } else {\n            basename = arr[0].path.split('/').pop().split('.')[0]\n        }\n        meta_val = basename + \".salmon_quant\"\n        \n        var quant_dir = \n        {\n        \"quant_dir\": meta_val,\n        };\n        \n        self[0] = inheritMetadata(self[0], inputs.in_reads)\n        return setMetadata(self[0], quant_dir)\n    \n    }\n}"
                  },
                  "sbg:fileTypes": "TAR",
                  "description": "All files outputed by Salmon Quant tool. Contains quantification files."
                },
                {
                  "id": "out_quant_log",
                  "doc": "Salmon quant log file.",
                  "label": "Salmon quant log",
                  "type": "File?",
                  "outputBinding": {
                    "glob": "${ \n    if(inputs.in_archive && inputs.in_reads && [].concat(inputs.in_reads).length > 0){\n        function sharedStart(array){\n            var A= array.concat().sort(), \n            a1= A[0], a2= A[A.length-1], L= a1.length, i= 0;\n            while(i<L && a1.charAt(i)=== a2.charAt(i)) i++;\n            return a1.substring(0, i);\n        }\n        \n        var arr = [].concat(inputs.in_reads);\n        var sample_ids = [];\n        var file_paths = [];\n        \n        for (i = 0; i < arr.length; i++) {\n            if(arr[i].metadata && arr[i].metadata.sample_id){\n                sample_ids.push(arr[i].metadata.sample_id)\n            } else {\n                file_paths.push(arr[i].nameroot)\n            }\n        }\n        \n        if(sample_ids.length >= 1){\n            var basename = sharedStart(sample_ids).replace(\"-\",\"_\");\n        }\n        else if(file_paths.length >= 1){\n            var basename = sharedStart(file_paths).replace(\"-\",\"_\");\n        } else {\n            var basename = arr[0].nameroot;\n        }\n        if(basename.endsWith(\"_\")) {\n            var basename = basename.slice(0,-1);\n        }\n        \n        var x = basename + \".salmon_quant/logs\"\n        var z = x + \"/\" + basename + \".salmon_quant.log\"\n        return z\n    }\n}",
                    "outputEval": "${\n    if(inputs.in_archive && inputs.in_reads && [].concat(inputs.in_reads).length > 0){\n        return inheritMetadata(self, inputs.in_reads)\n    }\n}"
                  },
                  "sbg:fileTypes": "LOG"
                },
                {
                  "id": "out_meta_info",
                  "doc": "A JSON format file called which contains meta information about the run, including stats such as the number of observed and mapped fragments, details of the bias modeling etc.",
                  "label": "Salmon meta info",
                  "type": "File?",
                  "outputBinding": {
                    "glob": "${ // rename meta_info.json file //\n    if(inputs.in_archive && inputs.in_reads && [].concat(inputs.in_reads).length > 0){\n        function sharedStart(array){\n            var A= array.concat().sort(), \n            a1= A[0], a2= A[A.length-1], L= a1.length, i= 0;\n            while(i<L && a1.charAt(i)=== a2.charAt(i)) i++;\n            return a1.substring(0, i);\n        }\n        \n        var arr = [].concat(inputs.in_reads);\n        var sample_ids = [];\n        var file_paths = [];\n        \n        for (i = 0; i < arr.length; i++) {\n            if(arr[i].metadata && arr[i].metadata.sample_id){\n                sample_ids.push(arr[i].metadata.sample_id)\n            } else {\n                file_paths.push(arr[i].nameroot)\n            }\n        }\n        \n        if(sample_ids.length >= 1){\n            var basename = sharedStart(sample_ids).replace(\"-\",\"_\");\n        }\n        else if(file_paths.length >= 1){\n            var basename = sharedStart(file_paths).replace(\"-\",\"_\");\n        } else {\n            var basename = arr[0].nameroot;\n        }\n        if(basename.endsWith(\"_\")) {\n            var basename = basename.slice(0,-1);\n        }\n        \n        var x = basename + \".salmon_quant/aux_info\";\n        var z = x + \"/\" + basename + \".meta_info.json\";\n        return z;\n    } else {\n        return \"echo 'Skipping quantification since reference index and input reads are not provided.' \" \n    }\n}",
                    "outputEval": "$(inheritMetadata(self, inputs.in_reads))"
                  },
                  "sbg:fileTypes": "JSON"
                },
                {
                  "id": "out_fraglen_dist",
                  "doc": "Fragment length distribution.",
                  "label": "Fragment length distribution",
                  "type": "File?",
                  "outputBinding": {
                    "glob": "${ // rename flenDist.txt file //\n    if(inputs.in_archive && inputs.in_reads && [].concat(inputs.in_reads).length > 0){\n        function sharedStart(array){\n            var A= array.concat().sort(), \n            a1= A[0], a2= A[A.length-1], L= a1.length, i= 0;\n            while(i<L && a1.charAt(i)=== a2.charAt(i)) i++;\n            return a1.substring(0, i);\n        }\n        \n        var arr = [].concat(inputs.in_reads);\n        var sample_ids = [];\n        var file_paths = [];\n        \n        for (i = 0; i < arr.length; i++) {\n            if(arr[i].metadata && arr[i].metadata.sample_id){\n                sample_ids.push(arr[i].metadata.sample_id)\n            } else {\n                file_paths.push(arr[i].nameroot)\n            }\n        }\n        \n        if(sample_ids.length >= 1){\n            var basename = sharedStart(sample_ids).replace(\"-\",\"_\");\n        }\n        else if(file_paths.length >= 1){\n            var basename = sharedStart(file_paths).replace(\"-\",\"_\");\n        } else {\n            var basename = arr[0].nameroot;\n        }\n        if(basename.endsWith(\"_\")) {\n            var basename = basename.slice(0,-1);\n        }\n        \n        var x = basename + \".salmon_quant/libParams\";\n        var z = x + \"/\" + basename + \".flenDist.txt\";\n        return z;\n    } else {\n        return \"echo 'Skipping quantification since reference index and input reads are not provided.' \" \n    }\n}",
                    "outputEval": "$(inheritMetadata(self, inputs.in_reads))"
                  },
                  "sbg:fileTypes": "TXT"
                }
              ],
              "doc": "**Salmon Quant - Reads** infers transcript abundance estimates from **RNA-seq data**, using the **Selective Alignment (SA)** algorithm. \n\n**SA** is designed to remain as fast as quasi-mapping while simultaneously eliminating many of its mapping errors. It relies upon alignment scoring to help differentiate between mapping loci that would otherwise be indistinguishable due to the multiple exact matches along the reference. Also, the improved mapping algorithm addresses the failure of direct alignment against the transcriptome, compared to spliced alignment to the genome by identifying and extracting sequence-similar decoy regions or using the entire genome as a decoy. The Salmon index is then augmented with these decoy sequences, which are handled in a special manner during mapping and alignment scoring, leading to a reduction of false mappings [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n- **FASTQ read files** is the required input port that accepts raw sequencing reads. \n- **Salmon index archive** is also required and accepts a salmon index file.\n- **Gene annotation** file can be used for gene-level aggregation of quantification results.\n- The workflow will generate transcript abundance estimates in plaintext format (**Transcript-level quantifications**), and an optional file containing **Gene-level quantifications** if the **Gene annotation** input is provided. \n- In addition to the quantification outputs, additional outputs can be produced if the proper options are selected. These files will be accessible in the TAR archive on the **Salmon Quant archive** output port. \n\n### Changes Introduced by Seven Bridges\n\n- The input sample ID will prefix all output files (inferred from **Sample ID** metadata if existent, or from filename otherwise), instead of having identical names between runs. \n\n### Common Issues and Important Notes\n\n- For paired-end read files, it is important to properly set the **Paired End** metadata field on your read files.\n- For FASTQ reads in multi-file format (i.e. two FASTQ files for paired-end 1 and two FASTQ files for paired-end2), proper metadata needs to be set (the following hierarchy is valid: **Sample ID/Library ID/Platform Unit ID/File Segment Number)**.\n- The GTF and FASTA files need to have compatible transcript IDs. \n\n### Performance Benchmarking\n\nThe main advantage of the Salmon software is that it is not computationally challenging, as alignment in the traditional sense is not performed. \nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes:\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost |  Instance (AWS) |\n|:---------------:|:-----------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     RNA-Seq     |  2 x 4.5 GB |     Yes    |     20M     |     101     |   8min   | $0.21| c4.8xlarge |\n|     RNA-Seq     | 2 x 17.4 GB |     Yes    |     76M     |     101     |   19min  | $0.5 | c4.8xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [Salmon paper](https://www.biorxiv.org/content/10.1101/657874v2)   \n[2] [Towards selective-alignment](https://www.biorxiv.org/content/10.1101/138800v2)",
              "label": "Salmon Quant - Reads",
              "arguments": [
                {
                  "prefix": "",
                  "shellQuote": false,
                  "position": 0,
                  "valueFrom": "${\n    if(inputs.in_archive && inputs.in_reads && [].concat(inputs.in_reads).length > 0){\n        return 'tar -xf'\n    }\n    else {\n        return \"echo 'Skipping quantification since reference index and input reads are not provided.' &&\" \n    }\n}"
                },
                {
                  "prefix": "",
                  "separate": false,
                  "shellQuote": false,
                  "position": 1,
                  "valueFrom": "${\n    if(inputs.in_archive){\n        return inputs.in_archive.path.split(\"/\").pop();\n    }\n}"
                },
                {
                  "prefix": "",
                  "separate": false,
                  "shellQuote": false,
                  "position": 2,
                  "valueFrom": "${\n    if(inputs.in_archive && inputs.in_reads && [].concat(inputs.in_reads).length > 0){\n        return '&& salmon quant'\n    }\n    else {\n        return \"echo 'Skipping quantification since reference index and input reads are not provided.' \" \n    }\n}\n"
                },
                {
                  "prefix": "-i",
                  "shellQuote": false,
                  "position": 5,
                  "valueFrom": "${\n    if(inputs.in_archive && inputs.in_archive.metadata){\n        return inputs.in_archive.metadata.index_name\n    }\n}"
                },
                {
                  "prefix": "-o",
                  "shellQuote": false,
                  "position": 36,
                  "valueFrom": "${ // output folder naming //\n    if(inputs.in_reads && [].concat(inputs.in_reads).length >0){\n        function sharedStart(array){\n            var A= array.concat().sort(), \n            a1= A[0], a2= A[A.length-1], L= a1.length, i= 0;\n            while(i<L && a1.charAt(i)=== a2.charAt(i)) i++;\n            return a1.substring(0, i);\n        }\n        \n        var arr = [].concat(inputs.in_reads);\n        var sample_ids = [];\n        var file_paths = [];\n        \n        for (i = 0; i < arr.length; i++) {\n            if(arr[i].metadata && arr[i].metadata.sample_id){\n                sample_ids.push(arr[i].metadata.sample_id)\n            } else {\n                file_paths.push(arr[i].nameroot)\n            }\n        }\n        \n        if(sample_ids.length >= 1){\n            var basename = sharedStart(sample_ids).replace(\"-\",\"_\");\n        }\n        else if(file_paths.length >= 1){\n            var basename = sharedStart(file_paths).replace(\"-\",\"_\");\n        } else {\n            var basename = arr[0].nameroot;\n        }\n        if(basename.endsWith(\"_\")) {\n            var basename = basename.slice(0,-1);\n        }\n        \n        return basename + \".salmon_quant\"\n    }\n}"
                },
                {
                  "prefix": "",
                  "shellQuote": false,
                  "position": 99,
                  "valueFrom": "${ // tar output folder // \n    if(inputs.in_archive && inputs.in_reads && [].concat(inputs.in_reads).length > 0){\n        function sharedStart(array){\n            var A= array.concat().sort(), \n            a1= A[0], a2= A[A.length-1], L= a1.length, i= 0;\n            while(i<L && a1.charAt(i)=== a2.charAt(i)) i++;\n            return a1.substring(0, i);\n        }\n        \n        var arr = [].concat(inputs.in_reads);\n        var sample_ids = [];\n        var file_paths = [];\n        \n        for (i = 0; i < arr.length; i++) {\n            if(arr[i].metadata && arr[i].metadata.sample_id){\n                sample_ids.push(arr[i].metadata.sample_id)\n            } else {\n                file_paths.push(arr[i].nameroot)\n            }\n        }\n        \n        if(sample_ids.length >= 1){\n            var basename = sharedStart(sample_ids).replace(\"-\",\"_\");\n        }\n        else if(file_paths.length >= 1){\n            var basename = sharedStart(file_paths).replace(\"-\",\"_\");\n        } else {\n            var basename = arr[0].nameroot;\n        }\n        if(basename.endsWith(\"_\")) {\n            var basename = basename.slice(0,-1);\n        }\n    \n        x = basename + \".salmon_quant\"\n        return \"&& tar -cf \" + x + \"_archive.tar \" + x;\n    } else {\n        return \"echo '&& Skipping quantification since reference index and input reads are not provided.' &&\" \n    }\n}"
                },
                {
                  "prefix": "",
                  "shellQuote": false,
                  "position": 100,
                  "valueFrom": "${ // rename unmaped names //\n    if(inputs.in_archive && inputs.in_reads && [].concat(inputs.in_reads).length > 0){\n        if (inputs.write_unmapped_names) {\n            function sharedStart(array){\n                var A= array.concat().sort(), \n                a1= A[0], a2= A[A.length-1], L= a1.length, i= 0;\n                while(i<L && a1.charAt(i)=== a2.charAt(i)) i++;\n                return a1.substring(0, i);\n            }\n            \n            var arr = [].concat(inputs.in_reads);\n            var sample_ids = [];\n            var file_paths = [];\n            \n            for (i = 0; i < arr.length; i++) {\n                if(arr[i].metadata && arr[i].metadata.sample_id){\n                    sample_ids.push(arr[i].metadata.sample_id)\n                } else {\n                    file_paths.push(arr[i].nameroot)\n                }\n            }\n            \n            if(sample_ids.length >= 1){\n                var basename = sharedStart(sample_ids).replace(\"-\",\"_\");\n            }\n            else if(file_paths.length >= 1){\n                var basename = sharedStart(file_paths).replace(\"-\",\"_\");\n            } else {\n                var basename = arr[0].nameroot;\n            }\n            if(basename.endsWith(\"_\")) {\n                var basename = basename.slice(0,-1);\n            }\n        \n            x = basename + \".salmon_quant\"\n            y = x + \"/aux_info/unmapped_names.txt\"\n            z = x + \"/aux_info/\" + x + \".unmapped_names.txt\"\n            return \"&& mv \" + y + \" \" + z;\n        }\n    } else {\n        return \"echo ' Skipping quantification since reference index and input reads are not provided.' &&\" \n    }\n}"
                },
                {
                  "prefix": "",
                  "shellQuote": false,
                  "position": 101,
                  "valueFrom": "${ // renam eq_classes.txt file //\n    if(inputs.in_archive && inputs.in_reads && [].concat(inputs.in_reads).length > 0){\n        if (inputs.dump_eq) {\n            function sharedStart(array){\n                var A= array.concat().sort(), \n                a1= A[0], a2= A[A.length-1], L= a1.length, i= 0;\n                while(i<L && a1.charAt(i)=== a2.charAt(i)) i++;\n                return a1.substring(0, i);\n            }\n            \n            var arr = [].concat(inputs.in_reads);\n            var sample_ids = [];\n            var file_paths = [];\n            \n            for (i = 0; i < arr.length; i++) {\n                if(arr[i].metadata && arr[i].metadata.sample_id){\n                    sample_ids.push(arr[i].metadata.sample_id)\n                } else {\n                    file_paths.push(arr[i].nameroot)\n                }\n            }\n            \n            if(sample_ids.length >= 1){\n                var basename = sharedStart(sample_ids).replace(\"-\",\"_\");\n            }\n            else if(file_paths.length >= 1){\n                var basename = sharedStart(file_paths).replace(\"-\",\"_\");\n            } else {\n                var basename = arr[0].nameroot;\n            }\n            if(basename.endsWith(\"_\")) {\n                var basename = basename.slice(0,-1);\n            }\n            \n            x = basename + \".salmon_quant\"\n            y = x + \"/aux_info/eq_classes.txt\"\n            z = x + \"/aux_info/\" + x + \".eq_classes.txt\"\n            return \"&& mv \" + y + \" \" + z\n        }\n    } else {\n        return \"echo 'Skipping quantification since reference index and input reads are not provided.' &&\" \n    }\n}"
                },
                {
                  "prefix": "",
                  "shellQuote": false,
                  "position": 102,
                  "valueFrom": "${ // rename gene-level counts //\n    if(inputs.in_archive && inputs.in_reads && [].concat(inputs.in_reads).length > 0){\n        if (inputs.in_annotation) {\n            function sharedStart(array){\n                var A= array.concat().sort(), \n                a1= A[0], a2= A[A.length-1], L= a1.length, i= 0;\n                while(i<L && a1.charAt(i)=== a2.charAt(i)) i++;\n                return a1.substring(0, i);\n            }\n            \n            var arr = [].concat(inputs.in_reads);\n            var sample_ids = [];\n            var file_paths = [];\n            \n            for (i = 0; i < arr.length; i++) {\n                if(arr[i].metadata && arr[i].metadata.sample_id){\n                    sample_ids.push(arr[i].metadata.sample_id)\n                } else {\n                    file_paths.push(arr[i].nameroot)\n                }\n            }\n            \n            if(sample_ids.length >= 1){\n                var basename = sharedStart(sample_ids).replace(\"-\",\"_\");\n            }\n            else if(file_paths.length >= 1){\n                var basename = sharedStart(file_paths).replace(\"-\",\"_\");\n            } else {\n                var basename = arr[0].nameroot;\n            }\n            if(basename.endsWith(\"_\")) {\n                var basename = basename.slice(0,-1);\n            }\n            \n            x = basename + \".salmon_quant\"\n            y = x + \"/quant.genes.sf\"\n            z = x + \"/\" + x + \".genes.sf\"\n            return \"&& mv \" + y + \" \" + z\n        }\n    } else {\n        return \"echo 'Skipping quantification since reference index and input reads are not provided.' &&\" \n    }\n}"
                },
                {
                  "prefix": "",
                  "shellQuote": false,
                  "position": 103,
                  "valueFrom": "${ // rename transcript-level counts file //\n    if(inputs.in_archive && inputs.in_reads && [].concat(inputs.in_reads).length > 0){\n        function sharedStart(array){\n            var A= array.concat().sort(), \n            a1= A[0], a2= A[A.length-1], L= a1.length, i= 0;\n            while(i<L && a1.charAt(i)=== a2.charAt(i)) i++;\n            return a1.substring(0, i);\n        }\n        \n        var arr = [].concat(inputs.in_reads);\n        var sample_ids = [];\n        var file_paths = [];\n        \n        for (i = 0; i < arr.length; i++) {\n            if(arr[i].metadata && arr[i].metadata.sample_id){\n                sample_ids.push(arr[i].metadata.sample_id)\n            } else {\n                file_paths.push(arr[i].nameroot)\n            }\n        }\n        \n        if(sample_ids.length >= 1){\n            var basename = sharedStart(sample_ids).replace(\"-\",\"_\");\n        }\n        else if(file_paths.length >= 1){\n            var basename = sharedStart(file_paths).replace(\"-\",\"_\");\n        } else {\n            var basename = arr[0].nameroot;\n        }\n        if(basename.endsWith(\"_\")) {\n            var basename = basename.slice(0,-1);\n        }\n        \n        x = basename + \".salmon_quant\"\n        y = x + \"/quant.sf\"\n        z = x + \"/\" + x + \".sf\"\n        return \"&& mv \" + y + \" \" + z;\n    } else {\n        return \"echo 'Skipping quantification since reference index and input reads are not provided.' &&\" \n    }\n}"
                },
                {
                  "prefix": "",
                  "shellQuote": false,
                  "position": 105,
                  "valueFrom": "${ // tar bootstaps folder //\n    if(inputs.in_archive && inputs.in_reads && [].concat(inputs.in_reads).length > 0){\n        if (inputs.num_bootstraps || inputs.num_gibbs_samples) {\n            function sharedStart(array){\n                var A= array.concat().sort(), \n                a1= A[0], a2= A[A.length-1], L= a1.length, i= 0;\n                while(i<L && a1.charAt(i)=== a2.charAt(i)) i++;\n                return a1.substring(0, i);\n            }\n            \n            var arr = [].concat(inputs.in_reads);\n            var sample_ids = [];\n            var file_paths = [];\n            \n            for (i = 0; i < arr.length; i++) {\n                if(arr[i].metadata && arr[i].metadata.sample_id){\n                    sample_ids.push(arr[i].metadata.sample_id)\n                } else {\n                    file_paths.push(arr[i].nameroot)\n                }\n            }\n            \n            if(sample_ids.length >= 1){\n                var basename = sharedStart(sample_ids).replace(\"-\",\"_\");\n            }\n            else if(file_paths.length >= 1){\n                var basename = sharedStart(file_paths).replace(\"-\",\"_\");\n            } else {\n                var basename = arr[0].nameroot;\n            }\n            if(basename.endsWith(\"_\")) {\n                var basename = basename.slice(0,-1);\n            }\n            \n            x = basename + \".salmon_quant\"\n            return \"&& tar -cf \" + x + \"_bootstrap_folder.tar \" + x + '/aux_info/bootstrap'\n        }\n    } else {\n        return \"echo 'Skipping quantification since reference index and input reads are not provided.' &&\" \n    }\n}"
                },
                {
                  "prefix": "",
                  "shellQuote": false,
                  "position": 106,
                  "valueFrom": "${ // rename log file //\n    if(inputs.in_archive && inputs.in_reads && [].concat(inputs.in_reads).length > 0){\n        function sharedStart(array){\n            var A= array.concat().sort(), \n            a1= A[0], a2= A[A.length-1], L= a1.length, i= 0;\n            while(i<L && a1.charAt(i)=== a2.charAt(i)) i++;\n            return a1.substring(0, i);\n        }\n        \n        var arr = [].concat(inputs.in_reads);\n        var sample_ids = [];\n        var file_paths = [];\n        \n        for (i = 0; i < arr.length; i++) {\n            if(arr[i].metadata && arr[i].metadata.sample_id){\n                sample_ids.push(arr[i].metadata.sample_id)\n            } else {\n                file_paths.push(arr[i].nameroot)\n            }\n        }\n        \n        if(sample_ids.length >= 1){\n            var basename = sharedStart(sample_ids).replace(\"-\",\"_\");\n        }\n        else if(file_paths.length >= 1){\n            var basename = sharedStart(file_paths).replace(\"-\",\"_\");\n        } else {\n            var basename = arr[0].nameroot;\n        }\n        if(basename.endsWith(\"_\")) {\n            var basename = basename.slice(0,-1);\n        }\n        \n        x = basename + \".salmon_quant/logs\"\n        y = x + \"/salmon_quant.log\"\n        z = x + \"/\" + basename + \".salmon_quant.log\"\n        return \"&& mv \" + y + \" \" + z;\n    } else {\n        return \"echo 'Skipping quantification since reference index and input reads are not provided.' \" \n    }\n}"
                },
                {
                  "prefix": "",
                  "shellQuote": false,
                  "position": 107,
                  "valueFrom": "${ // rename meta_info.json file //\n    if(inputs.in_archive && inputs.in_reads && [].concat(inputs.in_reads).length > 0){\n        function sharedStart(array){\n            var A= array.concat().sort(), \n            a1= A[0], a2= A[A.length-1], L= a1.length, i= 0;\n            while(i<L && a1.charAt(i)=== a2.charAt(i)) i++;\n            return a1.substring(0, i);\n        }\n        \n        var arr = [].concat(inputs.in_reads);\n        var sample_ids = [];\n        var file_paths = [];\n        \n        for (i = 0; i < arr.length; i++) {\n            if(arr[i].metadata && arr[i].metadata.sample_id){\n                sample_ids.push(arr[i].metadata.sample_id)\n            } else {\n                file_paths.push(arr[i].nameroot)\n            }\n        }\n        \n        if(sample_ids.length >= 1){\n            var basename = sharedStart(sample_ids).replace(\"-\",\"_\");\n        }\n        else if(file_paths.length >= 1){\n            var basename = sharedStart(file_paths).replace(\"-\",\"_\");\n        } else {\n            var basename = arr[0].nameroot;\n        }\n        if(basename.endsWith(\"_\")) {\n            var basename = basename.slice(0,-1);\n        }\n        \n        x = basename + \".salmon_quant/aux_info\"\n        y = x + \"/meta_info.json\"\n        z = x + \"/\" + basename + \".meta_info.json\"\n        return \"&& mv \" + y + \" \" + z;\n    } else {\n        return \"echo 'Skipping quantification since reference index and input reads are not provided.' \" \n    }\n}"
                },
                {
                  "prefix": "",
                  "shellQuote": false,
                  "position": 108,
                  "valueFrom": "${ // rename flenDist.txt file //\n    if(inputs.in_archive && inputs.in_reads && [].concat(inputs.in_reads).length > 0){\n        function sharedStart(array){\n            var A= array.concat().sort(), \n            a1= A[0], a2= A[A.length-1], L= a1.length, i= 0;\n            while(i<L && a1.charAt(i)=== a2.charAt(i)) i++;\n            return a1.substring(0, i);\n        }\n        \n        var arr = [].concat(inputs.in_reads);\n        var sample_ids = [];\n        var file_paths = [];\n        \n        for (i = 0; i < arr.length; i++) {\n            if(arr[i].metadata && arr[i].metadata.sample_id){\n                sample_ids.push(arr[i].metadata.sample_id)\n            } else {\n                file_paths.push(arr[i].nameroot)\n            }\n        }\n        \n        if(sample_ids.length >= 1){\n            var basename = sharedStart(sample_ids).replace(\"-\",\"_\");\n        }\n        else if(file_paths.length >= 1){\n            var basename = sharedStart(file_paths).replace(\"-\",\"_\");\n        } else {\n            var basename = arr[0].nameroot;\n        }\n        if(basename.endsWith(\"_\")) {\n            var basename = basename.slice(0,-1);\n        }\n        \n        var x = basename + \".salmon_quant/libParams\";\n        var y = x + \"/flenDist.txt\";\n        var z = x + \"/\" + basename + \".flenDist.txt\";\n        return \"&& mv \" + y + \" \" + z;\n    } else {\n        return \"echo 'Skipping quantification since reference index and input reads are not provided.' \" \n    }\n}"
                }
              ],
              "requirements": [
                {
                  "class": "ShellCommandRequirement"
                },
                {
                  "class": "ResourceRequirement",
                  "ramMin": "${\n    if(inputs.mem_per_job){\n        return inputs.mem_per_job\n    } else {\n        return 1000\n    }\n}",
                  "coresMin": "${\n    if(inputs.cpu_per_job){\n        return inputs.cpu_per_job\n    } else {\n        return 16\n    }\n}"
                },
                {
                  "class": "DockerRequirement",
                  "dockerImageId": "ea69041ddb8d42ee13362fe71f1149e5044edbd7cbf66ef4a1919f8736777007",
                  "dockerPull": "images.sbgenomics.com/nemanja.vucic/salmon-1-2-0:0"
                },
                {
                  "class": "InitialWorkDirRequirement",
                  "listing": [
                    "$(inputs.in_archive)"
                  ]
                },
                {
                  "class": "InlineJavascriptRequirement",
                  "expressionLib": [
                    "var updateMetadata = function(file, key, value) {\n    file['metadata'][key] = value;\n    return file;\n};\n\n\nvar setMetadata = function(file, metadata) {\n    if (!('metadata' in file))\n        file['metadata'] = metadata;\n    else {\n        for (var key in metadata) {\n            file['metadata'][key] = metadata[key];\n        }\n    }\n    return file\n};\n\nvar inheritMetadata = function(o1, o2) {\n    var commonMetadata = {};\n    if (!Array.isArray(o2)) {\n        o2 = [o2]\n    }\n    for (var i = 0; i < o2.length; i++) {\n        var example = o2[i]['metadata'];\n        for (var key in example) {\n            if (i == 0)\n                commonMetadata[key] = example[key];\n            else {\n                if (!(commonMetadata[key] == example[key])) {\n                    delete commonMetadata[key]\n                }\n            }\n        }\n    }\n    if (!Array.isArray(o1)) {\n        o1 = setMetadata(o1, commonMetadata)\n    } else {\n        for (var i = 0; i < o1.length; i++) {\n            o1[i] = setMetadata(o1[i], commonMetadata)\n        }\n    }\n    return o1;\n};\n\nvar toArray = function(file) {\n    return [].concat(file);\n};\n\nvar groupBy = function(files, key) {\n    var groupedFiles = [];\n    var tempDict = {};\n    for (var i = 0; i < files.length; i++) {\n        var value = files[i]['metadata'][key];\n        if (value in tempDict)\n            tempDict[value].push(files[i]);\n        else tempDict[value] = [files[i]];\n    }\n    for (var key in tempDict) {\n        groupedFiles.push(tempDict[key]);\n    }\n    return groupedFiles;\n};\n\nvar orderBy = function(files, key, order) {\n    var compareFunction = function(a, b) {\n        if (a['metadata'][key].constructor === Number) {\n            return a['metadata'][key] - b['metadata'][key];\n        } else {\n            var nameA = a['metadata'][key].toUpperCase();\n            var nameB = b['metadata'][key].toUpperCase();\n            if (nameA < nameB) {\n                return -1;\n            }\n            if (nameA > nameB) {\n                return 1;\n            }\n            return 0;\n        }\n    };\n\n    files = files.sort(compareFunction);\n    if (order == undefined || order == \"asc\")\n        return files;\n    else\n        return files.reverse();\n};",
                    "\nvar setMetadata = function(file, metadata) {\n    if (!('metadata' in file)) {\n        file['metadata'] = {}\n    }\n    for (var key in metadata) {\n        file['metadata'][key] = metadata[key];\n    }\n    return file\n};\nvar inheritMetadata = function(o1, o2) {\n    var commonMetadata = {};\n    if (!o2) {\n        return o1;\n    };\n    if (!Array.isArray(o2)) {\n        o2 = [o2]\n    }\n    for (var i = 0; i < o2.length; i++) {\n        var example = o2[i]['metadata'];\n        for (var key in example) {\n            if (i == 0)\n                commonMetadata[key] = example[key];\n            else {\n                if (!(commonMetadata[key] == example[key])) {\n                    delete commonMetadata[key]\n                }\n            }\n        }\n        for (var key in commonMetadata) {\n            if (!(key in example)) {\n                delete commonMetadata[key]\n            }\n        }\n    }\n    if (!Array.isArray(o1)) {\n        o1 = setMetadata(o1, commonMetadata)\n    } else {\n        for (var i = 0; i < o1.length; i++) {\n            o1[i] = setMetadata(o1[i], commonMetadata)\n        }\n    }\n    return o1;\n};",
                    "\nvar setMetadata = function(file, metadata) {\n    if (!('metadata' in file)) {\n        file['metadata'] = {}\n    }\n    for (var key in metadata) {\n        file['metadata'][key] = metadata[key];\n    }\n    return file\n};\nvar inheritMetadata = function(o1, o2) {\n    var commonMetadata = {};\n    if (!o2) {\n        return o1;\n    };\n    if (!Array.isArray(o2)) {\n        o2 = [o2]\n    }\n    for (var i = 0; i < o2.length; i++) {\n        var example = o2[i]['metadata'];\n        for (var key in example) {\n            if (i == 0)\n                commonMetadata[key] = example[key];\n            else {\n                if (!(commonMetadata[key] == example[key])) {\n                    delete commonMetadata[key]\n                }\n            }\n        }\n        for (var key in commonMetadata) {\n            if (!(key in example)) {\n                delete commonMetadata[key]\n            }\n        }\n    }\n    if (!Array.isArray(o1)) {\n        o1 = setMetadata(o1, commonMetadata)\n        if (o1.secondaryFiles) {\n            o1.secondaryFiles = inheritMetadata(o1.secondaryFiles, o2)\n        }\n    } else {\n        for (var i = 0; i < o1.length; i++) {\n            o1[i] = setMetadata(o1[i], commonMetadata)\n            if (o1[i].secondaryFiles) {\n                o1[i].secondaryFiles = inheritMetadata(o1[i].secondaryFiles, o2)\n            }\n        }\n    }\n    return o1;\n};"
                  ]
                }
              ],
              "sbg:publisher": "sbg",
              "sbg:toolkitVersion": "1.2.0",
              "sbg:cmdPreview": "tar -xf in_archive.tar.gz && salmon quant -i salmon_index  -r /path/to/sampleA_lane1_pe1.fastq -o sampleA_lane1_pe1.salmon_quant --auxDir aux_info  && tar -cf sampleA_lane1_pe1.salmon_quant_archive.tar sampleA_lane1_pe1.salmon_quant  && mv sampleA_lane1_pe1.salmon_quant/aux_info/unmapped_names.txt sampleA_lane1_pe1.salmon_quant/aux_info/sampleA_lane1_pe1.salmon_quant.unmapped_names.txt  && mv sampleA_lane1_pe1.salmon_quant/aux_info/eq_classes.txt sampleA_lane1_pe1.salmon_quant/aux_info/sampleA_lane1_pe1.salmon_quant.eq_classes.txt  && mv sampleA_lane1_pe1.salmon_quant/quant.genes.sf sampleA_lane1_pe1.salmon_quant/sampleA_lane1_pe1.salmon_quant.genes.sf  && mv sampleA_lane1_pe1.salmon_quant/quant.sf sampleA_lane1_pe1.salmon_quant/sampleA_lane1_pe1.salmon_quant.sf  && tar -cf sampleA_lane1_pe1.salmon_quant_bootstrap_folder.tar sampleA_lane1_pe1.salmon_quant/aux_info/bootstrap",
              "sbg:createdBy": "nemanja.vucic",
              "sbg:revisionNotes": "copied rev. 4 from dev.",
              "sbg:project": "nemanja.vucic/salmon-v1-2-0-demo",
              "sbg:links": [
                {
                  "id": "http://combine-lab.github.io/salmon/",
                  "label": "Salmon Homepage"
                },
                {
                  "id": "https://github.com/COMBINE-lab/salmon",
                  "label": "Salmon Source Code"
                },
                {
                  "id": "https://github.com/COMBINE-lab/salmon/releases/tag/v1.2.0",
                  "label": "Salmon Download"
                },
                {
                  "id": "https://www.biorxiv.org/content/10.1101/657874v2",
                  "label": "Salmon Publications"
                },
                {
                  "id": "http://salmon.readthedocs.org/en/latest/",
                  "label": "Salmon Documentation"
                }
              ],
              "sbg:content_hash": "aa18da4a9e44bf185c4174dd02f850167c096325c83489570f1a67380f9e3d8ec",
              "sbg:toolAuthor": "Rob Patro, Carl Kingsford, Steve Mount, Mohsen Zakeri",
              "sbg:projectName": "Salmon v1.2.0 - Demo",
              "sbg:id": "h-d0b74b8c/h-581ac68c/h-74d1bb25/0",
              "sbg:modifiedBy": "nemanja.vucic",
              "sbg:revisionsInfo": [
                {
                  "sbg:revisionNotes": null,
                  "sbg:revision": 0,
                  "sbg:modifiedBy": "nemanja.vucic",
                  "sbg:modifiedOn": 1586781399
                },
                {
                  "sbg:revisionNotes": "copied revision 2 from dev",
                  "sbg:revision": 1,
                  "sbg:modifiedBy": "nemanja.vucic",
                  "sbg:modifiedOn": 1586781425
                },
                {
                  "sbg:revisionNotes": "copied rev. 4 from dev.",
                  "sbg:revision": 2,
                  "sbg:modifiedBy": "nemanja.vucic",
                  "sbg:modifiedOn": 1588590919
                }
              ],
              "sbg:contributors": [
                "nemanja.vucic"
              ],
              "sbg:createdOn": 1586781399,
              "sbg:appVersion": [
                "v1.0"
              ],
              "sbg:latestRevision": 2,
              "sbg:validationErrors": [],
              "sbg:license": "GNU General Public License v3.0 only",
              "sbg:image_url": null,
              "sbg:sbgMaintained": false,
              "sbg:revision": 2,
              "sbg:toolkit": "Salmon",
              "sbg:modifiedOn": 1588590919,
              "sbg:categories": [
                "Transcriptomics",
                "Quantification",
                "CWL1.0"
              ]
            },
            "label": "Salmon Quant - Reads",
            "scatter": [
              "in_reads"
            ],
            "sbg:x": 720.5433349609375,
            "sbg:y": 239.0078125
          },
          {
            "id": "sbg_pair_fastqs_by_metadata",
            "in": [
              {
                "id": "fastq_list",
                "source": [
                  "in_reads"
                ]
              },
              {
                "id": "max_number_of_parallel_jobs",
                "default": 2,
                "source": "max_number_of_parallel_jobs"
              },
              {
                "id": "number_of_cpus",
                "default": 36,
                "source": "number_of_cpus"
              }
            ],
            "out": [
              {
                "id": "number_of_elements"
              },
              {
                "id": "tuple_list"
              }
            ],
            "run": {
              "class": "CommandLineTool",
              "cwlVersion": "v1.0",
              "$namespaces": {
                "sbg": "https://sevenbridges.com"
              },
              "id": "h-b30ace5a/h-5e175313/h-abd9599e/0",
              "baseCommand": [
                "echo"
              ],
              "inputs": [
                {
                  "id": "fastq_list",
                  "type": "File[]",
                  "label": "List of FASTQ files",
                  "doc": "List of the FASTQ files with properly set metadata fileds.",
                  "sbg:fileTypes": "FASTQ, FQ, FASTQ.GZ, FQ.GZ"
                },
                {
                  "sbg:toolDefaultValue": "4",
                  "sbg:category": "Options",
                  "id": "max_number_of_parallel_jobs",
                  "type": "int?",
                  "label": "Maximum number of parallel jobs",
                  "doc": "Maximum number of parallel jobs to allow in the tool downstream of this one."
                },
                {
                  "sbg:toolDefaultValue": "32",
                  "sbg:category": "Options",
                  "id": "number_of_cpus",
                  "type": "int?",
                  "label": "Number of CPUs",
                  "doc": "Number of CPUs available in the workflow that uses this tool. This number will be used to determine the optimal number of threads to use in the tool downstream of this one."
                }
              ],
              "outputs": [
                {
                  "id": "number_of_elements",
                  "doc": "Number of paired elements created from the input FASTQs",
                  "label": "Number of elements",
                  "type": "int?",
                  "outputBinding": {
                    "outputEval": "${\n    function get_meta_map(m, file, meta) {\n        if (meta in file.metadata) {\n            return m[file.metadata[meta]]\n        } else {\n            return m['Undefined']\n        }\n    }\n\n    function create_new_map(map, file, meta) {\n        if (meta in file.metadata) {\n            map[file.metadata[meta]] = {}\n            return map[file.metadata[meta]]\n        } else {\n            map['Undefined'] = {}\n            return map['Undefined']\n        }\n    }\n\n    arr = [].concat(inputs.fastq_list)\n    map = {}\n\n    for (i in arr) {\n\n        sm_map = get_meta_map(map, arr[i], 'sample_id')\n        if (!sm_map) sm_map = create_new_map(map, arr[i], 'sample_id')\n\n        lb_map = get_meta_map(sm_map, arr[i], 'library_id')\n        if (!lb_map) lb_map = create_new_map(sm_map, arr[i], 'library_id')\n\n        pu_map = get_meta_map(lb_map, arr[i], 'platform_unit_id')\n        if (!pu_map) pu_map = create_new_map(lb_map, arr[i], 'platform_unit_id')\n\n        if ('file_segment_number' in arr[i].metadata) {\n            if (pu_map[arr[i].metadata['file_segment_number']]) {\n                a = pu_map[arr[i].metadata['file_segment_number']]\n                ar = [].concat(a)\n                ar = ar.concat(arr[i])\n                pu_map[arr[i].metadata['file_segment_number']] = ar\n            } else pu_map[arr[i].metadata['file_segment_number']] = [].concat(arr[i])\n        } else {\n            if (pu_map['Undefined']) {\n                a = pu_map['Undefined']\n                ar = [].concat(a)\n                ar = ar.concat(arr[i])\n                pu_map['Undefined'] = ar\n            } else {\n                pu_map['Undefined'] = [].concat(arr[i])\n            }\n        }\n    }\n    tuple_list = []\n    for (sm in map)\n        for (lb in map[sm])\n            for (pu in map[sm][lb]) {\n                for (fsm in map[sm][lb][pu]) {\n                    list = map[sm][lb][pu][fsm]\n                    tuple_list.push(list)\n                }\n            }\n\n    var number_of_cpus = inputs.number_of_cpus ? inputs.number_of_cpus : 32\n    var threads = Math.floor(number_of_cpus / tuple_list.length)\n    return threads < number_of_cpus / inputs.max_number_of_parallel_jobs ? Math.floor(number_of_cpus / inputs.max_number_of_parallel_jobs) : threads\n}"
                  }
                },
                {
                  "id": "tuple_list",
                  "doc": "List of grouped FASTQ files by metadata fields.",
                  "label": "List of grouped FASTQ files",
                  "type": "File[]?",
                  "outputBinding": {
                    "outputEval": "${\n    function get_meta_map(m, file, meta) {\n        if (meta in file.metadata) {\n            return m[file.metadata[meta]]\n        } else {\n            return m['Undefined']\n        }\n    }\n\n    function create_new_map(map, file, meta) {\n        if (meta in file.metadata) {\n            map[file.metadata[meta]] = {}\n            return map[file.metadata[meta]]\n        } else {\n            map['Undefined'] = {}\n            return map['Undefined']\n        }\n    }\n\n    arr = [].concat(inputs.fastq_list)\n    map = {}\n\n    for (i in arr) {\n\n        sm_map = get_meta_map(map, arr[i], 'sample_id')\n        if (!sm_map) sm_map = create_new_map(map, arr[i], 'sample_id')\n\n        lb_map = get_meta_map(sm_map, arr[i], 'library_id')\n        if (!lb_map) lb_map = create_new_map(sm_map, arr[i], 'library_id')\n\n        pu_map = get_meta_map(lb_map, arr[i], 'platform_unit_id')\n        if (!pu_map) pu_map = create_new_map(lb_map, arr[i], 'platform_unit_id')\n\n        if ('file_segment_number' in arr[i].metadata) {\n            if (pu_map[arr[i].metadata['file_segment_number']]) {\n                a = pu_map[arr[i].metadata['file_segment_number']]\n                ar = [].concat(a)\n                ar = ar.concat(arr[i])\n                pu_map[arr[i].metadata['file_segment_number']] = ar\n            } else pu_map[arr[i].metadata['file_segment_number']] = [].concat(arr[i])\n        } else {\n            if (pu_map['Undefined']) {\n                a = pu_map['Undefined']\n                ar = [].concat(a)\n                ar = ar.concat(arr[i])\n                pu_map['Undefined'] = ar\n            } else {\n                pu_map['Undefined'] = [].concat(arr[i])\n            }\n        }\n    }\n    tuple_list = []\n    for (sm in map)\n        for (lb in map[sm])\n            for (pu in map[sm][lb]) {\n                for (fsm in map[sm][lb][pu]) {\n                    list = map[sm][lb][pu][fsm]\n                    tuple_list.push(list)\n                }\n            }\n    return tuple_list\n}"
                  },
                  "sbg:fileTypes": "FASTQ, FQ, FASTQ.GZ, FQ.GZ"
                }
              ],
              "doc": "Tool accepts list of FASTQ files groups them into separate lists. This grouping is done using metadata values and their hierarchy (Sample ID > Library ID > Platform unit ID > File segment number) which should create unique combinations for each pair of FASTQ files. Important metadata fields are Sample ID, Library ID, Platform unit ID and File segment number. Not all of these four metadata fields are required, but the present set has to be sufficient to create unique combinations for each pair of FASTQ files. Files with no paired end metadata are grouped in the same way as the ones with paired end metadata, generally they should be alone in a separate list. Files with no metadata set will be grouped together. \n\nIf there are more than two files in a group, this might create errors further down most pipelines and the user should check if the metadata fields for those files are set properly.",
              "label": "SBG Pair FASTQs by Metadata CWL 1.0",
              "arguments": [
                {
                  "prefix": "",
                  "separate": false,
                  "shellQuote": false,
                  "position": 0,
                  "valueFrom": "'Pairing FASTQs!'"
                }
              ],
              "requirements": [
                {
                  "class": "ShellCommandRequirement"
                },
                {
                  "class": "ResourceRequirement",
                  "ramMin": 1024,
                  "coresMin": 1
                },
                {
                  "class": "DockerRequirement",
                  "dockerImageId": "d41a0837ab81",
                  "dockerPull": "alpine"
                },
                {
                  "class": "InitialWorkDirRequirement",
                  "listing": [
                    "$(inputs.fastq_list)"
                  ]
                },
                {
                  "class": "InlineJavascriptRequirement",
                  "expressionLib": [
                    "var updateMetadata = function(file, key, value) {\n    file['metadata'][key] = value;\n    return file;\n};\n\n\nvar setMetadata = function(file, metadata) {\n    if (!('metadata' in file))\n        file['metadata'] = metadata;\n    else {\n        for (var key in metadata) {\n            file['metadata'][key] = metadata[key];\n        }\n    }\n    return file\n};\n\nvar inheritMetadata = function(o1, o2) {\n    var commonMetadata = {};\n    if (!Array.isArray(o2)) {\n        o2 = [o2]\n    }\n    for (var i = 0; i < o2.length; i++) {\n        var example = o2[i]['metadata'];\n        for (var key in example) {\n            if (i == 0)\n                commonMetadata[key] = example[key];\n            else {\n                if (!(commonMetadata[key] == example[key])) {\n                    delete commonMetadata[key]\n                }\n            }\n        }\n    }\n    if (!Array.isArray(o1)) {\n        o1 = setMetadata(o1, commonMetadata)\n    } else {\n        for (var i = 0; i < o1.length; i++) {\n            o1[i] = setMetadata(o1[i], commonMetadata)\n        }\n    }\n    return o1;\n};\n\nvar toArray = function(file) {\n    return [].concat(file);\n};\n\nvar groupBy = function(files, key) {\n    var groupedFiles = [];\n    var tempDict = {};\n    for (var i = 0; i < files.length; i++) {\n        var value = files[i]['metadata'][key];\n        if (value in tempDict)\n            tempDict[value].push(files[i]);\n        else tempDict[value] = [files[i]];\n    }\n    for (var key in tempDict) {\n        groupedFiles.push(tempDict[key]);\n    }\n    return groupedFiles;\n};\n\nvar orderBy = function(files, key, order) {\n    var compareFunction = function(a, b) {\n        if (a['metadata'][key].constructor === Number) {\n            return a['metadata'][key] - b['metadata'][key];\n        } else {\n            var nameA = a['metadata'][key].toUpperCase();\n            var nameB = b['metadata'][key].toUpperCase();\n            if (nameA < nameB) {\n                return -1;\n            }\n            if (nameA > nameB) {\n                return 1;\n            }\n            return 0;\n        }\n    };\n\n    files = files.sort(compareFunction);\n    if (order == undefined || order == \"asc\")\n        return files;\n    else\n        return files.reverse();\n};"
                  ]
                }
              ],
              "sbg:publisher": "sbg",
              "sbg:cmdPreview": "echo 'Pairing FASTQs!'",
              "sbg:createdBy": "nemanja.vucic",
              "sbg:revisionNotes": "first revision",
              "sbg:project": "bix-demo/sbgtools-demo",
              "sbg:content_hash": "ac92e9d06aa40851f888206ec27ae6a2e9f3ce50321b4ce579f8ad348f51ee555",
              "sbg:toolAuthor": "",
              "sbg:projectName": "SBGTools - Demo New",
              "sbg:id": "h-b30ace5a/h-5e175313/h-abd9599e/0",
              "sbg:modifiedBy": "nemanja.vucic",
              "sbg:revisionsInfo": [
                {
                  "sbg:revisionNotes": null,
                  "sbg:revision": 0,
                  "sbg:modifiedBy": "nemanja.vucic",
                  "sbg:modifiedOn": 1586762691
                },
                {
                  "sbg:revisionNotes": "first revision",
                  "sbg:revision": 1,
                  "sbg:modifiedBy": "nemanja.vucic",
                  "sbg:modifiedOn": 1586762729
                }
              ],
              "sbg:contributors": [
                "nemanja.vucic"
              ],
              "sbg:createdOn": 1586762691,
              "sbg:appVersion": [
                "v1.0"
              ],
              "sbg:latestRevision": 1,
              "sbg:validationErrors": [],
              "sbg:license": "Apache License 2.0",
              "sbg:image_url": null,
              "sbg:sbgMaintained": false,
              "sbg:revision": 1,
              "sbg:toolkit": "SBGTools",
              "sbg:modifiedOn": 1586762729,
              "sbg:categories": [
                "Converters",
                "Other"
              ]
            },
            "label": "SBG Pair FASTQs by Metadata",
            "sbg:x": 284.5937805175781,
            "sbg:y": 192.609375
          },
          {
            "id": "salmon_index_1_2_0",
            "in": [
              {
                "id": "in_transcriptome_or_index",
                "source": "in_transcriptome_or_index"
              },
              {
                "id": "kmer_len",
                "source": "kmer_len"
              },
              {
                "id": "gencode",
                "source": "gencode"
              },
              {
                "id": "keep_duplicates",
                "source": "keep_duplicates"
              },
              {
                "id": "in_reference_genome",
                "source": "in_reference_genome"
              },
              {
                "id": "threads",
                "default": 36
              },
              {
                "id": "in_annotation",
                "source": "in_annotation"
              }
            ],
            "out": [
              {
                "id": "out_index"
              },
              {
                "id": "out_tx2gene"
              }
            ],
            "run": {
              "class": "CommandLineTool",
              "cwlVersion": "v1.0",
              "$namespaces": {
                "sbg": "https://sevenbridges.com"
              },
              "id": "h-077115a8/h-bb84cfeb/h-69dfc47c/0",
              "baseCommand": [],
              "inputs": [
                {
                  "description": "Transcript FASTA file, or an already generated Salmon index archive.",
                  "sbg:category": "Options",
                  "id": "in_transcriptome_or_index",
                  "type": "File",
                  "label": "Transcript FASTA or Salmon Index",
                  "doc": "Transcript FASTA file, or an already generated Salmon index archive.",
                  "sbg:fileTypes": "FA, FASTA, FA.GZ, FASTA.GZ, TAR"
                },
                {
                  "sbg:toolDefaultValue": "31",
                  "sbg:altPrefix": "--kmerLen",
                  "description": "The size of k-mers that should be used for the quasi index. K-mer length should be an odd number.",
                  "sbg:category": "Options",
                  "id": "kmer_len",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "-k",
                    "shellQuote": false,
                    "position": 6
                  },
                  "label": "K-mer length",
                  "doc": "The size of k-mers that should be used for the quasi index. K-mer length should be an odd number."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "This flag will expect the input transcript FASTA to be in GENCODE format and will split the transcript name at the first '|' character. These reduced names will be used in the output and when looking for these transcripts in a gene to transcript GTF.",
                  "sbg:category": "Options",
                  "id": "gencode",
                  "type": "boolean?",
                  "label": "Gencode FASTA",
                  "doc": "This flag will expect the input transcript FASTA to be in GENCODE format and will split the transcript name at the first '|' character. These reduced names will be used in the output and when looking for these transcripts in a gene to transcript GTF."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "Build the index using a sparse sampling of k-mer positions This will require less memory (especially during quantification), but will take longer to construct and can slow down mapping / alignment.",
                  "sbg:category": "Options",
                  "id": "sparse",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--sparse",
                    "shellQuote": false,
                    "position": 7
                  },
                  "label": "Sparse",
                  "doc": "Build the index using a sparse sampling of k-mer positions This will require less memory (especially during quantification), but will take longer to construct and can slow down mapping / alignment."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "description": "This flag will disable the default indexing behavior of discarding sequence-identical duplicate transcripts. If this flag is passed, then duplicate transcripts that appear in the input will be retained and quantified separately.",
                  "sbg:category": "Options",
                  "id": "keep_duplicates",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--keepDuplicates",
                    "shellQuote": false,
                    "position": 8
                  },
                  "label": "Keep duplicates",
                  "doc": "This flag will disable the default indexing behavior of discarding sequence-identical duplicate transcripts. If this flag is passed, then duplicate transcripts that appear in the input will be retained and quantified separately."
                },
                {
                  "description": "Treat these sequences as decoys that may have sequence homologous to some known transcript.",
                  "sbg:category": "Options",
                  "id": "in_reference_genome",
                  "type": "File?",
                  "label": "Genome FASTA",
                  "doc": "Provide genome FASTA file to generate decoy sequences and combine genome and transcriptome reference used for selective alignment.",
                  "sbg:fileTypes": "FA, FASTA, FA.GZ, FASTA.GZ, TSV"
                },
                {
                  "sbg:toolDefaultValue": "1000",
                  "description": "Amount of RAM memory to be used per job. Defaults to 1000MB for Single threaded jobs,and all of the available memory on the instance for multi-threaded jobs.",
                  "sbg:category": "Platform options",
                  "id": "mem_per_job",
                  "type": "int?",
                  "label": "Memory per job",
                  "doc": "Amount of RAM memory to be used per job. Defaults to 1000MB for Single threaded jobs,and all of the available memory on the instance for multi-threaded jobs."
                },
                {
                  "sbg:toolDefaultValue": "1",
                  "description": "Number of CPUs to be used per job.",
                  "sbg:category": "Platform options",
                  "id": "cpu_per_job",
                  "type": "int?",
                  "label": "CPU per job",
                  "doc": "Number of CPUs to be used per job."
                },
                {
                  "sbg:toolDefaultValue": "2",
                  "sbg:altPrefix": "--threads",
                  "description": "The number of threads to use concurrently.",
                  "sbg:category": "Options",
                  "id": "threads",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "-p",
                    "shellQuote": false,
                    "position": 9
                  },
                  "label": "Number of threads",
                  "doc": "Number of threads to use during indexing."
                },
                {
                  "sbg:toolDefaultValue": "-1",
                  "sbg:altPrefix": "--filterSize",
                  "description": "The size of the Bloom filter that will be used by TwoPaCo during indexing. The filter will beof size 2^{filterSize}. The default value of -1 means that the filter size will be automatically set based on the number of distinct k-mers in the input, as estimated by nthll.",
                  "sbg:category": "Options",
                  "id": "filter_size",
                  "type": "int?",
                  "inputBinding": {
                    "prefix": "--filterSize",
                    "shellQuote": false,
                    "position": 10
                  },
                  "label": "Filter size",
                  "doc": "The size of the Bloom filter that will be used by TwoPaCo during indexing. The filter will beof size 2^{filterSize}. The default value of -1 means that the filter size will be automatically set based on the number of distinct k-mers in the input, as estimated by nthll."
                },
                {
                  "sbg:category": "Additional options",
                  "id": "in_annotation",
                  "type": "File?",
                  "label": "GTF annotation",
                  "doc": "GTF annotation file used for mapping transcripts to genes.",
                  "sbg:fileTypes": "GTF, GTF.GZ"
                },
                {
                  "sbg:toolDefaultValue": "Na",
                  "sbg:category": "Additional options",
                  "id": "out_prefix",
                  "type": "string?",
                  "label": "Output file prefix",
                  "doc": "Output file prefix."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "sbg:category": "Additional options",
                  "id": "no_tx2gene",
                  "type": "boolean?",
                  "label": "Do not create transcripts to genes mappings",
                  "doc": "Do not create transcripts to genes mappings file."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "sbg:category": "Options",
                  "id": "features",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--features",
                    "shellQuote": false,
                    "position": 10
                  },
                  "label": "Reference in the TSV format",
                  "doc": "This flag will expect the input reference to be in the tsv file format, and will split the feature name at the first 'tab' character. These reduced names will be used in the output and when looking for the sequence of the features GTF."
                },
                {
                  "sbg:toolDefaultValue": "off",
                  "sbg:category": "Options",
                  "id": "keep_fixed_fasta",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--keepFixedFasta",
                    "shellQuote": false,
                    "position": 10
                  },
                  "label": "Keep fixed fasta",
                  "doc": "Retain the fixed fasta file (without short transcripts and duplicates, clipped, etc.) generated during indexing."
                }
              ],
              "outputs": [
                {
                  "id": "out_index",
                  "doc": "Folder containing the indices from the specified alignment process (quasi or SMEM). To be used by Salmon Quant tool.",
                  "label": "Salmon index archive",
                  "type": "File?",
                  "outputBinding": {
                    "glob": "*tar",
                    "outputEval": "${\n    if(inputs.in_transcriptome_or_index){\n        //inheritMetadata(self, inputs.in_transcriptome_or_index)\n            var inheritMetadata = function(o1, o2) {\n            var commonMetadata = {};\n            if (!Array.isArray(o2)) {\n                o2 = [o2]\n            }\n            for (var i = 0; i < o2.length; i++) {\n                var example = o2[i]['metadata'];\n                for (var key in example) {\n                    if (i == 0)\n                    commonMetadata[key] = example[key];\n                    else {\n                        if (!(commonMetadata[key] == example[key])) {\n                            delete commonMetadata[key]\n                            }\n                        }\n                    }\n                }\n            if (!Array.isArray(o1)) {\n                o1 = setMetadata(o1, commonMetadata)\n            } else {\n                for (var i = 0; i < o1.length; i++) {\n                    o1[i] = setMetadata(o1[i], commonMetadata)\n                    }\n                }\n            return o1;\n        };\n        \n        var setMetadata = function(file, metadata) {\n            if (!('metadata' in file))\n                file['metadata'] = metadata;\n            else {       \n                for (var key in metadata) {\n                    file['metadata'][key] = metadata[key];\n                }\n            }\n            return file\n        };\n        \n        var ref_ext = inputs.in_transcriptome_or_index.nameext.toLowerCase();\n        if (ref_ext == '.gz') {\n            var str = inputs.in_transcriptome_or_index.nameroot.split('.').slice(0,-1).join('.')\n        }\n        else if(ref_ext !== '.tar'){\n            var str = inputs.in_transcriptome_or_index.nameroot\n        }\n        \n        if (ref_ext == '.tar') {\n         index_dir = inputs.in_transcriptome_or_index.metadata.index_name\n        } else {\n         index_dir = str + \"_salmon_index\"\n        }\n        var index_name = \n        {\n        \"index_name\": index_dir,\n        };\n        \n        self[0] = inheritMetadata(self[0], inputs.in_transcriptome_or_index)\n        return setMetadata(self[0], index_name)\n    }\n}"
                  },
                  "sbg:fileTypes": "TAR",
                  "description": "Folder containing the indices from the specified alignment process (quasi or SMEM). To be used by Salmon Quant tool."
                },
                {
                  "id": "out_tx2gene",
                  "doc": "File containing transcript to gene mappings.",
                  "label": "Transcript to gene mappings",
                  "type": "File?",
                  "outputBinding": {
                    "glob": "*.tsv",
                    "outputEval": "${\n    if(inputs.in_transcriptome_or_index){\n        return inheritMetadata(self, inputs.in_transcriptome_or_index)\n    }\n}"
                  },
                  "sbg:fileTypes": "TSV"
                }
              ],
              "doc": "**Salmon Index** tool builds an index necessary for the **Salmon Quant** and **Salmon Alevin** tools. To create an index, it uses a transcriptome reference file in FASTA format. Additionally, one can provide genome reference along with transcriptome to create a hybrid index compatible with the improved mapping algorithm named **Selective Alignment (SA)**.\n\n**SA** is designed to remain as fast as quasi-mapping while simultaneously eliminating many of its mapping errors. It relies upon alignment scoring to help differentiate between mapping loci that would otherwise be indistinguishable due to the multiple exact matches along the reference. Also, the improved mapping algorithm addresses the failure of direct alignment against the transcriptome, compared to spliced alignment to the genome by identifying and extracting sequence-similar decoy regions or using the entire genome as a decoy. The Salmon index is then augmented with these decoy sequences, which are handled in a special manner during mapping and alignment scoring, leading to a reduction of false mappings [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\nA **Transcript FASTA or Salmon Index** needs to be provided as an input to the tool.\nIt is recommended to provide **Genome FASTA** to be used as a *decoy* sequence when generating the index. \nTo create a file containing transcripts to genes mappings, required for scRNA-seq analysis with **Salmon Alevin** and gene-level abundance estimation with **Salmon Quant** tools, provide a GTF annotation file to the **GTF annotation** input port.\n\n### Changes Introduced by Seven Bridges\n\n - An already generated **Salmon index archive** can be provided to the **Salmon Index** tool (**Transcript FASTA or Salmon Index Archive** input), to skip indexing and reduce processing time if this tool is a part of a workflow.\n - Included bash scripts for extracting target names from genome reference, concatenation of transcriptome and genome reference and for creating transcripts to genes mappings file (if any ERCC contigs are present in the genome file, they will not be included in the list of decoy sequences).\n - Parameter `--gencode` will be automatically included in the command line if the transcriptome fasta file contains `gencode` string in the filename.\n\n### Common Issues and Important Notes\n\nThere are no common issues and important notes concerning this tool.\n\n### Performance Benchmarking\n\nWhen provided with transcriptome reference only, the **Salmon Index** tool builds the index structure for **Salmon** in a short time. The expected time of task execution in such cases is under  5 minutes, with a cost of $0.05 on the default c5.2xlarge instance (AWS).\nOn the other hand, if genome reference is provided along with transcriptome, the expected time of task execution can be as long as 40 minutes with the cost of $0.66 on the c5.4xlarge instance (AWS) having 16 CPUs.\n\n*Note:* We used human reference genome and transcriptome for benchmarking purposes. Execution time and cost can vary for other genomes and transcriptomes.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### References\n\n[1] [Salmon paper](https://www.biorxiv.org/content/10.1101/657874v2)   \n[2] [Towards selective-alignment](https://www.biorxiv.org/content/10.1101/138800v2)",
              "label": "Salmon Index",
              "arguments": [
                {
                  "prefix": "",
                  "shellQuote": false,
                  "position": 0,
                  "valueFrom": "${\n    if(inputs.in_reference_genome){\n        return \"bash createDecoys.sh && bash filter_ercc_from_fasta.sh && \"\n    }\n}"
                },
                {
                  "prefix": "",
                  "shellQuote": false,
                  "position": 1,
                  "valueFrom": "${\n    if(inputs.in_reference_genome){\n        return \"bash mergeReferences.sh && samtools faidx filtered_genome.fa && \"\n    }\n}"
                },
                {
                  "prefix": "",
                  "shellQuote": false,
                  "position": 2,
                  "valueFrom": "${\n    if(inputs.in_transcriptome_or_index){\n      var ref_ext = inputs.in_transcriptome_or_index.nameext.toLowerCase();\n      \n      if (ref_ext == '.fa' || ref_ext == '.fasta' || ref_ext == '.gz') {\n        return \"salmon index\"\n      } else if (ref_ext == '.tar') {\n        return \"echo 'Tar bundle provided, skipping indexing.' \"\n      }\n    } else {\n        return \"echo 'Skipping indexing since no index or transcriptome file is provided.' \"\n    }\n}\n"
                },
                {
                  "prefix": "-t",
                  "shellQuote": false,
                  "position": 3,
                  "valueFrom": "${\n    if(inputs.in_reference_genome){\n        \n        var tr_fasta = [].concat(inputs.in_transcriptome_or_index)[0]\n        var ext = tr_fasta.nameext.toLowerCase();\n        \n        if(ext == '.gz'){\n            return \"gentrome.fa.gz -d decoys.txt\"\n        } \n        else if(ext == '.fasta' || ext == '.fa'){\n            return \"gentrome.fa -d decoys.txt\"\n\n        }\n        \n    } else {\n        return inputs.in_transcriptome_or_index\n    }\n}"
                },
                {
                  "prefix": "-i",
                  "shellQuote": false,
                  "position": 4,
                  "valueFrom": "${// output dir name //\n    if(inputs.in_transcriptome_or_index){\n      var ref_ext = inputs.in_transcriptome_or_index.nameext.toLowerCase();\n      if (ref_ext == '.gz') {\n        var str = inputs.in_transcriptome_or_index.nameroot.split('.').slice(0,-1).join('.')\n      } else {\n        var str = inputs.in_transcriptome_or_index.nameroot\n      }\n      return str + \"_salmon_index\"\n    }\n}"
                },
                {
                  "prefix": "",
                  "shellQuote": false,
                  "position": 5,
                  "valueFrom": "${// gencode FASTA //\n  if (inputs.gencode) {\n    return \"--gencode\"\n  } else if ([].concat(inputs.in_transcriptome_or_index)[0].path.toLowerCase().indexOf('gencode') !== -1) {\n    return \"--gencode\"\n  } else {\n    return \"\"\n  }\n}"
                },
                {
                  "prefix": "",
                  "shellQuote": false,
                  "position": 99,
                  "valueFrom": "${ // tar output dir //\n    if(inputs.in_transcriptome_or_index){\n      var ref_ext = inputs.in_transcriptome_or_index.nameext.toLowerCase();\n      if (ref_ext == '.gz') {\n        var str = inputs.in_transcriptome_or_index.nameroot.split('.').slice(0,-1).join('.')\n      } else {\n        var str = inputs.in_transcriptome_or_index.nameroot\n      }\n      var index_dir = str + \"_salmon_index\"\n\n      if (ref_ext == '.tar') {\n        return \"\"\n      } else {\n\n        if(inputs.out_prefix){\n            return \"&& tar -vcf \" + inputs.out_prefix + \".salmon-1.2.0-index-archive.tar \" + index_dir\n        }\n        else if(inputs.in_reference_genome){\n            return \"&& tar -vcf \" + str + \".gentrome.salmon-1.2.0-index-archive.tar \" + index_dir\n        } else {\n            return \"&& tar -vcf \" + str + \".salmon-1.2.0-index-archive.tar \" + index_dir\n        }\n\n      }\n      \n    } else {\n        return \"echo 'Skipping indexing since no index or transcriptome file is provided.' \"\n    }\n}"
                },
                {
                  "prefix": "",
                  "shellQuote": false,
                  "position": 100,
                  "valueFrom": "${\n    if(inputs.in_annotation && inputs.no_tx2gene != true){\n        return \"&& bash bioawk.sh\"\n    } else {\n        return ''\n    }\n}"
                }
              ],
              "requirements": [
                {
                  "class": "ShellCommandRequirement"
                },
                {
                  "class": "ResourceRequirement",
                  "ramMin": "${\n    if(inputs.mem_per_job){\n        return inputs.mem_per_job\n    } else {\n        return 1000\n    }\n}",
                  "coresMin": "${\n    if(inputs.cpu_per_job){\n        return inputs.cpu_per_job\n    } else {\n        return 16\n    }\n}"
                },
                {
                  "class": "DockerRequirement",
                  "dockerPull": "images.sbgenomics.com/uros_sipetic/salmon:1.2.0"
                },
                {
                  "class": "InitialWorkDirRequirement",
                  "listing": [
                    "$(inputs.in_transcriptome_or_index)",
                    {
                      "entryname": "bioawk.sh",
                      "entry": "${\n    var cat;\n    var cmd;\n    \n    if(inputs.in_annotation){\n        var ext = inputs.in_annotation.nameext.toLowerCase();\n        if(ext == '.gz'){\n            cat = 'zcat'\n        } \n        else if(ext == '.gtf'){\n            cat = 'cat'\n        }\n\n        if (inputs.gencode || [].concat(inputs.in_transcriptome_or_index)[0].path.toLowerCase().indexOf('gencode') !== -1) {\n            var col_num = \"4\"\n        } else {\n            var col_num = \"6\"\n        }\n        \n        var ref_ext = inputs.in_transcriptome_or_index.nameext.toLowerCase();\n        if (ref_ext == '.gz') {\n            var out_prefix = inputs.in_transcriptome_or_index.nameroot.split('.').slice(0,-1).join('.')\n        } else {\n            var out_prefix = inputs.in_transcriptome_or_index.nameroot\n        }\n        \n        var cmd = 'bioawk -c gff \\'$feature==\"transcript\"';\n        cmd += \" {print $group}' <(\"\n        cmd += cat + ' ' + inputs.in_annotation.path\n        cmd += \") | awk -F ' ' '{print substr($\" + col_num + \",2,length($\" + col_num + \")-3) \"\n        cmd += '\"\\\\t\" substr($2,2,length($2)-3)}'\n        cmd += \"' - > \" + out_prefix + \".txp2gene.tsv\"\n        \n        return \"\\\n        #!/bin/bash\\n\\\n        #\\n\\\n        \" + cmd + \"\\n\\\n        \\\n        \"\n    }\n}",
                      "writable": false
                    },
                    {
                      "entryname": "createDecoys.sh",
                      "entry": "${\n    var cat;\n    var cmd;\n    if(inputs.in_reference_genome){\n        var ext = inputs.in_reference_genome.nameext.toLowerCase();\n        if(ext == '.gz'){\n            cat = 'zcat'\n        } \n        else if(ext == '.fasta' || ext == '.fa'){\n            cat = 'cat'\n        }\n    \n        cmd = 'grep \"^>\" <(' + cat + ' ' + inputs.in_reference_genome.path\n        cmd += ') | cut -d \" \" -f 1 > decoys_tmp.txt'\n        cmd += \" && sed -i -e 's/>//g' decoys_tmp.txt\"\n        cmd += \" && awk '!/^ERCC/' decoys_tmp.txt > decoys2.txt\"\n        cmd += \" && awk '!/AABR07022993.1/' decoys2.txt > decoys3.txt\"\n        cmd += \" && awk '!/AABR07023006.1/' decoys3.txt > decoys4.txt\"\n        cmd += \" && awk '!/JCAR017_p10003/' decoys4.txt > decoys5.txt\"\n        cmd += \" && awk '!/BMS_cyLILRA1/' decoys5.txt > decoys.txt\"\n\n\n        return \"\\\n        #!/bin/bash\\n\\\n        #\\n\\\n        \" + cmd + \"\\n\\\n        \\\n        \"\n    }\n}",
                      "writable": false
                    },
                    {
                      "entryname": "mergeReferences.sh",
                      "entry": "${\n    var cmd;\n    if(inputs.in_reference_genome){\n        \n        var tr_fasta = [].concat(inputs.in_transcriptome_or_index)[0]\n        var ext = tr_fasta.nameext.toLowerCase();\n        \n        if(ext == '.gz'){\n            var cat = 'zcat'\n            var filtered_fasta = 'filtered_genome.fa.gz'\n            var gentrome = 'gentrome.fa.gz'\n        } \n        else if(ext == '.fasta' || ext == '.fa'){\n            var cat = 'cat'\n            var filtered_fasta = 'filtered_genome.fa'\n            var gentrome = 'gentrome.fa'\n        }\n        \n        cmd = cat + ' ' + inputs.in_transcriptome_or_index.path + ' ' + filtered_fasta + ' > ' + gentrome\n        \n        return \"\\\n        #!/bin/bash\\n\\\n        #\\n\\\n        \" + cmd + \"\\n\\\n        \\\n        \"\n    }\n}",
                      "writable": false
                    },
                    {
                      "entryname": "filter_ercc_from_fasta.sh",
                      "entry": "${\n    var cmd;\n    var fasta = [].concat(inputs.in_reference_genome)[0]\n    if(fasta){\n        var ext = fasta.nameext.toLowerCase();\n        if(ext == '.gz'){\n            var cat = 'zcat'\n        } \n        else if(ext == '.fasta' || ext == '.fa'){\n            var cat = 'cat'\n        }\n    \n        cmd = \"sed -e 's/^/>/' decoys.txt > edited_decoys.txt && \" + cat + \" \" + fasta.path + \" | awk '{ if ((NR>1)&&($0~/^>/)) { printf(\\\"\\\\n%s\\\", $0); } else if (NR==1) { printf(\\\"%s\\\", $0); } else { printf(\\\"\\\\t%s\\\", $0); } }' - | grep -Fwf edited_decoys.txt - | tr \\\"\\\\t\\\" \\\"\\\\n\\\" > filtered_genome.fa\"\n        \n        var transcriptome_fasta = [].concat(inputs.in_transcriptome_or_index)[0]\n        var tr_ext = transcriptome_fasta.nameext.toLowerCase();\n        if(tr_ext == '.gz'){\n            cmd += ' && gzip -k filtered_genome.fa'\n        } \n        \n        return \"\\\n        #!/bin/bash\\n\\\n        #\\n\\\n        \" + cmd + \"\\n\\\n        \\\n        \"\n    }\n}",
                      "writable": false
                    }
                  ]
                },
                {
                  "class": "InlineJavascriptRequirement",
                  "expressionLib": [
                    "var updateMetadata = function(file, key, value) {\n    file['metadata'][key] = value;\n    return file;\n};\n\n\nvar setMetadata = function(file, metadata) {\n    if (!('metadata' in file))\n        file['metadata'] = metadata;\n    else {\n        for (var key in metadata) {\n            file['metadata'][key] = metadata[key];\n        }\n    }\n    return file\n};\n\nvar inheritMetadata = function(o1, o2) {\n    var commonMetadata = {};\n    if (!Array.isArray(o2)) {\n        o2 = [o2]\n    }\n    for (var i = 0; i < o2.length; i++) {\n        var example = o2[i]['metadata'];\n        for (var key in example) {\n            if (i == 0)\n                commonMetadata[key] = example[key];\n            else {\n                if (!(commonMetadata[key] == example[key])) {\n                    delete commonMetadata[key]\n                }\n            }\n        }\n    }\n    if (!Array.isArray(o1)) {\n        o1 = setMetadata(o1, commonMetadata)\n    } else {\n        for (var i = 0; i < o1.length; i++) {\n            o1[i] = setMetadata(o1[i], commonMetadata)\n        }\n    }\n    return o1;\n};\n\nvar toArray = function(file) {\n    return [].concat(file);\n};\n\nvar groupBy = function(files, key) {\n    var groupedFiles = [];\n    var tempDict = {};\n    for (var i = 0; i < files.length; i++) {\n        var value = files[i]['metadata'][key];\n        if (value in tempDict)\n            tempDict[value].push(files[i]);\n        else tempDict[value] = [files[i]];\n    }\n    for (var key in tempDict) {\n        groupedFiles.push(tempDict[key]);\n    }\n    return groupedFiles;\n};\n\nvar orderBy = function(files, key, order) {\n    var compareFunction = function(a, b) {\n        if (a['metadata'][key].constructor === Number) {\n            return a['metadata'][key] - b['metadata'][key];\n        } else {\n            var nameA = a['metadata'][key].toUpperCase();\n            var nameB = b['metadata'][key].toUpperCase();\n            if (nameA < nameB) {\n                return -1;\n            }\n            if (nameA > nameB) {\n                return 1;\n            }\n            return 0;\n        }\n    };\n\n    files = files.sort(compareFunction);\n    if (order == undefined || order == \"asc\")\n        return files;\n    else\n        return files.reverse();\n};",
                    "\nvar setMetadata = function(file, metadata) {\n    if (!('metadata' in file)) {\n        file['metadata'] = {}\n    }\n    for (var key in metadata) {\n        file['metadata'][key] = metadata[key];\n    }\n    return file\n};\nvar inheritMetadata = function(o1, o2) {\n    var commonMetadata = {};\n    if (!o2) {\n        return o1;\n    };\n    if (!Array.isArray(o2)) {\n        o2 = [o2]\n    }\n    for (var i = 0; i < o2.length; i++) {\n        var example = o2[i]['metadata'];\n        for (var key in example) {\n            if (i == 0)\n                commonMetadata[key] = example[key];\n            else {\n                if (!(commonMetadata[key] == example[key])) {\n                    delete commonMetadata[key]\n                }\n            }\n        }\n        for (var key in commonMetadata) {\n            if (!(key in example)) {\n                delete commonMetadata[key]\n            }\n        }\n    }\n    if (!Array.isArray(o1)) {\n        o1 = setMetadata(o1, commonMetadata)\n    } else {\n        for (var i = 0; i < o1.length; i++) {\n            o1[i] = setMetadata(o1[i], commonMetadata)\n        }\n    }\n    return o1;\n};"
                  ]
                }
              ],
              "hints": [
                {
                  "class": "sbg:SaveLogs",
                  "value": "decoys.txt"
                },
                {
                  "class": "sbg:SaveLogs",
                  "value": "filtered_genome.fa.fai"
                },
                {
                  "class": "sbg:SaveLogs",
                  "value": "createDecoys.sh"
                },
                {
                  "class": "sbg:SaveLogs",
                  "value": "mergeReferences.sh"
                },
                {
                  "class": "sbg:SaveLogs",
                  "value": "filter_ercc_from_fasta.sh"
                }
              ],
              "sbg:project": "nemanja.vucic/salmon-v1-2-0-demo",
              "sbg:publisher": "sbg",
              "sbg:toolkitVersion": "1.2.0",
              "sbg:image_url": null,
              "sbg:createdBy": "nemanja.vucic",
              "sbg:revisionNotes": "in_transcriptome_or_index set as required",
              "sbg:toolAuthor": "Rob Patro, Avi Srivastava",
              "sbg:links": [
                {
                  "id": "http://combine-lab.github.io/salmon/",
                  "label": "Salmon Homepage"
                },
                {
                  "id": "https://github.com/COMBINE-lab/salmon",
                  "label": "Salmon Source Code"
                },
                {
                  "id": "https://github.com/COMBINE-lab/salmon/releases/tag/v1.2.0",
                  "label": "Salmon Download"
                },
                {
                  "id": "https://www.biorxiv.org/content/10.1101/657874v2",
                  "label": "Salmon Publications"
                },
                {
                  "id": "http://salmon.readthedocs.org/en/latest/",
                  "label": "Salmon Documentation"
                }
              ],
              "sbg:content_hash": "ac6a2fbdaa00733e72db594b7438aec55f0dd9ae82ce7efd17a608d9694bc50c8",
              "sbg:projectName": "Salmon v1.2.0 - Demo",
              "sbg:id": "h-077115a8/h-bb84cfeb/h-69dfc47c/0",
              "sbg:modifiedBy": "nemanja.vucic",
              "sbg:contributors": [
                "nemanja.vucic"
              ],
              "sbg:createdOn": 1586781314,
              "sbg:wrapperAuthor": "nemanja.vucic",
              "sbg:appVersion": [
                "v1.0"
              ],
              "sbg:expand_workflow": false,
              "sbg:validationErrors": [],
              "sbg:license": "GNU General Public License v3.0",
              "sbg:latestRevision": 4,
              "sbg:revisionsInfo": [
                {
                  "sbg:revisionNotes": null,
                  "sbg:revision": 0,
                  "sbg:modifiedBy": "nemanja.vucic",
                  "sbg:modifiedOn": 1586781314
                },
                {
                  "sbg:revisionNotes": "copied revision 12 from dev",
                  "sbg:revision": 1,
                  "sbg:modifiedBy": "nemanja.vucic",
                  "sbg:modifiedOn": 1586781356
                },
                {
                  "sbg:revisionNotes": "fix for compressed and uncompressed files inconsistencies",
                  "sbg:revision": 2,
                  "sbg:modifiedBy": "nemanja.vucic",
                  "sbg:modifiedOn": 1590695220
                },
                {
                  "sbg:revisionNotes": "updated description to include gencode parameter",
                  "sbg:revision": 3,
                  "sbg:modifiedBy": "nemanja.vucic",
                  "sbg:modifiedOn": 1590696306
                },
                {
                  "sbg:revisionNotes": "in_transcriptome_or_index set as required",
                  "sbg:revision": 4,
                  "sbg:modifiedBy": "nemanja.vucic",
                  "sbg:modifiedOn": 1590743964
                }
              ],
              "sbg:sbgMaintained": false,
              "sbg:revision": 4,
              "sbg:toolkit": "Salmon",
              "sbg:modifiedOn": 1590743964,
              "sbg:categories": [
                "Transcriptomics",
                "Quantification",
                "CWL1.0"
              ]
            },
            "label": "Salmon Index",
            "sbg:x": 284.5937805175781,
            "sbg:y": 320.3984375
          },
          {
            "id": "sbg_create_rsem_tpm_counts_matrix_tx",
            "in": [
              {
                "id": "column_name",
                "default": [
                  "numreads"
                ],
                "source": [
                  "column_name"
                ]
              },
              {
                "id": "output_name",
                "default": "expression.matrix.tx",
                "source": "output_name"
              },
              {
                "id": "abundance_estimates",
                "source": [
                  "salmon_quant_reads_1_2_0/out_quant_sf"
                ]
              },
              {
                "id": "add_tsv_suffix",
                "default": true
              }
            ],
            "out": [
              {
                "id": "expression_matrix"
              }
            ],
            "run": {
              "class": "CommandLineTool",
              "cwlVersion": "v1.0",
              "$namespaces": {
                "sbg": "https://sevenbridges.com"
              },
              "id": "h-0bd6d308/h-8000e7ea/h-055eb2b5/0",
              "baseCommand": [],
              "inputs": [
                {
                  "sbg:toolDefaultValue": "'tpm'",
                  "sbg:category": "Options",
                  "id": "column_name",
                  "type": [
                    "null",
                    {
                      "type": "array",
                      "items": "string",
                      "inputBinding": {
                        "separate": true
                      }
                    }
                  ],
                  "inputBinding": {
                    "prefix": "--column_name",
                    "itemSeparator": ",",
                    "shellQuote": false,
                    "position": 4,
                    "valueFrom": "${\n    if (self == 0) {\n        self = null;\n        inputs.column_name = null\n    };\n\n\n    return inputs.column_name ? inputs.column_name : 'tpm'\n}"
                  },
                  "label": "Column name",
                  "doc": "Column name chose to aggregate results over."
                },
                {
                  "sbg:toolDefaultValue": "expression_matrix",
                  "sbg:category": "Options",
                  "id": "output_name",
                  "type": "string?",
                  "inputBinding": {
                    "prefix": "-o",
                    "shellQuote": false,
                    "position": 3,
                    "valueFrom": "${\n  if (inputs.abundance_estimates) {\n    if (self == 0) {\n        self = null;\n        inputs.output_name = null\n    };\n    \n    function flatten(files){\n      var a = []\n  \t  for(var i=0;i<files.length;i++){\n        if(files[i]){\n          if(files[i].constructor == Array) {\n            a = a.concat(flatten(files[i]))\n          } else {\n            a = a.concat(files[i])\n          }\n        }\n      }\n      \n      var b = a.filter(function (el) {\n        return el != null\n      })\n      \n      return b\n    }\n    \n    var tmp_1 = [].concat(inputs.abundance_estimates)\n    var tmp = flatten(tmp_1)\n\n    if (tmp[0] && tmp[0].path) {\n        var x = tmp\n        if (inputs.output_name) {\n            var suffix = inputs.output_name\n        } else {\n            var suffix = 'expression_matrix'\n        }\n        return suffix\n    }\n  }\n}"
                  },
                  "label": "Output file name",
                  "doc": "Name of the outputted counts matrix file.",
                  "default": 0
                },
                {
                  "sbg:category": "Inputs",
                  "id": "abundance_estimates",
                  "type": [
                    "null",
                    {
                      "type": "array",
                      "items": "File",
                      "inputBinding": {
                        "separate": true
                      }
                    }
                  ],
                  "inputBinding": {
                    "prefix": "",
                    "itemSeparator": ",",
                    "shellQuote": false,
                    "position": 1,
                    "valueFrom": "${\n  function flatten(files){\n      var a = []\n  \t  for(var i=0;i<files.length;i++){\n        if(files[i]){\n          if(files[i].constructor == Array) {\n            a = a.concat(flatten(files[i]))\n          } else {\n            a = a.concat(files[i])\n          }\n        }\n      }\n      \n      var b = a.filter(function (el) {\n        return el != null\n      })\n      \n      return b\n    }\n    \n    var tmp_1 = [].concat(inputs.abundance_estimates)\n    var tmp = flatten(tmp_1)\n    \n  if (tmp[0] && tmp[0].path) {\n  var x = tmp\n  var cmd = '-i '\n  if (inputs.star) {\n    for (var i=0;i<x.length;i++) {\n      var tmp = x[i].path.split('/').pop().split('.')\n      cmd += tmp[0] + '.reheaded.' + tmp.slice(1,tmp.length).join('.')+ \",\"\n    }\n  } else {\n    for (var i=0;i<x.length;i++) {\n      cmd += x[i].path + \",\"\n    }\n  }\n  return cmd.slice(0,cmd.length-1)\n  }\n}\n\n"
                  },
                  "label": "Abundance estimates",
                  "doc": "Abundance estimates generated by tools like RSEM, STAR, Kallisto or Salmon.",
                  "sbg:fileTypes": "RESULTS, SF, TSV, TAB"
                },
                {
                  "sbg:toolDefaultValue": "False",
                  "sbg:category": "Inputs",
                  "id": "star",
                  "type": "boolean?",
                  "inputBinding": {
                    "shellQuote": false,
                    "position": 0
                  },
                  "label": "STAR input files",
                  "doc": "If aggregating STAR count results, set this option to True. This will essentially parse the input STAR count file to get rid of the header, and add the following column names: 'unstranded', '1st_strand', '2nd_strand'. Use these column names in the \"Column name\" option to aggregate the results over them."
                },
                {
                  "sbg:toolDefaultValue": "Gene_or_Transcript_ID",
                  "sbg:category": "Inputs",
                  "id": "feature_name",
                  "type": "string?",
                  "inputBinding": {
                    "prefix": "--feature_name",
                    "shellQuote": false,
                    "position": 5
                  },
                  "label": "Feature name.",
                  "doc": "Feature name, i.e. \"gene\" or \"isoform\", to be used in the header."
                },
                {
                  "id": "input",
                  "type": "File?",
                  "inputBinding": {
                    "shellQuote": false,
                    "position": 0
                  }
                },
                {
                  "sbg:toolDefaultValue": "False",
                  "sbg:altPrefix": "-t",
                  "sbg:category": "Options",
                  "id": "add_tsv_suffix",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--tsv",
                    "shellQuote": false,
                    "position": 6
                  },
                  "label": "Add TSV suffix",
                  "doc": "Add '.tsv' suffix to the output filename."
                }
              ],
              "outputs": [
                {
                  "id": "expression_matrix",
                  "doc": "Matrix files(s) containing expression values across all genes/transcripts for multiple provided inputs.",
                  "label": "Expression matrix",
                  "type": "File[]?",
                  "outputBinding": {
                    "glob": "${\n    function flatten(files){\n      var a = []\n  \t  for(var i=0;i<files.length;i++){\n        if(files[i]){\n          if(files[i].constructor == Array) {\n            a = a.concat(flatten(files[i]))\n          } else {\n            a = a.concat(files[i])\n          }\n        }\n      }\n      \n      var b = a.filter(function (el) {\n        return el != null\n      })\n      \n      return b\n    }\n    \n    var tmp_1 = [].concat(inputs.abundance_estimates)\n    var tmp = flatten(tmp_1)\n    \n    \n    if (tmp[0] && tmp[0].path) {\n        var x = tmp\n        if (inputs.output_name) {\n            var prefix = inputs.output_name\n        } else {\n            var prefix = 'expression_matrix'\n        }\n        return prefix + \"*\"\n    }\n}",
                    "outputEval": "${\n    function flatten(files){\n      var a = []\n  \t  for(var i=0;i<files.length;i++){\n        if(files[i]){\n          if(files[i].constructor == Array) {\n            a = a.concat(flatten(files[i]))\n          } else {\n            a = a.concat(files[i])\n          }\n        }\n      }\n      \n      var b = a.filter(function (el) {\n        return el != null\n      })\n      \n      return b\n    }\n    \n    var tmp_1 = [].concat(inputs.abundance_estimates)\n    var tmp = flatten(tmp_1)\n    \n    return inheritMetadata(self, tmp)\n}"
                  }
                }
              ],
              "doc": "This tool takes multiple abundance estimates files outputted by tools like RSEM, Kallisto or Salmon and creates expression matrices, based on the input column that the user specifies (the default is 'tpm', but any other string can be input here, like 'fpkm', 'counts' or similar, or even specifying multiple columns), that can be used for further downstream analysis.\n\nThis tool can also be used to aggregate any kind of results in tab-delimited format and create a matrix like file, it was just originally developed for creating expression matrices. \n\n### Common Issues ###\nNone",
              "label": "SBG Create Expression Matrix CWL1.0",
              "arguments": [
                {
                  "prefix": "",
                  "shellQuote": false,
                  "position": 0,
                  "valueFrom": "${\n  function flatten(files){\n      var a = []\n  \t  for(var i=0;i<files.length;i++){\n        if(files[i]){\n          if(files[i].constructor == Array) {\n            a = a.concat(flatten(files[i]))\n          } else {\n            a = a.concat(files[i])\n          }\n        }\n      }\n      \n      var b = a.filter(function (el) {\n        return el != null\n      })\n      \n      return b\n    }\n    \n    var tmp_1 = [].concat(inputs.abundance_estimates)\n    var tmp = flatten(tmp_1)\n    \n    if (tmp[0] && tmp[0].path) {\n    if (inputs.star) {\n      var x = tmp\n      var y = x[0].path.split('/')\n      y.pop()\n      var z = y.join('/') + '/'\n      return \"python parse_star_counts.py --path \" + z + \" && python create_tpm_matrix.py\"\n    } else {\n      return \"python create_tpm_matrix.py\"\n    }\n  } else {\n      return \"echo 'no inputs provided, skipping analysis'\"\n  }\n}"
                },
                {
                  "prefix": "",
                  "shellQuote": false,
                  "position": 2,
                  "valueFrom": "${\n  if (inputs.abundance_estimates) {\n    function flatten(files){\n      var a = []\n  \t  for(var i=0;i<files.length;i++){\n        if(files[i]){\n          if(files[i].constructor == Array) {\n            a = a.concat(flatten(files[i]))\n          } else {\n            a = a.concat(files[i])\n          }\n        }\n      }\n      \n      var b = a.filter(function (el) {\n        return el != null\n      })\n      \n      return b\n    }\n    \n    var tmp_1 = [].concat(inputs.abundance_estimates)\n    var tmp = flatten(tmp_1)\n    \n    if (tmp[0] && tmp[0].path) {\n        var quants = tmp\n        var samples = []\n        if (quants[0] && quants[0].metadata && quants[0].metadata.sample_id) {\n            for (var i = 0; i < quants.length; i++) {\n                samples = samples.concat(quants[i].metadata.sample_id)\n            }\n        } else {\n            for (var i = 0; i < quants.length; i++) {\n                samples = samples.concat(quants[i].path.split('/').pop().split('.')[0])\n            }\n        }\n        return '--sample_ids ' + samples\n    }\n  }\n}"
                }
              ],
              "requirements": [
                {
                  "class": "ShellCommandRequirement"
                },
                {
                  "class": "ResourceRequirement",
                  "ramMin": 1000,
                  "coresMin": 1
                },
                {
                  "class": "DockerRequirement",
                  "dockerPull": "images.sbgenomics.com/dusan_randjelovic/sci-python:2.7"
                },
                {
                  "class": "InitialWorkDirRequirement",
                  "listing": [
                    {
                      "entryname": "create_tpm_matrix.py",
                      "entry": "import argparse\n\n\ndef parse_quant_file(quant_files, sample_ids, out_name, column_name, feature_name):\n\n    # Open all files for reading and an output file for writing; make the header\n    handles = []\n    writer = open(out_name, 'w')\n    writer.write(feature_name)\n\n    k = 0\n    for item in quant_files:\n        sample_name = sample_ids[k]\n        k = k+1\n        suppl = open(item)\n        header = [x.lower() for x in suppl.next().rstrip().split('\\t')]\n        TPM_column = header.index(column_name)\n        #suppl.next()  # throw away the header\n        writer.write('\\t' + sample_name)\n        handles.append(suppl)\n\n    # Iterate over files and lines\n    eof = False\n    while not eof:\n        writer.write('\\n')\n        for i, suppl in enumerate(handles):\n            try:\n                items = suppl.next().rstrip().split('\\t')\n            except StopIteration:\n                eof = True\n                break\n            if i == 0:\n                writer.write('{}\\t{}'.format(items[0].split('|')[0], items[TPM_column]))\n            else:\n                writer.write('\\t' + items[TPM_column])\n\n    writer.close()\n\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser(description='Parse a set of abundance estimate files into a matrix of count values.')\n    parser.add_argument('--input', '-i', help='Comma-separated list of abundance estimate input files.')\n    parser.add_argument('--sample_ids', '-s', help='Comma-separated list of sample_ids.')\n    parser.add_argument('--out', '-o', help='Output file name.', default='expression_matrix')\n    parser.add_argument('--column_name', '-c', help='Column name chosen to aggregate results over.')\n    parser.add_argument('--feature_name', '-f', help='Feature name, i.e. \"gene\" or \"isoform\", to be used in the header.', required=False)\n    parser.add_argument('--tsv', '-t', help='Add \".tsv\" to the output filename.', action='store_true')\n\n    args = parser.parse_args()\n\n    quant_inputs = args.input.split(',')\n    samples = args.sample_ids.split(',')\n    columns = [x.lower() for x in args.column_name.split(',')]\n\n    if args.feature_name:\n        feature = args.feature_name\n    else:\n        feature = 'Gene_or_Transcript_ID'\n\n    if args.tsv:\n        extension = '.tsv'\n    else:\n        extension = ''\n\n    for column in columns:\n        if args.out:\n            if args.out.endswith('-'):\n                out_name = args.out + column\n            else:\n                out_name = args.out + '.' + column\n        else:\n            out_name = 'expression_matrix.' + column\n        out_name = out_name + extension\n        parse_quant_file(quant_files=quant_inputs, sample_ids=samples, out_name=out_name, column_name = column, feature_name=feature)",
                      "writable": false
                    },
                    "${\n  function flatten(files){\n      var a = []\n  \t  for(var i=0;i<files.length;i++){\n        if(files[i]){\n          if(files[i].constructor == Array) {\n            a = a.concat(flatten(files[i]))\n          } else {\n            a = a.concat(files[i])\n          }\n        }\n      }\n      \n      var b = a.filter(function (el) {\n        return el != null\n      })\n      \n      return b\n    }\n    \n    var tmp_1 = [].concat(inputs.abundance_estimates)\n    var tmp = flatten(tmp_1)\n    \n    if (tmp[0] && tmp[0].path) {\n        return tmp\n    }\n}",
                    {
                      "entryname": "parse_star_counts.py",
                      "entry": "import glob\nimport argparse\n\nparser = argparse.ArgumentParser(description='Testes.')\nparser.add_argument('-p','--path')\nargs = vars(parser.parse_args())\npath = args['path']\nfiles = glob.glob(path + '*.out.tab')\nfor file in files:\n    tmp = file.split('/')[-1].split('.')\n    tmp2 = tmp[0]\n    tmp3 = tmp[1:]\n    output_file = tmp2+'.reheaded.'+\".\".join(tmp3)\n    p = 'Gene_ID\\tunstranded-counts\\t1st_strand-counts\\t2nd_strand-counts\\n'\n    with open(file) as f:\n        for line in f:\n            if not (line.startswith('N_unmapped') or line.startswith('N_multimapping') or line.startswith('N_noFeature') or line.startswith('N_ambiguous')):\n                p += line\n    with open (output_file,\"w\") as f:\n        f.write(p)\n",
                      "writable": false
                    }
                  ]
                },
                {
                  "class": "InlineJavascriptRequirement",
                  "expressionLib": [
                    "var updateMetadata = function(file, key, value) {\n    file['metadata'][key] = value;\n    return file;\n};\n\n\nvar setMetadata = function(file, metadata) {\n    if (!('metadata' in file)) {\n        file['metadata'] = {}\n    }\n    for (var key in metadata) {\n        file['metadata'][key] = metadata[key];\n    }\n    return file\n};\n\nvar inheritMetadata = function(o1, o2) {\n    var commonMetadata = {};\n    if (!Array.isArray(o2)) {\n        o2 = [o2]\n    }\n    for (var i = 0; i < o2.length; i++) {\n        var example = o2[i]['metadata'];\n        for (var key in example) {\n            if (i == 0)\n                commonMetadata[key] = example[key];\n            else {\n                if (!(commonMetadata[key] == example[key])) {\n                    delete commonMetadata[key]\n                }\n            }\n        }\n    }\n    if (!Array.isArray(o1)) {\n        o1 = setMetadata(o1, commonMetadata)\n    } else {\n        for (var i = 0; i < o1.length; i++) {\n            o1[i] = setMetadata(o1[i], commonMetadata)\n        }\n    }\n    return o1;\n};\n\nvar toArray = function(file) {\n    return [].concat(file);\n};\n\nvar groupBy = function(files, key) {\n    var groupedFiles = [];\n    var tempDict = {};\n    for (var i = 0; i < files.length; i++) {\n        var value = files[i]['metadata'][key];\n        if (value in tempDict)\n            tempDict[value].push(files[i]);\n        else tempDict[value] = [files[i]];\n    }\n    for (var key in tempDict) {\n        groupedFiles.push(tempDict[key]);\n    }\n    return groupedFiles;\n};\n\nvar orderBy = function(files, key, order) {\n    var compareFunction = function(a, b) {\n        if (a['metadata'][key].constructor === Number) {\n            return a['metadata'][key] - b['metadata'][key];\n        } else {\n            var nameA = a['metadata'][key].toUpperCase();\n            var nameB = b['metadata'][key].toUpperCase();\n            if (nameA < nameB) {\n                return -1;\n            }\n            if (nameA > nameB) {\n                return 1;\n            }\n            return 0;\n        }\n    };\n\n    files = files.sort(compareFunction);\n    if (order == undefined || order == \"asc\")\n        return files;\n    else\n        return files.reverse();\n};"
                  ]
                }
              ],
              "sbg:publisher": "sbg",
              "sbg:toolkitVersion": "v1.0",
              "sbg:image_url": null,
              "sbg:createdBy": "uros_sipetic",
              "sbg:revisionNotes": "Update tsv boolean param",
              "sbg:project": "bix-demo/sbgtools-demo",
              "sbg:content_hash": "af9eb43909ced15861a7d4f78e93513e9c73de466c176ee545be4bb1e95d85592",
              "sbg:toolAuthor": "Seven Bridges Genomics",
              "sbg:projectName": "SBGTools - Demo New",
              "sbg:id": "h-0bd6d308/h-8000e7ea/h-055eb2b5/0",
              "sbg:modifiedBy": "uros_sipetic",
              "sbg:revisionsInfo": [
                {
                  "sbg:revisionNotes": "Upgraded to v1.0 from bix-demo/sbgtools-demo/sbg-create-rsem-tpm-counts-matrix",
                  "sbg:revision": 0,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1554909464
                },
                {
                  "sbg:revisionNotes": "Add CWL1.0 to name",
                  "sbg:revision": 1,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1554909508
                },
                {
                  "sbg:revisionNotes": "Add CWL1.0 to name, for real this time",
                  "sbg:revision": 2,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1567682542
                },
                {
                  "sbg:revisionNotes": "Add proper staging",
                  "sbg:revision": 3,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1570116631
                },
                {
                  "sbg:revisionNotes": "Update expressions to work in passthrough mode",
                  "sbg:revision": 4,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1581607254
                },
                {
                  "sbg:revisionNotes": "Update output expression to work in passthrough mode",
                  "sbg:revision": 5,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1581607327
                },
                {
                  "sbg:revisionNotes": "Update output_name expression",
                  "sbg:revision": 6,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1581691410
                },
                {
                  "sbg:revisionNotes": "flatten null elements",
                  "sbg:revision": 7,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1582819401
                },
                {
                  "sbg:revisionNotes": "Fix bug in js expression regarding output name forming",
                  "sbg:revision": 8,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1582823156
                },
                {
                  "sbg:revisionNotes": "Update output inherit metadata expression to flatten null elements",
                  "sbg:revision": 9,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1582823896
                },
                {
                  "sbg:revisionNotes": "Generalize for one sample as well",
                  "sbg:revision": 10,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1585847869
                },
                {
                  "sbg:revisionNotes": "Add support for multiple matrices.",
                  "sbg:revision": 11,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1586100092
                },
                {
                  "sbg:revisionNotes": "Add support for STAR",
                  "sbg:revision": 12,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1586781438
                },
                {
                  "sbg:revisionNotes": "fix bug with trailing comma",
                  "sbg:revision": 13,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1586785216
                },
                {
                  "sbg:revisionNotes": "Add TAB input file format. Output is now array of files",
                  "sbg:revision": 14,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1586863294
                },
                {
                  "sbg:revisionNotes": "Make analysis optional",
                  "sbg:revision": 15,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1586943683
                },
                {
                  "sbg:revisionNotes": "Fix bug with lowercase column names",
                  "sbg:revision": 16,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1586963615
                },
                {
                  "sbg:revisionNotes": "python code bug fix",
                  "sbg:revision": 17,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1586964833
                },
                {
                  "sbg:revisionNotes": "do proper passthrough",
                  "sbg:revision": 18,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1586984313
                },
                {
                  "sbg:revisionNotes": "Add feature_name option",
                  "sbg:revision": 19,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1587406550
                },
                {
                  "sbg:revisionNotes": "Make small changes to default output names",
                  "sbg:revision": 20,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1588885679
                },
                {
                  "sbg:revisionNotes": "Update glob",
                  "sbg:revision": 21,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1588925845
                },
                {
                  "sbg:revisionNotes": "feature_name is not required anymore",
                  "sbg:revision": 22,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1590536542
                },
                {
                  "sbg:revisionNotes": "Add tsv param",
                  "sbg:revision": 23,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1590682898
                },
                {
                  "sbg:revisionNotes": "Update tsv boolean param",
                  "sbg:revision": 24,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1590686843
                }
              ],
              "sbg:contributors": [
                "uros_sipetic"
              ],
              "sbg:createdOn": 1554909464,
              "sbg:appVersion": [
                "v1.0"
              ],
              "sbg:latestRevision": 24,
              "sbg:validationErrors": [],
              "sbg:license": "Apache License 2.0",
              "sbg:cmdPreview": "python create_tpm_matrix.py  --sample_ids SAMPLEA,SAMPLEB",
              "sbg:sbgMaintained": false,
              "sbg:revision": 24,
              "sbg:toolkit": "SBGTools",
              "sbg:modifiedOn": 1590686843,
              "sbg:categories": [
                "Utilities",
                "SBGTools",
                "Other"
              ]
            },
            "label": "SBG Create Expression Matrix - Transcripts",
            "sbg:x": 1123.3009033203125,
            "sbg:y": 0
          },
          {
            "id": "sbg_create_rsem_tpm_counts_matrix_genes",
            "in": [
              {
                "id": "column_name",
                "default": [
                  "numreads"
                ],
                "source": [
                  "column_name_1"
                ]
              },
              {
                "id": "output_name",
                "default": "expression.matrix.gene",
                "source": "output_name_1"
              },
              {
                "id": "abundance_estimates",
                "source": [
                  "salmon_quant_reads_1_2_0/out_quant_genes_sf"
                ]
              },
              {
                "id": "add_tsv_suffix",
                "default": true
              }
            ],
            "out": [
              {
                "id": "expression_matrix"
              }
            ],
            "run": {
              "class": "CommandLineTool",
              "cwlVersion": "v1.0",
              "$namespaces": {
                "sbg": "https://sevenbridges.com"
              },
              "id": "h-74763193/h-743193f8/h-fc86b738/0",
              "baseCommand": [],
              "inputs": [
                {
                  "sbg:toolDefaultValue": "'tpm'",
                  "sbg:category": "Options",
                  "id": "column_name",
                  "type": [
                    "null",
                    {
                      "type": "array",
                      "items": "string",
                      "inputBinding": {
                        "separate": true
                      }
                    }
                  ],
                  "inputBinding": {
                    "prefix": "--column_name",
                    "itemSeparator": ",",
                    "shellQuote": false,
                    "position": 4,
                    "valueFrom": "${\n    if (self == 0) {\n        self = null;\n        inputs.column_name = null\n    };\n\n\n    return inputs.column_name ? inputs.column_name : 'tpm'\n}"
                  },
                  "label": "Column name",
                  "doc": "Column name chose to aggregate results over."
                },
                {
                  "sbg:toolDefaultValue": "expression_matrix",
                  "sbg:category": "Options",
                  "id": "output_name",
                  "type": "string?",
                  "inputBinding": {
                    "prefix": "-o",
                    "shellQuote": false,
                    "position": 3,
                    "valueFrom": "${\n  if (inputs.abundance_estimates) {\n    if (self == 0) {\n        self = null;\n        inputs.output_name = null\n    };\n    \n    function flatten(files){\n      var a = []\n  \t  for(var i=0;i<files.length;i++){\n        if(files[i]){\n          if(files[i].constructor == Array) {\n            a = a.concat(flatten(files[i]))\n          } else {\n            a = a.concat(files[i])\n          }\n        }\n      }\n      \n      var b = a.filter(function (el) {\n        return el != null\n      })\n      \n      return b\n    }\n    \n    var tmp_1 = [].concat(inputs.abundance_estimates)\n    var tmp = flatten(tmp_1)\n\n    if (tmp[0] && tmp[0].path) {\n        var x = tmp\n        if (inputs.output_name) {\n            var suffix = inputs.output_name\n        } else {\n            var suffix = 'expression_matrix'\n        }\n        return suffix\n    }\n  }\n}"
                  },
                  "label": "Output file name",
                  "doc": "Name of the outputted counts matrix file.",
                  "default": 0
                },
                {
                  "sbg:category": "Inputs",
                  "id": "abundance_estimates",
                  "type": [
                    "null",
                    {
                      "type": "array",
                      "items": "File",
                      "inputBinding": {
                        "separate": true
                      }
                    }
                  ],
                  "inputBinding": {
                    "prefix": "",
                    "itemSeparator": ",",
                    "shellQuote": false,
                    "position": 1,
                    "valueFrom": "${\n  function flatten(files){\n      var a = []\n  \t  for(var i=0;i<files.length;i++){\n        if(files[i]){\n          if(files[i].constructor == Array) {\n            a = a.concat(flatten(files[i]))\n          } else {\n            a = a.concat(files[i])\n          }\n        }\n      }\n      \n      var b = a.filter(function (el) {\n        return el != null\n      })\n      \n      return b\n    }\n    \n    var tmp_1 = [].concat(inputs.abundance_estimates)\n    var tmp = flatten(tmp_1)\n    \n  if (tmp[0] && tmp[0].path) {\n  var x = tmp\n  var cmd = '-i '\n  if (inputs.star) {\n    for (var i=0;i<x.length;i++) {\n      var tmp = x[i].path.split('/').pop().split('.')\n      cmd += tmp[0] + '.reheaded.' + tmp.slice(1,tmp.length).join('.')+ \",\"\n    }\n  } else {\n    for (var i=0;i<x.length;i++) {\n      cmd += x[i].path + \",\"\n    }\n  }\n  return cmd.slice(0,cmd.length-1)\n  }\n}\n\n"
                  },
                  "label": "Abundance estimates",
                  "doc": "Abundance estimates generated by tools like RSEM, STAR, Kallisto or Salmon.",
                  "sbg:fileTypes": "RESULTS, SF, TSV, TAB"
                },
                {
                  "sbg:toolDefaultValue": "False",
                  "sbg:category": "Inputs",
                  "id": "star",
                  "type": "boolean?",
                  "inputBinding": {
                    "shellQuote": false,
                    "position": 0
                  },
                  "label": "STAR input files",
                  "doc": "If aggregating STAR count results, set this option to True. This will essentially parse the input STAR count file to get rid of the header, and add the following column names: 'unstranded', '1st_strand', '2nd_strand'. Use these column names in the \"Column name\" option to aggregate the results over them."
                },
                {
                  "sbg:toolDefaultValue": "Gene_or_Transcript_ID",
                  "sbg:category": "Inputs",
                  "id": "feature_name",
                  "type": "string?",
                  "inputBinding": {
                    "prefix": "--feature_name",
                    "shellQuote": false,
                    "position": 5
                  },
                  "label": "Feature name.",
                  "doc": "Feature name, i.e. \"gene\" or \"isoform\", to be used in the header."
                },
                {
                  "id": "input",
                  "type": "File?",
                  "inputBinding": {
                    "shellQuote": false,
                    "position": 0
                  }
                },
                {
                  "sbg:toolDefaultValue": "False",
                  "sbg:altPrefix": "-t",
                  "sbg:category": "Options",
                  "id": "add_tsv_suffix",
                  "type": "boolean?",
                  "inputBinding": {
                    "prefix": "--tsv",
                    "shellQuote": false,
                    "position": 6
                  },
                  "label": "Add TSV suffix",
                  "doc": "Add '.tsv' suffix to the output filename."
                }
              ],
              "outputs": [
                {
                  "id": "expression_matrix",
                  "doc": "Matrix files(s) containing expression values across all genes/transcripts for multiple provided inputs.",
                  "label": "Expression matrix",
                  "type": "File[]?",
                  "outputBinding": {
                    "glob": "${\n    function flatten(files){\n      var a = []\n  \t  for(var i=0;i<files.length;i++){\n        if(files[i]){\n          if(files[i].constructor == Array) {\n            a = a.concat(flatten(files[i]))\n          } else {\n            a = a.concat(files[i])\n          }\n        }\n      }\n      \n      var b = a.filter(function (el) {\n        return el != null\n      })\n      \n      return b\n    }\n    \n    var tmp_1 = [].concat(inputs.abundance_estimates)\n    var tmp = flatten(tmp_1)\n    \n    \n    if (tmp[0] && tmp[0].path) {\n        var x = tmp\n        if (inputs.output_name) {\n            var prefix = inputs.output_name\n        } else {\n            var prefix = 'expression_matrix'\n        }\n        return prefix + \"*\"\n    }\n}",
                    "outputEval": "${\n    function flatten(files){\n      var a = []\n  \t  for(var i=0;i<files.length;i++){\n        if(files[i]){\n          if(files[i].constructor == Array) {\n            a = a.concat(flatten(files[i]))\n          } else {\n            a = a.concat(files[i])\n          }\n        }\n      }\n      \n      var b = a.filter(function (el) {\n        return el != null\n      })\n      \n      return b\n    }\n    \n    var tmp_1 = [].concat(inputs.abundance_estimates)\n    var tmp = flatten(tmp_1)\n    \n    return inheritMetadata(self, tmp)\n}"
                  }
                }
              ],
              "doc": "This tool takes multiple abundance estimates files outputted by tools like RSEM, Kallisto or Salmon and creates expression matrices, based on the input column that the user specifies (the default is 'tpm', but any other string can be input here, like 'fpkm', 'counts' or similar, or even specifying multiple columns), that can be used for further downstream analysis.\n\nThis tool can also be used to aggregate any kind of results in tab-delimited format and create a matrix like file, it was just originally developed for creating expression matrices. \n\n### Common Issues ###\nNone",
              "label": "SBG Create Expression Matrix CWL1.0",
              "arguments": [
                {
                  "prefix": "",
                  "shellQuote": false,
                  "position": 0,
                  "valueFrom": "${\n  function flatten(files){\n      var a = []\n  \t  for(var i=0;i<files.length;i++){\n        if(files[i]){\n          if(files[i].constructor == Array) {\n            a = a.concat(flatten(files[i]))\n          } else {\n            a = a.concat(files[i])\n          }\n        }\n      }\n      \n      var b = a.filter(function (el) {\n        return el != null\n      })\n      \n      return b\n    }\n    \n    var tmp_1 = [].concat(inputs.abundance_estimates)\n    var tmp = flatten(tmp_1)\n    \n    if (tmp[0] && tmp[0].path) {\n    if (inputs.star) {\n      var x = tmp\n      var y = x[0].path.split('/')\n      y.pop()\n      var z = y.join('/') + '/'\n      return \"python parse_star_counts.py --path \" + z + \" && python create_tpm_matrix.py\"\n    } else {\n      return \"python create_tpm_matrix.py\"\n    }\n  } else {\n      return \"echo 'no inputs provided, skipping analysis'\"\n  }\n}"
                },
                {
                  "prefix": "",
                  "shellQuote": false,
                  "position": 2,
                  "valueFrom": "${\n  if (inputs.abundance_estimates) {\n    function flatten(files){\n      var a = []\n  \t  for(var i=0;i<files.length;i++){\n        if(files[i]){\n          if(files[i].constructor == Array) {\n            a = a.concat(flatten(files[i]))\n          } else {\n            a = a.concat(files[i])\n          }\n        }\n      }\n      \n      var b = a.filter(function (el) {\n        return el != null\n      })\n      \n      return b\n    }\n    \n    var tmp_1 = [].concat(inputs.abundance_estimates)\n    var tmp = flatten(tmp_1)\n    \n    if (tmp[0] && tmp[0].path) {\n        var quants = tmp\n        var samples = []\n        if (quants[0] && quants[0].metadata && quants[0].metadata.sample_id) {\n            for (var i = 0; i < quants.length; i++) {\n                samples = samples.concat(quants[i].metadata.sample_id)\n            }\n        } else {\n            for (var i = 0; i < quants.length; i++) {\n                samples = samples.concat(quants[i].path.split('/').pop().split('.')[0])\n            }\n        }\n        return '--sample_ids ' + samples\n    }\n  }\n}"
                }
              ],
              "requirements": [
                {
                  "class": "ShellCommandRequirement"
                },
                {
                  "class": "ResourceRequirement",
                  "ramMin": 1000,
                  "coresMin": 1
                },
                {
                  "class": "DockerRequirement",
                  "dockerPull": "images.sbgenomics.com/dusan_randjelovic/sci-python:2.7"
                },
                {
                  "class": "InitialWorkDirRequirement",
                  "listing": [
                    {
                      "entryname": "create_tpm_matrix.py",
                      "entry": "import argparse\n\n\ndef parse_quant_file(quant_files, sample_ids, out_name, column_name, feature_name):\n\n    # Open all files for reading and an output file for writing; make the header\n    handles = []\n    writer = open(out_name, 'w')\n    writer.write(feature_name)\n\n    k = 0\n    for item in quant_files:\n        sample_name = sample_ids[k]\n        k = k+1\n        suppl = open(item)\n        header = [x.lower() for x in suppl.next().rstrip().split('\\t')]\n        TPM_column = header.index(column_name)\n        #suppl.next()  # throw away the header\n        writer.write('\\t' + sample_name)\n        handles.append(suppl)\n\n    # Iterate over files and lines\n    eof = False\n    while not eof:\n        writer.write('\\n')\n        for i, suppl in enumerate(handles):\n            try:\n                items = suppl.next().rstrip().split('\\t')\n            except StopIteration:\n                eof = True\n                break\n            if i == 0:\n                writer.write('{}\\t{}'.format(items[0].split('|')[0], items[TPM_column]))\n            else:\n                writer.write('\\t' + items[TPM_column])\n\n    writer.close()\n\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser(description='Parse a set of abundance estimate files into a matrix of count values.')\n    parser.add_argument('--input', '-i', help='Comma-separated list of abundance estimate input files.')\n    parser.add_argument('--sample_ids', '-s', help='Comma-separated list of sample_ids.')\n    parser.add_argument('--out', '-o', help='Output file name.', default='expression_matrix')\n    parser.add_argument('--column_name', '-c', help='Column name chosen to aggregate results over.')\n    parser.add_argument('--feature_name', '-f', help='Feature name, i.e. \"gene\" or \"isoform\", to be used in the header.', required=False)\n    parser.add_argument('--tsv', '-t', help='Add \".tsv\" to the output filename.', action='store_true')\n\n    args = parser.parse_args()\n\n    quant_inputs = args.input.split(',')\n    samples = args.sample_ids.split(',')\n    columns = [x.lower() for x in args.column_name.split(',')]\n\n    if args.feature_name:\n        feature = args.feature_name\n    else:\n        feature = 'Gene_or_Transcript_ID'\n\n    if args.tsv:\n        extension = '.tsv'\n    else:\n        extension = ''\n\n    for column in columns:\n        if args.out:\n            if args.out.endswith('-'):\n                out_name = args.out + column\n            else:\n                out_name = args.out + '.' + column\n        else:\n            out_name = 'expression_matrix.' + column\n        out_name = out_name + extension\n        parse_quant_file(quant_files=quant_inputs, sample_ids=samples, out_name=out_name, column_name = column, feature_name=feature)",
                      "writable": false
                    },
                    "${\n  function flatten(files){\n      var a = []\n  \t  for(var i=0;i<files.length;i++){\n        if(files[i]){\n          if(files[i].constructor == Array) {\n            a = a.concat(flatten(files[i]))\n          } else {\n            a = a.concat(files[i])\n          }\n        }\n      }\n      \n      var b = a.filter(function (el) {\n        return el != null\n      })\n      \n      return b\n    }\n    \n    var tmp_1 = [].concat(inputs.abundance_estimates)\n    var tmp = flatten(tmp_1)\n    \n    if (tmp[0] && tmp[0].path) {\n        return tmp\n    }\n}",
                    {
                      "entryname": "parse_star_counts.py",
                      "entry": "import glob\nimport argparse\n\nparser = argparse.ArgumentParser(description='Testes.')\nparser.add_argument('-p','--path')\nargs = vars(parser.parse_args())\npath = args['path']\nfiles = glob.glob(path + '*.out.tab')\nfor file in files:\n    tmp = file.split('/')[-1].split('.')\n    tmp2 = tmp[0]\n    tmp3 = tmp[1:]\n    output_file = tmp2+'.reheaded.'+\".\".join(tmp3)\n    p = 'Gene_ID\\tunstranded-counts\\t1st_strand-counts\\t2nd_strand-counts\\n'\n    with open(file) as f:\n        for line in f:\n            if not (line.startswith('N_unmapped') or line.startswith('N_multimapping') or line.startswith('N_noFeature') or line.startswith('N_ambiguous')):\n                p += line\n    with open (output_file,\"w\") as f:\n        f.write(p)\n",
                      "writable": false
                    }
                  ]
                },
                {
                  "class": "InlineJavascriptRequirement",
                  "expressionLib": [
                    "var updateMetadata = function(file, key, value) {\n    file['metadata'][key] = value;\n    return file;\n};\n\n\nvar setMetadata = function(file, metadata) {\n    if (!('metadata' in file)) {\n        file['metadata'] = {}\n    }\n    for (var key in metadata) {\n        file['metadata'][key] = metadata[key];\n    }\n    return file\n};\n\nvar inheritMetadata = function(o1, o2) {\n    var commonMetadata = {};\n    if (!Array.isArray(o2)) {\n        o2 = [o2]\n    }\n    for (var i = 0; i < o2.length; i++) {\n        var example = o2[i]['metadata'];\n        for (var key in example) {\n            if (i == 0)\n                commonMetadata[key] = example[key];\n            else {\n                if (!(commonMetadata[key] == example[key])) {\n                    delete commonMetadata[key]\n                }\n            }\n        }\n    }\n    if (!Array.isArray(o1)) {\n        o1 = setMetadata(o1, commonMetadata)\n    } else {\n        for (var i = 0; i < o1.length; i++) {\n            o1[i] = setMetadata(o1[i], commonMetadata)\n        }\n    }\n    return o1;\n};\n\nvar toArray = function(file) {\n    return [].concat(file);\n};\n\nvar groupBy = function(files, key) {\n    var groupedFiles = [];\n    var tempDict = {};\n    for (var i = 0; i < files.length; i++) {\n        var value = files[i]['metadata'][key];\n        if (value in tempDict)\n            tempDict[value].push(files[i]);\n        else tempDict[value] = [files[i]];\n    }\n    for (var key in tempDict) {\n        groupedFiles.push(tempDict[key]);\n    }\n    return groupedFiles;\n};\n\nvar orderBy = function(files, key, order) {\n    var compareFunction = function(a, b) {\n        if (a['metadata'][key].constructor === Number) {\n            return a['metadata'][key] - b['metadata'][key];\n        } else {\n            var nameA = a['metadata'][key].toUpperCase();\n            var nameB = b['metadata'][key].toUpperCase();\n            if (nameA < nameB) {\n                return -1;\n            }\n            if (nameA > nameB) {\n                return 1;\n            }\n            return 0;\n        }\n    };\n\n    files = files.sort(compareFunction);\n    if (order == undefined || order == \"asc\")\n        return files;\n    else\n        return files.reverse();\n};"
                  ]
                }
              ],
              "sbg:publisher": "sbg",
              "sbg:toolkitVersion": "v1.0",
              "sbg:image_url": null,
              "sbg:createdBy": "uros_sipetic",
              "sbg:revisionNotes": "Update tsv boolean param",
              "sbg:project": "bix-demo/sbgtools-demo",
              "sbg:content_hash": "af9eb43909ced15861a7d4f78e93513e9c73de466c176ee545be4bb1e95d85592",
              "sbg:toolAuthor": "Seven Bridges Genomics",
              "sbg:projectName": "SBGTools - Demo New",
              "sbg:id": "h-74763193/h-743193f8/h-fc86b738/0",
              "sbg:modifiedBy": "uros_sipetic",
              "sbg:revisionsInfo": [
                {
                  "sbg:revisionNotes": "Upgraded to v1.0 from bix-demo/sbgtools-demo/sbg-create-rsem-tpm-counts-matrix",
                  "sbg:revision": 0,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1554909464
                },
                {
                  "sbg:revisionNotes": "Add CWL1.0 to name",
                  "sbg:revision": 1,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1554909508
                },
                {
                  "sbg:revisionNotes": "Add CWL1.0 to name, for real this time",
                  "sbg:revision": 2,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1567682542
                },
                {
                  "sbg:revisionNotes": "Add proper staging",
                  "sbg:revision": 3,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1570116631
                },
                {
                  "sbg:revisionNotes": "Update expressions to work in passthrough mode",
                  "sbg:revision": 4,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1581607254
                },
                {
                  "sbg:revisionNotes": "Update output expression to work in passthrough mode",
                  "sbg:revision": 5,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1581607327
                },
                {
                  "sbg:revisionNotes": "Update output_name expression",
                  "sbg:revision": 6,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1581691410
                },
                {
                  "sbg:revisionNotes": "flatten null elements",
                  "sbg:revision": 7,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1582819401
                },
                {
                  "sbg:revisionNotes": "Fix bug in js expression regarding output name forming",
                  "sbg:revision": 8,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1582823156
                },
                {
                  "sbg:revisionNotes": "Update output inherit metadata expression to flatten null elements",
                  "sbg:revision": 9,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1582823896
                },
                {
                  "sbg:revisionNotes": "Generalize for one sample as well",
                  "sbg:revision": 10,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1585847869
                },
                {
                  "sbg:revisionNotes": "Add support for multiple matrices.",
                  "sbg:revision": 11,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1586100092
                },
                {
                  "sbg:revisionNotes": "Add support for STAR",
                  "sbg:revision": 12,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1586781438
                },
                {
                  "sbg:revisionNotes": "fix bug with trailing comma",
                  "sbg:revision": 13,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1586785216
                },
                {
                  "sbg:revisionNotes": "Add TAB input file format. Output is now array of files",
                  "sbg:revision": 14,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1586863294
                },
                {
                  "sbg:revisionNotes": "Make analysis optional",
                  "sbg:revision": 15,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1586943683
                },
                {
                  "sbg:revisionNotes": "Fix bug with lowercase column names",
                  "sbg:revision": 16,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1586963615
                },
                {
                  "sbg:revisionNotes": "python code bug fix",
                  "sbg:revision": 17,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1586964833
                },
                {
                  "sbg:revisionNotes": "do proper passthrough",
                  "sbg:revision": 18,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1586984313
                },
                {
                  "sbg:revisionNotes": "Add feature_name option",
                  "sbg:revision": 19,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1587406550
                },
                {
                  "sbg:revisionNotes": "Make small changes to default output names",
                  "sbg:revision": 20,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1588885679
                },
                {
                  "sbg:revisionNotes": "Update glob",
                  "sbg:revision": 21,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1588925845
                },
                {
                  "sbg:revisionNotes": "feature_name is not required anymore",
                  "sbg:revision": 22,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1590536542
                },
                {
                  "sbg:revisionNotes": "Add tsv param",
                  "sbg:revision": 23,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1590682898
                },
                {
                  "sbg:revisionNotes": "Update tsv boolean param",
                  "sbg:revision": 24,
                  "sbg:modifiedBy": "uros_sipetic",
                  "sbg:modifiedOn": 1590686843
                }
              ],
              "sbg:contributors": [
                "uros_sipetic"
              ],
              "sbg:createdOn": 1554909464,
              "sbg:appVersion": [
                "v1.0"
              ],
              "sbg:latestRevision": 24,
              "sbg:validationErrors": [],
              "sbg:license": "Apache License 2.0",
              "sbg:cmdPreview": "python create_tpm_matrix.py  --sample_ids SAMPLEA,SAMPLEB",
              "sbg:sbgMaintained": false,
              "sbg:revision": 24,
              "sbg:toolkit": "SBGTools",
              "sbg:modifiedOn": 1590686843,
              "sbg:categories": [
                "Utilities",
                "SBGTools",
                "Other"
              ]
            },
            "label": "SBG Create Expression Matrix - Genes",
            "sbg:x": 1123.3009033203125,
            "sbg:y": 106.78125
          }
        ],
        "hints": [
          {
            "class": "sbg:AWSInstanceType",
            "value": "c4.8xlarge;ebs-gp2;1024"
          },
          {
            "class": "sbg:AlibabaCloudInstanceType",
            "value": "ecs.c5.8xlarge;cloud_ssd;600"
          },
          {
            "class": "sbg:GoogleInstanceType",
            "value": "n1-standard-32;pd-ssd;4096"
          }
        ],
        "requirements": [
          {
            "class": "ScatterFeatureRequirement"
          },
          {
            "class": "InlineJavascriptRequirement"
          },
          {
            "class": "StepInputExpressionRequirement"
          }
        ],
        "sbg:image_url": "https://cgc.sbgenomics.com/ns/brood/images/admin/sbg-public-data/salmon-workflow-1-2-0/2.png",
        "sbg:links": [
          {
            "id": "http://combine-lab.github.io/salmon/",
            "label": "Salmon Homepage"
          },
          {
            "id": "https://github.com/COMBINE-lab/salmon",
            "label": "Salmon Source Code"
          },
          {
            "id": "https://github.com/COMBINE-lab/salmon/releases/tag/v1.2.0",
            "label": "Salmon Download"
          },
          {
            "id": "https://www.biorxiv.org/content/10.1101/657874v2",
            "label": "Salmon Publications"
          },
          {
            "id": "http://salmon.readthedocs.org/en/latest/",
            "label": "Salmon Documentation"
          }
        ],
        "sbg:expand_workflow": false,
        "sbg:toolAuthor": "Rob Patro, Avi Srivastava",
        "sbg:revisionsInfo": [
          {
            "sbg:revision": 0,
            "sbg:modifiedBy": "admin",
            "sbg:modifiedOn": 1586797708,
            "sbg:revisionNotes": null
          },
          {
            "sbg:revision": 1,
            "sbg:modifiedBy": "admin",
            "sbg:modifiedOn": 1586797830,
            "sbg:revisionNotes": "copied rev. 9 from dev."
          },
          {
            "sbg:revision": 2,
            "sbg:modifiedBy": "admin",
            "sbg:modifiedOn": 1590749484,
            "sbg:revisionNotes": "Salmon Index:in_transcriptome_or_index set as required"
          }
        ],
        "sbg:wrapperAuthor": "nemanja.vucic",
        "sbg:license": "GNU General Public License v3.0",
        "sbg:projectName": "SBG Public data",
        "sbg:categories": [
          "Transcriptomics",
          "Quantification",
          "CWL1.0"
        ],
        "sbg:appVersion": [
          "v1.0"
        ],
        "sbg:id": "admin/sbg-public-data/salmon-workflow-1-2-0/2",
        "sbg:revision": 2,
        "sbg:revisionNotes": "Salmon Index:in_transcriptome_or_index set as required",
        "sbg:modifiedOn": 1590749484,
        "sbg:modifiedBy": "admin",
        "sbg:createdOn": 1586797708,
        "sbg:createdBy": "admin",
        "sbg:project": "admin/sbg-public-data",
        "sbg:sbgMaintained": false,
        "sbg:validationErrors": [],
        "sbg:contributors": [
          "admin"
        ],
        "sbg:latestRevision": 2,
        "sbg:publisher": "sbg",
        "sbg:content_hash": "af2f7b66c274e27d6a65da416c929c2f7857e7dbfecc8ae653553116b8faae21d",
        "sbg:workflowLanguage": "CWL"
      },
      "label": "Salmon workflow 1.2.0",
      "sbg:x": -560.0009765625,
      "sbg:y": -292.49200439453125
    },
    {
      "id": "fastqc_0_11_9",
      "in": [
        {
          "id": "in_reads",
          "source": [
            "in_reads"
          ]
        }
      ],
      "out": [
        {
          "id": "out_html_report"
        },
        {
          "id": "out_zip"
        }
      ],
      "run": {
        "class": "CommandLineTool",
        "cwlVersion": "v1.0",
        "$namespaces": {
          "sbg": "https://sevenbridges.com"
        },
        "id": "admin/sbg-public-data/fastqc-0-11-9/5",
        "baseCommand": [],
        "inputs": [
          {
            "sbg:category": "File inputs",
            "sbg:altPrefix": "-a",
            "id": "adapters_file",
            "type": "File?",
            "inputBinding": {
              "prefix": "--adapters",
              "shellQuote": false,
              "position": 1
            },
            "label": "Adapters",
            "doc": "Specifies a non-default file which contains the list of adapter sequences which will be explicity searched against the library. The file must contain sets of named adapters in the form name[tab]sequence.  Lines prefixed with a hash will be ignored.",
            "sbg:fileTypes": "TXT"
          },
          {
            "sbg:category": "Options",
            "id": "casava",
            "type": "boolean?",
            "inputBinding": {
              "prefix": "--casava",
              "separate": false,
              "shellQuote": false,
              "position": 1
            },
            "label": "Casava",
            "doc": "Files come from raw casava output. Files in the same sample group (differing only by the group number) will be analysed as a set rather than individually. Sequences with the filter flag set in the header will be excluded from the analysis. Files must have the same names given to them by casava (including being gzipped and ending with .gz) otherwise they won't be grouped together correctly."
          },
          {
            "sbg:category": "File inputs",
            "sbg:altPrefix": "-c",
            "id": "contaminants_file",
            "type": "File?",
            "inputBinding": {
              "prefix": "--contaminants",
              "shellQuote": false,
              "position": 1
            },
            "label": "Contaminants",
            "doc": "Specifies a non-default file which contains the list of contaminants to screen overrepresented sequences against. The file must contain sets of named contaminants in the form name[tab]sequence.  Lines prefixed with a hash will be ignored.",
            "sbg:fileTypes": "TXT"
          },
          {
            "sbg:category": "Execution parameters",
            "sbg:toolDefaultValue": "Determined by the number of input files",
            "id": "cpu_per_job",
            "type": "int?",
            "label": "Number of CPUs",
            "doc": "Number of CPUs to be allocated per execution of FastQC."
          },
          {
            "sbg:category": "Options",
            "sbg:toolDefaultValue": "FASTQ",
            "sbg:altPrefix": "-f",
            "id": "format",
            "type": [
              "null",
              {
                "type": "enum",
                "symbols": [
                  "bam",
                  "sam",
                  "bam_mapped",
                  "sam_mapped",
                  "fastq"
                ],
                "name": "format"
              }
            ],
            "inputBinding": {
              "prefix": "--format",
              "shellQuote": false,
              "position": 1
            },
            "label": "Format",
            "doc": "Bypasses the normal sequence file format detection and forces the program to use the specified format.  Valid formats are BAM, SAM, BAM_mapped, SAM_mapped and FASTQ."
          },
          {
            "sbg:category": "File inputs",
            "id": "in_reads",
            "type": "File[]",
            "inputBinding": {
              "shellQuote": false,
              "position": 101
            },
            "label": "Input file",
            "doc": "Input file.",
            "sbg:fileTypes": "FASTQ, FQ, FASTQ.GZ, FQ.GZ, BAM, SAM"
          },
          {
            "sbg:category": "Options",
            "sbg:toolDefaultValue": "7",
            "sbg:altPrefix": "-f",
            "id": "kmers",
            "type": "int?",
            "inputBinding": {
              "prefix": "--kmers",
              "shellQuote": false,
              "position": 1
            },
            "label": "Kmers",
            "doc": "Specifies the length of Kmer to look for in the Kmer content module. Specified Kmer length must be between 2 and 10. Default length is 7 if not specified."
          },
          {
            "sbg:category": "File inputs",
            "sbg:altPrefix": "-l",
            "id": "limits_file",
            "type": "File?",
            "inputBinding": {
              "prefix": "--limits",
              "shellQuote": false,
              "position": 1
            },
            "label": "Limits",
            "doc": "Specifies a non-default file which contains a set of criteria which will be used to determine the warn/error limits for the various modules.  This file can also be used to selectively remove some modules from the output all together.  The format needs to mirror the default limits.txt file found in the Configuration folder.",
            "sbg:fileTypes": "TXT"
          },
          {
            "sbg:category": "Execution parameters",
            "sbg:toolDefaultValue": "Determined by the number of input files",
            "id": "mem_per_job",
            "type": "int?",
            "label": "Memory per job [MB]",
            "doc": "Amount of memory allocated per execution of FastQC job."
          },
          {
            "sbg:category": "Options",
            "id": "min_length",
            "type": "int?",
            "inputBinding": {
              "prefix": "--min_length",
              "shellQuote": false,
              "position": 1
            },
            "label": "Min length",
            "doc": "Sets an artificial lower limit on the length of the sequence to be shown in the report.  As long as you set this to a value greater or equal to your longest read length then this will be the sequence length used to create your read groups.  This can be useful for making directly comaparable statistics from datasets with somewhat variable read lengths."
          },
          {
            "sbg:category": "Options",
            "id": "nano",
            "type": "boolean?",
            "inputBinding": {
              "prefix": "--nano",
              "separate": false,
              "shellQuote": false,
              "position": 1
            },
            "label": "Nano",
            "doc": "Files come from naopore sequences and are in fast5 format. In this mode you can pass in directories to process and the program will take in all fast5 files within those directories and produce a single output file from the sequences found in all files."
          },
          {
            "sbg:category": "Options",
            "id": "nofilter",
            "type": "boolean?",
            "inputBinding": {
              "prefix": "--nofilter",
              "shellQuote": false,
              "position": 1
            },
            "label": "No filter",
            "doc": "If running with --casava then don't remove read flagged by casava as poor quality when performing the QC analysis."
          },
          {
            "sbg:category": "Options",
            "id": "nogroup",
            "type": "boolean?",
            "inputBinding": {
              "prefix": "--nogroup",
              "separate": false,
              "shellQuote": false,
              "position": 1
            },
            "label": "Nogroup",
            "doc": "Disable grouping of bases for reads >50bp. All reports will show data for every base in the read.  WARNING: Using this option will cause fastqc to crash and burn if you use it on really long reads, and your plots may end up a ridiculous size. You have been warned."
          },
          {
            "sbg:category": "Options",
            "sbg:altPrefix": "-q",
            "id": "quiet",
            "type": "boolean?",
            "inputBinding": {
              "prefix": "--quiet",
              "shellQuote": false,
              "position": 1
            },
            "label": "Quiet",
            "doc": "Supress all progress messages on stdout and only report errors."
          },
          {
            "sbg:category": "Options",
            "sbg:toolDefaultValue": "1",
            "sbg:altPrefix": "-t",
            "id": "threads",
            "type": "int?",
            "inputBinding": {
              "prefix": "--threads",
              "shellQuote": false,
              "position": 1,
              "valueFrom": "${\n    if (self == 0) {\n        self = null;\n        inputs.threads = null\n    };\n\n\n    //if \"threads\" is not specified\n    //number of threads is determined based on number of inputs\n    if (!inputs.threads) {\n        inputs.threads = [].concat(inputs.in_reads).length\n    }\n    return Math.min(inputs.threads, 7)\n}"
            },
            "label": "Threads",
            "doc": "Specifies the number of files which can be processed simultaneously.  Each thread will be allocated 250MB of memory so you shouldn't run more threads than your available memory will cope with, and not more than 6 threads on a 32 bit machine.",
            "default": 0
          }
        ],
        "outputs": [
          {
            "id": "out_html_report",
            "doc": "FastQC reports in HTML format.",
            "label": "HTML reports",
            "type": "File[]?",
            "outputBinding": {
              "glob": "*.html",
              "outputEval": "$(inheritMetadata(self, inputs.in_reads))"
            },
            "sbg:fileTypes": "HTML"
          },
          {
            "id": "out_zip",
            "doc": "Zip archive of the report.",
            "label": "Report zip",
            "type": "File[]?",
            "outputBinding": {
              "glob": "*_fastqc.zip",
              "outputEval": "$(inheritMetadata(self, inputs.in_reads))"
            },
            "sbg:fileTypes": "ZIP"
          }
        ],
        "doc": "**FastQC** reads a set of sequence files and produces a quality control report from each one. These reports consist of a number of different modules, each of which will help identify a different type of potential problem in your data [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n\n### Common Use Cases\n\n**FastQC** is a tool which takes a FASTQ file and runs a series of tests on it to generate a comprehensive QC report.  This report will tell you if there is anything unusual about your sequence.  Each test is flagged as a pass, warning, or fail depending on how far it departs from what you would expect from a normal large dataset with no significant biases.  It is important to stress that warnings or even failures do not necessarily mean that there is a problem with your data, only that it is unusual.  It is possible that the biological nature of your sample means that you would expect this particular bias in your results.\n\n- In order to search the library for specific adapter sequences, a TXT file with the adapter sequences needs to be provided on the **Adapters** (`--adapters/-a`) input port. The lines in the file must follow the name [tab] sequence format.\n- In order to search the overrepresented sequences for specific contaminants, a TXT file with the contaminant sequences needs to be provided on the **Contaminants** (`--contaminants/-c`) input port. The lines in the file must follow the name [tab] sequence format.\n- In order to determine the warn/error limits for the various modules or remove some modules from the output, a TXT file with sets of criteria needs to be provided on the **Limits** (`--limits/-l`) input port. The lines in the file must follow the parameter [tab] warn/error [tab] value format.\n\n\n### Changes introduced by Seven Bridges\n\nNo modifications to the original tool representation have been made.\n\n\n### Common Issues and Important Notes\n\nUser can manually set CPU/Memory requirements by providing values on the **Number of CPUs** and **Memory per job [MB]** input ports. If neither of these two is provided and number of threads has been specified on the **Threads** (`--threads/-t`) input port, both CPU and memory per job will be determined by the provided number of threads; if neither number of CPUs/memory per job nor number of threads have been provided as inputs, the CPU and memory requirements will be determined according to the number of files provided on the **Input file** input port.\n\n\n### Performance Benchmarking\n\nThe speed and cost of the workflow depend on the size of the input FASTQ files. The following table showcases the metrics for the task running on the c4.2xlarge on-demand AWS instance. The price can be significantly reduced by using spot instances (set by default). Visit [The Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.\n\nFastq file 1 size(.gz) | Fastq file 2 size(.gz) | Duration | Cost | Instance type (AWS) |\n|---------------|-----------------|-----------|--------|-----|\n| 700 MB | 680 MB | 2 min. | $0.01 | c4.2xlarge |\n| 12.6 GB | 12.6 GB | 25 min. | $0.11 | c4.2xlarge |\n| 23.8 GB | 26.8 GB | 1 hour |$0.27 | c4.2xlarge |\n| 47.9 GB | 48.9 GB | 1 hour 40 min. | $0.44 | c4.2xlarge |\n\n### References\n\n[1] [FastQC GitHub](https://github.com/s-andrews/FastQC/blob/master/fastqc)",
        "label": "FastQC CWL 1.0",
        "arguments": [
          {
            "shellQuote": false,
            "position": 0,
            "valueFrom": "/opt/FastQC/fastqc"
          },
          {
            "prefix": "",
            "shellQuote": false,
            "position": 1,
            "valueFrom": "--noextract"
          },
          {
            "prefix": "--outdir",
            "shellQuote": false,
            "position": 1,
            "valueFrom": "."
          }
        ],
        "requirements": [
          {
            "class": "ShellCommandRequirement"
          },
          {
            "class": "ResourceRequirement",
            "ramMin": "${\n    // if mem_per_job is set, it takes precedence\n    if (inputs.mem_per_job) {\n        return inputs.mem_per_job\n    }\n    // if threads parameter is set, memory req is set based on the number of threads\n    else if (inputs.threads) {\n        return 1024 + 300 * inputs.threads\n    }\n    // else the memory req is determined by the number of input files, up to 7 -- default\n    else return (1024 + 300 * Math.min([].concat(inputs.in_reads).length, 7))\n}",
            "coresMin": "${\n    // if cpus_per_job is set, it takes precedence\n    if (inputs.cpu_per_job) {\n        return inputs.cpu_per_job\n    }\n    // if threads parameter is set, the number of CPUs is set based on that parametere\n    else if (inputs.threads) {\n        return inputs.threads\n    }\n    // else the number of CPUs is determined by the number of input files, up to 7 -- default\n    else return Math.min([].concat(inputs.in_reads).length, 7)\n}"
          },
          {
            "class": "DockerRequirement",
            "dockerImageId": "759c4c8fbafd",
            "dockerPull": "images.sbgenomics.com/stefan_cidilko/fastqc-0-11-9:0"
          },
          {
            "class": "InitialWorkDirRequirement",
            "listing": []
          },
          {
            "class": "InlineJavascriptRequirement",
            "expressionLib": [
              "var updateMetadata = function(file, key, value) {\n    file['metadata'][key] = value;\n    return file;\n};\n\n\nvar setMetadata = function(file, metadata) {\n    if (!('metadata' in file)) {\n        file['metadata'] = {}\n    }\n    for (var key in metadata) {\n        file['metadata'][key] = metadata[key];\n    }\n    return file\n};\n\nvar inheritMetadata = function(o1, o2) {\n    var commonMetadata = {};\n    if (!Array.isArray(o2)) {\n        o2 = [o2]\n    }\n    for (var i = 0; i < o2.length; i++) {\n        var example = o2[i]['metadata'];\n        for (var key in example) {\n            if (i == 0)\n                commonMetadata[key] = example[key];\n            else {\n                if (!(commonMetadata[key] == example[key])) {\n                    delete commonMetadata[key]\n                }\n            }\n        }\n    }\n    if (!Array.isArray(o1)) {\n        o1 = setMetadata(o1, commonMetadata)\n    } else {\n        for (var i = 0; i < o1.length; i++) {\n            o1[i] = setMetadata(o1[i], commonMetadata)\n        }\n    }\n    return o1;\n};\n\nvar toArray = function(file) {\n    return [].concat(file);\n};\n\nvar groupBy = function(files, key) {\n    var groupedFiles = [];\n    var tempDict = {};\n    for (var i = 0; i < files.length; i++) {\n        var value = files[i]['metadata'][key];\n        if (value in tempDict)\n            tempDict[value].push(files[i]);\n        else tempDict[value] = [files[i]];\n    }\n    for (var key in tempDict) {\n        groupedFiles.push(tempDict[key]);\n    }\n    return groupedFiles;\n};\n\nvar orderBy = function(files, key, order) {\n    var compareFunction = function(a, b) {\n        if (a['metadata'][key].constructor === Number) {\n            return a['metadata'][key] - b['metadata'][key];\n        } else {\n            var nameA = a['metadata'][key].toUpperCase();\n            var nameB = b['metadata'][key].toUpperCase();\n            if (nameA < nameB) {\n                return -1;\n            }\n            if (nameA > nameB) {\n                return 1;\n            }\n            return 0;\n        }\n    };\n\n    files = files.sort(compareFunction);\n    if (order == undefined || order == \"asc\")\n        return files;\n    else\n        return files.reverse();\n};",
              "\nvar setMetadata = function(file, metadata) {\n    if (!('metadata' in file)) {\n        file['metadata'] = {}\n    }\n    for (var key in metadata) {\n        file['metadata'][key] = metadata[key];\n    }\n    return file\n};\n\nvar inheritMetadata = function(o1, o2) {\n    var commonMetadata = {};\n    if (!Array.isArray(o2)) {\n        o2 = [o2]\n    }\n    for (var i = 0; i < o2.length; i++) {\n        var example = o2[i]['metadata'];\n        for (var key in example) {\n            if (i == 0)\n                commonMetadata[key] = example[key];\n            else {\n                if (!(commonMetadata[key] == example[key])) {\n                    delete commonMetadata[key]\n                }\n            }\n        }\n    }\n    if (!Array.isArray(o1)) {\n        o1 = setMetadata(o1, commonMetadata)\n    } else {\n        for (var i = 0; i < o1.length; i++) {\n            o1[i] = setMetadata(o1[i], commonMetadata)\n        }\n    }\n    return o1;\n};"
            ]
          }
        ],
        "sbg:links": [
          {
            "label": "Homepage",
            "id": "http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"
          },
          {
            "label": "Source Code",
            "id": "http://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.5_source.zip"
          },
          {
            "label": "Wiki",
            "id": "https://wiki.hpcc.msu.edu/display/Bioinfo/FastQC+Tutorial"
          },
          {
            "label": "Download",
            "id": "http://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.9.zip"
          },
          {
            "label": "Publication",
            "id": "http://www.bioinformatics.babraham.ac.uk/projects/fastqc"
          }
        ],
        "sbg:cmdPreview": "/opt/FastQC/fastqc  --noextract --outdir .  /path/to/input_fastq-1.fastq  /path/to/input_fastq-2.fastq",
        "sbg:revisionsInfo": [
          {
            "sbg:revision": 0,
            "sbg:modifiedBy": "admin",
            "sbg:modifiedOn": 1582563140,
            "sbg:revisionNotes": null
          },
          {
            "sbg:revision": 1,
            "sbg:modifiedBy": "admin",
            "sbg:modifiedOn": 1582563253,
            "sbg:revisionNotes": "revision 1"
          },
          {
            "sbg:revision": 2,
            "sbg:modifiedBy": "admin",
            "sbg:modifiedOn": 1582563253,
            "sbg:revisionNotes": "Reviewed descriptions"
          },
          {
            "sbg:revision": 3,
            "sbg:modifiedBy": "admin",
            "sbg:modifiedOn": 1582563253,
            "sbg:revisionNotes": "descriptions updated"
          },
          {
            "sbg:revision": 4,
            "sbg:modifiedBy": "admin",
            "sbg:modifiedOn": 1582563253,
            "sbg:revisionNotes": "Descriptions updated"
          },
          {
            "sbg:revision": 5,
            "sbg:modifiedBy": "admin",
            "sbg:modifiedOn": 1582563254,
            "sbg:revisionNotes": "Descriptions changed"
          }
        ],
        "sbg:categories": [
          "Utilities",
          "FASTQ Processing",
          "QC",
          "CWL1.0"
        ],
        "sbg:projectName": "SBG Public data",
        "sbg:toolkitVersion": "0.11.9",
        "sbg:toolAuthor": "Babraham Institute",
        "sbg:toolkit": "FastQC",
        "sbg:image_url": null,
        "sbg:license": "GNU General Public License v3.0 only",
        "sbg:appVersion": [
          "v1.0"
        ],
        "sbg:id": "admin/sbg-public-data/fastqc-0-11-9/5",
        "sbg:revision": 5,
        "sbg:revisionNotes": "Descriptions changed",
        "sbg:modifiedOn": 1582563254,
        "sbg:modifiedBy": "admin",
        "sbg:createdOn": 1582563140,
        "sbg:createdBy": "admin",
        "sbg:project": "admin/sbg-public-data",
        "sbg:sbgMaintained": false,
        "sbg:validationErrors": [],
        "sbg:contributors": [
          "admin"
        ],
        "sbg:latestRevision": 5,
        "sbg:publisher": "sbg",
        "sbg:content_hash": "ac9955d6aaa0aa804f228bf42cc9ac2d6381a54d32941aba66390704d01070d9a",
        "sbg:workflowLanguage": "CWL"
      },
      "label": "FastQC CWL 1.0",
      "sbg:x": -629.0960083007812,
      "sbg:y": -47.753326416015625
    },
    {
      "id": "deseq2_1_26_0",
      "in": [
        {
          "id": "in_gene_annotation",
          "source": "in_annotation"
        },
        {
          "id": "factor",
          "default": "induced_hbv_replication",
          "source": "factor"
        },
        {
          "id": "abundances",
          "linkMerge": "merge_flattened",
          "source": [
            "salmon_workflow_1_2_0/out_quant_sf"
          ]
        },
        {
          "id": "quantification_tool",
          "default": "salmon"
        },
        {
          "id": "collapse_replicates",
          "linkMerge": "merge_flattened"
        },
        {
          "id": "cond1",
          "linkMerge": "merge_flattened",
          "default": "+HBV",
          "source": [
            "cond1"
          ]
        },
        {
          "id": "cond2",
          "linkMerge": "merge_flattened",
          "default": "-HBV",
          "source": [
            "cond2"
          ]
        }
      ],
      "out": [
        {
          "id": "html_report"
        },
        {
          "id": "normalized_counts"
        },
        {
          "id": "results"
        },
        {
          "id": "rdata"
        },
        {
          "id": "pheno_out"
        }
      ],
      "run": {
        "class": "CommandLineTool",
        "cwlVersion": "v1.1",
        "$namespaces": {
          "sbg": "https://sevenbridges.com"
        },
        "id": "admin/sbg-public-data/deseq2-1-26-0/7",
        "baseCommand": [],
        "inputs": [
          {
            "sbg:category": "Options",
            "sbg:toolDefaultValue": "0.1",
            "id": "fdr_cutoff",
            "type": "float?",
            "inputBinding": {
              "prefix": "--sigTreshold=",
              "separate": false,
              "shellQuote": false,
              "position": 7
            },
            "label": "FDR cutoff",
            "doc": "During the analysis independent filtering is automatically performed based on the mean of normalized counts for each gene, optimizing the number of genes which will have an adjusted p value below a given FDR cutoff. By default this cutoff is set to 0.1. If the adjusted p value cutoff will be a value other than 0.1 - user should set it here manually."
          },
          {
            "sbg:category": "Options",
            "sbg:toolDefaultValue": "parametric",
            "id": "fit_type",
            "type": [
              "null",
              {
                "type": "enum",
                "symbols": [
                  "parametric",
                  "local",
                  "mean"
                ],
                "name": "fit_type"
              }
            ],
            "inputBinding": {
              "prefix": "--fitType=",
              "separate": false,
              "shellQuote": false,
              "position": 6
            },
            "label": "Fit type",
            "doc": "A local smoothed dispersion fit is automatically substitited in the case that the parametric curve doesn\u2019t fit the observed dispersion mean relationship. This can be prespecified by providing the argument fitType=\"local\". Additionally, using the mean of gene-wise disperion estimates as the fitted value can be specified by providing the argument fitType=\"mean\"."
          },
          {
            "sbg:category": "Input files",
            "id": "in_gene_annotation",
            "type": "File?",
            "inputBinding": {
              "prefix": "--gtf=",
              "separate": false,
              "shellQuote": false,
              "position": 8
            },
            "label": "Gene annotation",
            "doc": "If the abundance estimates provided are on a transcript-level, tximport will be used to summarize them for gene-level analysis. Gene annotation needs to be supplied then.",
            "sbg:fileTypes": "GTF"
          },
          {
            "sbg:category": "Options",
            "id": "title",
            "type": "string?",
            "label": "Analysis title",
            "doc": "Output files will share this common prefix."
          },
          {
            "sbg:category": "Options",
            "id": "factor",
            "type": "string",
            "inputBinding": {
              "prefix": "--factor=",
              "separate": false,
              "shellQuote": false,
              "position": 3
            },
            "label": "Covariate of interest",
            "doc": "The samples will be grouped according to the chosen variable of interest. This needs to match either a column name in the provided phenotype data CSV file or a metadata key. If the latter is true, then all the input files need to have this metadata field populated."
          },
          {
            "sbg:category": "Options",
            "sbg:toolDefaultValue": "False",
            "id": "auto_mean_filter_off",
            "type": "boolean?",
            "inputBinding": {
              "prefix": "--auto_mean_filter_off=",
              "separate": false,
              "shellQuote": false,
              "position": 14
            },
            "label": "Turn off the independent filtering",
            "doc": "Turn off the independent filtering. The filtering is performed by default using the mean of normalized counts as a filter statistic. A threshold on the filter statistic is found which optimizes the number of adjusted p values lower than a significance level alpha. The adjusted p values for the genes which do not pass the filter threshold are set to NA."
          },
          {
            "sbg:category": "Options",
            "sbg:toolDefaultValue": "False",
            "id": "beta_prior",
            "type": "boolean?",
            "inputBinding": {
              "prefix": "--beta_prior=",
              "separate": false,
              "shellQuote": false,
              "position": 15
            },
            "label": "log2 fold change shrinkage",
            "doc": "Turn on the log2 fold change shrinkage."
          },
          {
            "sbg:category": "Input files",
            "id": "abundances",
            "type": "File[]",
            "label": "Expression data",
            "doc": "Gene or transcript abundances for each sample.",
            "sbg:fileTypes": "TXT, SF, TSV, RESULTS, CSV"
          },
          {
            "sbg:category": "Input files",
            "id": "pheno_data",
            "type": "File?",
            "inputBinding": {
              "prefix": "--addMeta=",
              "separate": false,
              "shellQuote": false,
              "position": 10,
              "valueFrom": "${\n    if (inputs.pheno_data) {\n        return inputs.pheno_data.path\n    } else return \"pheno_data.csv\"\n}"
            },
            "label": "Phenotype data",
            "doc": "A CSV file with phenotype data for all samples.",
            "sbg:fileTypes": "CSV"
          },
          {
            "id": "quantification_tool",
            "type": {
              "type": "enum",
              "symbols": [
                "htseq",
                "kallisto",
                "salmon",
                "sailfish",
                "rsem",
                "stringtie"
              ],
              "name": "quantification_tool"
            },
            "inputBinding": {
              "prefix": "--quant=",
              "separate": false,
              "shellQuote": false,
              "position": 3
            },
            "label": "Quantification tool",
            "doc": "Tool that generated abundance estimates."
          },
          {
            "sbg:category": "Options",
            "id": "control_for",
            "type": "string[]?",
            "inputBinding": {
              "prefix": "--control_for=",
              "separate": false,
              "itemSeparator": ",",
              "shellQuote": false,
              "position": 20
            },
            "label": "Control variables",
            "doc": "Optional vector of strings representing the names of potential confounders. Must correspond to metadata keys or columns in supplied phenotype data csv."
          },
          {
            "sbg:category": "Options",
            "sbg:toolDefaultValue": "0",
            "id": "minrowmean",
            "type": "float?",
            "inputBinding": {
              "prefix": "--minrowmean=",
              "separate": false,
              "shellQuote": false,
              "position": 7
            },
            "label": "Pre-filtering threhold",
            "doc": "The value entered here is used as a threshold for pre-filtering and only those genes that have at least that many reads on average are analyzed. While it is not necessary to pre-filter low count genes, by doing so we reduce the memory requirements and increase the speed."
          },
          {
            "sbg:category": "Options",
            "sbg:toolDefaultValue": "False",
            "id": "ignoreTxVersion",
            "type": "boolean?",
            "inputBinding": {
              "prefix": "--ignoreTxVersion=",
              "separate": false,
              "shellQuote": false,
              "position": 16
            },
            "label": "ignoreTxVersion",
            "doc": "Ignore transcript version in transcript abundance input files."
          },
          {
            "sbg:category": "Options",
            "id": "collapse_replicates",
            "type": "string?",
            "inputBinding": {
              "prefix": "--collapse=",
              "separate": false,
              "shellQuote": false,
              "position": 17
            },
            "label": "Grouping factor for collapsing technical replicates",
            "doc": "A grouping factor (metadata field or a column in the pheno data file), to use for collapsing technical replicates."
          },
          {
            "sbg:category": "Options",
            "id": "cond1",
            "type": "string?",
            "inputBinding": {
              "prefix": "--cond1=",
              "separate": false,
              "shellQuote": false,
              "position": 4
            },
            "label": "Factor level - test",
            "doc": "The numerator for the log fold change calculation."
          },
          {
            "sbg:category": "Options",
            "id": "cond2",
            "type": "string?",
            "inputBinding": {
              "prefix": "--cond2=",
              "separate": false,
              "shellQuote": false,
              "position": 5
            },
            "label": "Factor level - reference",
            "doc": "The denominator for the log fold change calculation."
          }
        ],
        "outputs": [
          {
            "id": "html_report",
            "doc": "HTML report.",
            "label": "HTML report",
            "type": "File?",
            "outputBinding": {
              "glob": "*.b64html",
              "outputEval": "${\n    return inheritMetadata(self, inputs.abundances)\n\n}"
            },
            "sbg:fileTypes": "HTML"
          },
          {
            "id": "normalized_counts",
            "doc": "Counts normalized using estimated sample-specific normalization factors.",
            "label": "Normalized counts",
            "type": "File?",
            "outputBinding": {
              "glob": "*raw_counts.txt",
              "outputEval": "${\n    return inheritMetadata(self, inputs.abundances)\n\n}"
            },
            "sbg:fileTypes": "TXT"
          },
          {
            "id": "results",
            "doc": "Output CSV file.",
            "label": "DESeq2 analysis results.",
            "type": "File?",
            "outputBinding": {
              "glob": "*out.csv",
              "outputEval": "${\n    return inheritMetadata(self, inputs.abundances)\n\n}"
            },
            "sbg:fileTypes": "CSV"
          },
          {
            "id": "rdata",
            "doc": "Workspace image.",
            "label": "RData file",
            "type": "File[]?",
            "outputBinding": {
              "glob": "*.RData"
            },
            "sbg:fileTypes": "RDATA"
          },
          {
            "id": "pheno_out",
            "type": "File?",
            "outputBinding": {
              "glob": "pheno_data.csv"
            }
          }
        ],
        "doc": "**DESeq2** performs differential gene expression analysis by use of negative binomial generalized linear models. It analyzes estimated read counts from several samples, each belonging to one of two or more conditions under study, searching for systematic changes between conditions, as compared to within-condition variability. \n\nThe Bioconductor/R package **DESeq2** provides a set of functions for importing data, performing exploratory analysis and finally testing for differential expression. This CWL tool is a wrapper around the script based on the standard workflow for this type of analysis [1].\n\n**DESeq2** offers two kinds of hypothesis tests: the Wald test, where we use the estimated standard error of a log2 fold change to test if it is equal to zero, and the likelihood ratio test (LRT). The LRT examines two models for the counts, a full model with a certain number of terms and a reduced model, in which some of the terms of the full model are removed. The test determines if the increased likelihood of the data using the extra terms in the full model is more than expected if those extra terms are truly zero. The LRT is therefore useful for testing multiple terms at once, for example testing 3 or more levels of a factor at once, or all interactions between two variables [2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\nAs input files, please use one of the following: \n\n- **HTSeq**, **RSEM** or **StringTie** gene level abundance estimates;\n- **Salmon**, **Sailfish** or **Kallisto** transcript level abundance estimates.\n\nIf the abundance estimates provided are on a transcript-level, **tximport** will be used to summarize them for gene-level analysis. **Gene annotation** (in GTF format) needs to be supplied then.\n\nTo fit a generalized linear model for each gene, besides gene abundance estimates - some phenotype information is needed. In the simplest case, only a single independant variable is used to explain the expression levels. Experiments with more than one variable influencing the counts can be analyzed using design formula that includes the additional covariates. \n\nThere are two options for providing phenotype information:\n\n1. By indicating API keys for metadata fields that need to be included in the design. Phenotype information will then consist of variables you listed as **Covariate of interest** and **Control variables**.\n2. By including a CSV file (**Phenotype data** input) that contains a row for each sample, with Sample ID in the first column. These Sample IDs need to match those in input files metadata. Also, a single line header with variable names should be included.\n\nExample CSV content below:\n\n```\nsample_id,library,sex,condition\ntreated1,paired-end,male,treated\ntreated2,single-end,male,treated\ntreated3,paired-end,female,treated\nuntreated1,single-end,male,untreated\nuntreated2,paired-end,female,untreated\nuntreated3,paired-end,female,untreated\nuntreated4,paired-end,male,untreated\n```\n\nSupplying a CSV like this while entering \"condition\" for the value of the **Covariate of interest** parameter and \"library\" in **Control variables** will test for differential expression between treated and untreated samples, while controlling for effect of library preparation.\n\nThe information about sample belonging to the treated or the untreated group can also be kept in the metadata. To use a metadata field for splitting the samples into groups for testing, enter its metadata key for the **Covariate of interest** parameter. All the input files need to have this metadata field populated. To control for possible confounders, enter their API keys as **Control variables**.\n\n### Changes Introduced by Seven Bridges\n\nAlthough the script covers different use cases and gives the user some flexibility to tailor the analysis to his own needs, not everything is customizable.\n\nThe user does not choose the type of test that will be performed. The appropriate test is chosen automatically:\n\n- if there are more than two values (levels) to a chosen **Covariate of interest** - LRT is used. \n- if the **Covariate of interest** has only two different values - Wald test is used to test for differential expression.\n\nThe analysis report contains the list of input parameters, phenotype data table, a heatmap of input samples with cluster dendrogram, dispersion estimates plot and an MA plot showing the log2 fold changes attributable to a given gene over the mean of normalized counts and a short summary of results.\n\n### Common Issues and Important Notes\n\n- Any metadata key entered as **Covariate of interest** or **Control variables** needs to exist and it's field be populated in all the samples (**Expression data**) if phenotype data is read from the metadata. Otherwise, if a CSV is supplied - these keys need to match the column names in it's header. Keep in mind that metadata keys are usually different to what is seen on the front-end. To match metadata keys to their corresponding values on the front-end please refer to the table on [this link](https://docs.sevenbridges.com/docs/metadata-on-the-seven-bridges-platform). To learn how to add custom metadata field to expression data files refer to the [following document](https://docs.sevenbridges.com/docs/format-of-a-manifest-file) (section: _Modifying metadata via the visual interface_).\n- Be careful when choosing covariates - generalized linear model fitting will fail if model matrix is not full rank!\n- If your task fails with \"none of the transcripts in the quantification files are present in the first column of tx2gene\" message in the error log, and you are certain that you are using the proper GTF file - you can try rerunning the task with **ignoreTxVersion** option selected. This can happen if you, for example, download the transcriptome FASTA from the Ensembl website and use it to build aligner index - transcript version will then be included in transcript ID in the quantification output file, while in the GTF it's kept as a separate attribute so the transcript IDs will not match.\n\n### Performance Benchmarking\n\nThe execution time for performing differential expression analysis on 6 samples (3 in each group), using transcripts from GENCODE Release 27 (GRCh38.p10) takes 5-6 minutes on the default instance; the price is negligible (~ 0.01$). Unless specified otherwise, the default instance used to run the __DESeq2__ tool will be c4.large (AWS) with 256GB storage.\n\n### References\n\n[1] [RNA-seq workflow: gene-level exploratory analysis and differential expression](https://www.bioconductor.org/help/workflows/rnaseqGene/)\n\n[2] [DESeq2 vignette](https://bioconductor.org/packages/release/bioc/vignettes/DESeq2/inst/doc/DESeq2.html)",
        "label": "DESeq2",
        "arguments": [
          {
            "prefix": "",
            "shellQuote": false,
            "position": 100,
            "valueFrom": "${\n    function sharedStart(array) {\n        var A = array.concat().sort(),\n            a1 = A[0],\n            a2 = A[A.length - 1],\n            L = a1.length,\n            i = 0;\n        while (i < L && a1.charAt(i) === a2.charAt(i)) i++;\n        return a1.substring(0, i);\n    }\n\n    if (!inputs.title) {\n        var path_list = []\n        inputs.abundances.forEach(function(f) {\n            return path_list.push(f.path.replace(/\\\\/g, '/').replace(/.*\\//, ''))\n        })\n        var common_prefix = sharedStart(path_list)\n        if (common_prefix.length > 0) {\n            title = common_prefix.concat(\".DEAnalysis\")\n        } else {\n            var title = \"DEAnalysis\"\n        }\n    } else {\n        var title = inputs.title.replace(/ /g, \"_\")\n    }\n\n    var cmd = \"--title=\" + title + \" && mv dsq2.html \" + title + \".deseq2.1.26.0.summary_report.html && python sbg_html_to_b64.py --input \" + title + \".deseq2.1.26.0.summary_report.html\"\n\n    return cmd\n}"
          },
          {
            "shellQuote": false,
            "position": 0,
            "valueFrom": "Rscript -e \"knitr::spin('dsq2.R', knit=F)\" && Rscript -e \"rmarkdown::render('dsq2.Rmd', 'html_document')\" --args"
          }
        ],
        "requirements": [
          {
            "class": "ShellCommandRequirement"
          },
          {
            "class": "ResourceRequirement",
            "ramMin": 2000,
            "coresMin": 2
          },
          {
            "class": "DockerRequirement",
            "dockerPull": "images.sbgenomics.com/marko_zecevic/deseq2:1.26.0"
          },
          {
            "class": "InitialWorkDirRequirement",
            "listing": [
              {
                "entryname": "dsq2.R",
                "entry": "#' ---\n#' title: \"DESeq2 Differential Expression analysis report\"\n#' output:\n#'   html_document:\n#'       toc: true\n#' ---\n\n#+ options, echo = F, results = 'hide', warning = F, error = F, message = F\noptions(stringsAsFactors = FALSE, width = 100)\n\n#+ collect arguments, echo = F, results = 'hide', warning = F, error = F, message = F\nargs <- commandArgs(TRUE)\n\n# Parse arguments (the expected form is --arg=value)\nparseArgs <- function(x) strsplit(sub(\"^--\", \"\", x), \"=\")\nargsDF <- as.data.frame(do.call(\"rbind\", parseArgs(args)))\nDSQ2arg <- as.list(as.character(argsDF$V2))\nnames(DSQ2arg) <- argsDF$V1\n\n#+ title, echo = FALSE\nif (!is.null(DSQ2arg$title)) cat(DSQ2arg$title)\n\n#+ checks and defaults, echo = F, results = 'hide', warning = F, error = F, message = F\n# the parameters here are: \n# quantification_tool (R), split_by (R), factor (R), factor levels (cond1 and cond2), \n# fit_type, fdr_cutoff, outlier_replace_off, outlier_filter_off, \n# auto_mean_filter_off, beta_prior\n\n# is the gtf provided? if it is - we can use tximport to switch from transcript to gene level\nif (is.null(DSQ2arg$gtf)) {\n    if (!(DSQ2arg$quant %in% c(\"htseq\", \"rsem\", \"stringtie\"))) stop(\"Please provide reference annotation if using transcript level abundance estimates.\")\n}\n\n## variables we want to control for\nif(!(is.null(DSQ2arg$control_for))) {\n    control_for <- unlist(strsplit(DSQ2arg$control_for, \",\", TRUE))\n}\n\n## default csv is 'pheno_data.csv'\nif(is.null(DSQ2arg$addMeta)) {\n    DSQ2arg$addMeta <- \"pheno_data.csv\"\n}\n\n## no of cores is by default no of available cores - 1\nif(is.null(DSQ2arg$cores)) {\n    DSQ2arg$cores <- parallel::detectCores() - 1\n}\n\n## fitType default value is \"parametric\"\nif(is.null(DSQ2arg$fitType)) {\n    DSQ2arg$fitType <- \"parametric\"\n}\n\n# fdr_cutoff default (significance treshold)\nif(is.null(DSQ2arg$sigTreshold)) {\n    DSQ2arg$sigTreshold <- 0.1\n}\n\nDSQ2arg$sigTreshold <- as.numeric(DSQ2arg$sigTreshold)\n\n# minrowmean default - genes with mean values below this threshold will be filtered out\nif(is.null(DSQ2arg$minrowmean)) {\n    DSQ2arg$minrowmean <- 0\n}\n\n# turn off the outlier replacement\n# Turns off the outlier replacement which is on by default. When there are 7 or more replicates for a given sample, counts with large Cook\u2019s distance will automatically be replaced with trimmed means over all samples, scaled up by the size factor or normalization factor. This approach is conservative, it will not lead to false positives, as it replaces the outlier value with the value predicted by the null hypothesis.\nif (is.null(DSQ2arg$outlier_replace_off)) {\n    minRep <- 7\n} else {\n    minRep <- Inf\n}\n\n# turn off Cook's distance filtering\n# Turn off Cook's distance filtering. Genes which contain a Cook\u2019s distance above a cutoff for samples which have 3 or more replicates are automatically flagged. The p values and adjusted p values for these genes are set to NA. At least 3 replicates are required for flagging, as it is difficult to judge which sample might be an outlier with only 2 replicates.\nif (is.null(DSQ2arg$outlier_filter_off)) {\n    cooksCutoff <- TRUE\n} else {\n    cooksCutoff <- FALSE\n}\n\n# turn off the independent filtering\nif (is.null(DSQ2arg$auto_mean_filter_off)) {\n    independentFiltering <- TRUE\n} else {\n    independentFiltering <- FALSE\n}\n\n# log2 fold change shrinkage?\nif (is.null(DSQ2arg$beta_prior)) {\n    betaPrior <- FALSE\n} else {\n    betaPrior <- TRUE\n}\n\n# ignore transcript version?\nif (is.null(DSQ2arg$ignoreTxVersion)) {\n    DSQ2arg$ignoreTxVersion <- FALSE\n} else {\n    DSQ2arg$ignoreTxVersion <- TRUE\n}\n\n#+ load packages, echo = F, results = 'hide', warning = F, error = F, message = F\nsuppressPackageStartupMessages({\n    library(\"DESeq2\")\n    library(\"BiocParallel\")\n    library(\"RColorBrewer\")\n    library(\"gplots\")\n    library(\"reshape2\")\n    library(\"grid\")\n    library(\"ggplot2\")\n    library(\"plyr\")\n    library(\"stringr\")\n})\n\nregister(MulticoreParam(DSQ2arg$cores))\n\nsampleTable <- read.table('counts.txt', col.names = c('sampleName', 'fileName'))\n\n# read the phenotype data from a csv file\nmeta <- read.csv(DSQ2arg$addMeta, header = TRUE, stringsAsFactors = TRUE, na.strings = \"\")\n\nind <- match(meta[,1], sampleTable$sampleName)\nsampleTable <- sampleTable[ind, , drop = FALSE]\n\naddcols <- colnames(meta)[2:length(colnames(meta))]\nfor (i in 1:length(addcols)) {\n    sampleTable[, addcols[i]] <- meta[, addcols[i]]\n}\n\nsampleTable <- sampleTable[complete.cases(sampleTable),]\n\nif (!(is.null(DSQ2arg$control_for))) {\n    frml <- paste(c(control_for, DSQ2arg$factor), collapse = '+')\n} else frml <- DSQ2arg$factor\n\nvalid <- function(level) {\n    if (is.null(level)) return(FALSE)\n    if (!(level %in% levels(sampleTable[, DSQ2arg$factor]))) return(FALSE)\n    return(TRUE)\n}\n\ninvalid <- function(level) {\n    return(!(valid(level)))\n}\n\ncorrectLevels <- sum(sapply(list(DSQ2arg$cond1, DSQ2arg$cond2), valid))\n\nif (grepl(\":\", DSQ2arg$factor, fixed=TRUE) & all(strsplit(DSQ2arg$factor,\":\")[[1]] %in% addcols)) {\n    testType <- \"LRT\"\n    designFormula <- as.formula(paste0(\"~ \", frml, \" + \", DSQ2arg$factor))\n    reducedFormula <- as.formula(paste0(\"~\", frml))\n} else if (length(levels(sampleTable[, DSQ2arg$factor])) > 2 & (correctLevels < 2)) {\n    testType <- \"LRT\"\n    designFormula <- as.formula(paste0(\"~ \", frml))\n    \n    redcols <- addcols[!(addcols == DSQ2arg$factor)]\n    if (length(redcols) > 0) {\n        rfrml <- paste(redcols, collapse = '+')\n    } else {\n        rfrml <- \"1\"\n    }\n    reducedFormula <- as.formula(paste0(\"~\", rfrml))\n} else {\n    testType <- \"Wald\"\n    designFormula <- as.formula(paste0(\"~ \", frml))\n    if (invalid(DSQ2arg$cond1) & invalid(DSQ2arg$cond2)) {\n        DSQ2arg$cond1 <- levels(sampleTable[, DSQ2arg$factor])[2]\n        DSQ2arg$cond2 <- levels(sampleTable[, DSQ2arg$factor])[1]\n    } else if (invalid(DSQ2arg$cond1)) {\n        DSQ2arg$cond1 <- setdiff(levels(sampleTable[, DSQ2arg$factor]), DSQ2arg$cond2)\n    } else if (invalid(DSQ2arg$cond2)) {\n        DSQ2arg$cond2 <- setdiff(levels(sampleTable[, DSQ2arg$factor]), DSQ2arg$cond1)\n    }\n}\n\nmultmerge = function(filenames){\n    datalist = lapply(filenames, function(x){read.csv(file=x,header=T)})\n    Reduce(function(x,y) {merge(x,y, by=\"gene_id\")}, datalist)\n}\n\n#+ load data into DESeq2, echo = F, results = 'hide', warning = F, error = F, message = F\nif (DSQ2arg$quant==\"htseq\") {\n    # construct the object from HTSeq files\n    dds <- DESeqDataSetFromHTSeqCount(sampleTable = sampleTable,\n                                      directory = \"\",\n                                      design =  designFormula)\n} else {\n\n    txiFiles <- paste0('/',as.character(sampleTable[,2]))\n    names(txiFiles) <- as.character(sampleTable[,1])\n\n    if (DSQ2arg$quant==\"stringtie\") {\n        countsmat <- multmerge(txiFiles)\n        rownames(countsmat) <- countsmat$gene_id\n        countsmat <- countsmat[,-1]\n        dds <- DESeqDataSetFromMatrix(countsmat, \n                                      sampleTable[,3:ncol(sampleTable),drop=FALSE],\n                                      designFormula)\n    } else {\n        # construct the object using tximport\n        library(\"tximport\")\n        # if gtf is available and needed - make the tx2gene table\n        if (!is.null(DSQ2arg$gtf)) {\n            suppressPackageStartupMessages({\n                library(\"GenomicFeatures\")\n            })\n            txdb <- makeTxDbFromGFF(DSQ2arg$gtf, format=\"gtf\")\n            k <- keys(txdb, keytype = \"GENEID\")\n            df <- select(txdb, keys = k, keytype = \"GENEID\", columns = \"TXNAME\")\n            tx2gene <- df[, 2:1]  # tx ID, then gene ID\n        }\n        if (DSQ2arg$quant==\"rsem\") {\n            txi <- tximport(txiFiles, type=\"rsem\", txIn=FALSE, \n                            ignoreTxVersion = DSQ2arg$ignoreTxVersion)\n        } else txi <- tximport(txiFiles, type=DSQ2arg$quant, tx2gene=tx2gene, \n                               ignoreTxVersion = DSQ2arg$ignoreTxVersion)\n        # https://support.bioconductor.org/p/84304/#84368\n        txi$length[txi$length == 0] <- 1\n        dds <- DESeqDataSetFromTximport(txi,\n                                        sampleTable[,3:ncol(sampleTable),drop=FALSE],\n                                        designFormula)\n    }\n}\n\nkeep <- rowMeans(counts(dds)) >= DSQ2arg$minrowmean\ndds <- dds[keep,]\n\n\n\n\n\nif (!is.null(DSQ2arg$collapse_replicates)) {\n    dds <- collapseReplicates(dds, dds[[DSQ2arg$collapse_replicates]])\n    rownames(colData(dds)) <- dds[[DSQ2arg$collapse_replicates]]\n}\n\n\n\n\n\n#' # Input data and experiment design\n#' sample table:\n#+ print sample table, echo = F, warning = F, error = F, message = F\nsampleTable[, -2, drop=FALSE]\n\n#' counts matrix size:\n#+ matrix size, echo = F, warning = F, error = F, message = F\ncat(paste(ncol(dds), \"samples with counts over\", nrow(dds), \"genes\\n\"))\n\n#' design formula:\n#+ print design formula, echo = F, warning = F, error = F, message = F\n# we only need the two factor levels (reference and the one we are testing \n# against the reference) when Wald test is chosen.\nif (testType==\"LRT\") {\n    cat(\"full: \", paste0(as.character(designFormula), collapse = \"\"))\n    cat(\"\\n\")\n    cat(\"reduced: \", paste0(as.character(reducedFormula), collapse = \"\"))\n} else designFormula\n\n#+ run the analysis, echo = F, warning = F, error = F, message = F\nif (testType==\"Wald\") {\n    dds <- DESeq(dds, fitType=DSQ2arg$fitType, betaPrior=betaPrior, \n                 minReplicatesForReplace=minRep, parallel=TRUE)\n} else {\n    dds <- DESeq(dds, test = \"LRT\", fitType=DSQ2arg$fitType, \n                 reduced = reducedFormula, minReplicatesForReplace=minRep, parallel=TRUE)   \n}\n\nexpThr <- 100\n\n#' # Exploratory analysis\n#' ## PCA plot\n#+ pca, fig.width = 10, fig.height = 7, fig.align = \"center\", echo = F, warning = F, error = F, message = F\nif (dim(sampleTable)[1] <= expThr) {\n    rld <- rlog(dds)\n    d <- plotPCA(rld, intgroup=rev(DSQ2arg$factor), ntop = 1000, returnData=TRUE)\n    labs <- rownames(colData(dds))\n    \n    rv <- rowVars(assay(rld))\n    select <- order(rv, decreasing=TRUE)[seq_len(500)]\n    pca <- prcomp(t(assay(rld)[select,]))\n    props <- round(summary(pca)$importance[2,1:2], digits = 4)*100\n    \n    print(ggplot(d, aes(x=PC1,y=PC2, col=group,label=factor(labs)), environment = environment()) \n          + geom_point() + geom_text(size=3, vjust=\"inward\",hjust=\"inward\") \n          + labs(x = paste0(\"PC1 (\", props[1], \"%)\"), y = paste0(\"PC2 (\", props[2], \"%)\")))\n} else {\n    cat(\"To shorten the analysis run time and minimize the cost, we skip PCA when there are more than 100 samples in the analysis.\")\n}\n\n#' ## Cluster dendrogram\n#+ clustering, fig.width = 10, fig.height = 7, fig.align = \"center\", echo = F, warning = F, error = F, message = F\nif (dim(sampleTable)[1] <= expThr) {\n    dat <- assay(rld)\n    colnames(dat) <- labs\n    distsRL <- dist(t(dat))\n    mat <- as.matrix(distsRL)\n    hc <- hclust(distsRL)\n    hmcol <- colorRampPalette(brewer.pal(9, \"GnBu\"))(100)\n    heatmap.2(mat, Rowv=as.dendrogram(hc), symm=TRUE, trace=\"none\", col = rev(hmcol),\n              main=\"Sample-to-sample distances\", margin=c(13,13))\n} else {\n    cat(\"This plot is omitted when there are more than 100 samples in the analysis.\")\n}\n\n#' ## Box plots of raw and normalized counts\n#+ normalization, fig.width = 10, fig.height = 7, fig.align = \"center\", echo = F, warning = F, error = F, message = F\nif (dim(sampleTable)[1] <= expThr) {\n    log.raw = log2(counts(dds) + 1)\n    log.raw = stack(log.raw)\n    # adding group to log.raw\n    log.raw$group = colData(dds)[log.raw$col, DSQ2arg$factor]\n    log.raw = arrange(as.data.frame(log.raw), group)\n    log.raw$col = factor(log.raw$col, levels = unique(log.raw$col))\n    # boxplot\n    plot(ggplot(log.raw, aes(x = col, y = value, fill = group)) + \n             geom_boxplot(size = 0.3, outlier.size = 0.25) +\n             labs(x=\"sample\", y=\"gene counts\") + \n             theme(axis.text.x = element_text(angle = 45, hjust = 1), axis.title = element_text(size = 14)) + \n             ggtitle(\"log2 transformed gene counts\") + \n             theme(plot.title = element_text(size=14), legend.position = \"bottom\", legend.title = element_text(face = \"bold\")))\n    \n    log.norm = log2(counts(dds, normalized=TRUE) + 1)\n    log.norm = stack(log.norm)\n    # adding group to log.norm\n    log.norm$group = colData(dds)[log.norm$col, DSQ2arg$factor]\n    log.norm = arrange(as.data.frame(log.norm), group)\n    log.norm$col = factor(log.norm$col, levels = unique(log.norm$col))\n    # boxplot\n    plot(ggplot(log.norm, aes(x = col, y = value, fill = group)) + \n             geom_boxplot(size = 0.3, outlier.size = 0.25) +\n             labs(x=\"sample\", y=\"gene counts\") + \n             theme(axis.text.x = element_text(angle = 45, hjust = 1), axis.title = element_text(size = 14)) + \n             ggtitle(\"log2 transformed normalized counts\") + \n             theme(plot.title = element_text(size=14), legend.position = \"bottom\", legend.title = element_text(face = \"bold\")))\n} else {\n    cat(\"These plots are omitted when there are more than 100 samples in the analysis.\")\n}\n\n#' # DE analysis\n#' Parameters used for running DESeq() function:\n#+ parameters de, echo = F, warning = F, error = F, message = F\ncat(paste(\"test :\", testType))\ncat(paste(\"fitType :\", DSQ2arg$fitType))\nif (betaPrior & testType == \"LRT\") {\n    cat(\"Beta prior switched to FALSE because test type is LRT.\")\n} else cat(paste(\"betaPrior :\", betaPrior))\ncat(paste(\"minReplicatesForReplace :\", minRep))\n\n#' ## Dispersion estimation\n#+ dispersion, fig.width = 10, fig.height = 7, fig.align = \"center\", echo = F, warning = F, error = F, message = F\nplotDispEsts(dds, main=\"Dispersion estimates\")\n\n#+ write normalized counts to a file, echo = F, results = 'hide', warning = F, error = F, message = F\nallLevels <- levels(colData(dds)[[DSQ2arg$factor]])\nlabs <- paste0(rownames(colData(dds)), \":\", do.call(paste, as.list(colData(dds)[DSQ2arg$factor])))\nnormalizedCounts<-counts(dds,normalized=TRUE)\ncolnames(normalizedCounts)<-labs\nwrite.table(normalizedCounts, file=paste0(DSQ2arg$title,\".raw_counts.txt\"), sep=\"\\t\", quote=FALSE)\n\n#+ export results, echo = F, results = 'hide', warning = F, error = F, message = F\nif (testType==\"Wald\") {\n    res <- results(dds, contrast=c(DSQ2arg$factor, DSQ2arg$cond1, DSQ2arg$cond2),\n                   cooksCutoff=cooksCutoff, alpha = DSQ2arg$sigTreshold,\n                   independentFiltering=independentFiltering, parallel=TRUE)\n} else {\n    res <- results(dds, cooksCutoff=cooksCutoff, alpha = DSQ2arg$sigTreshold,\n                   independentFiltering=independentFiltering, parallel=TRUE)\n}\n\n#' Parameters used for exporting results:\n#+ parameters res, echo = F, warning = F, error = F, message = F\ncat(paste(\"cooksCutoff :\", cooksCutoff))\ncat(paste(\"independentFiltering :\", independentFiltering))\ncat(paste(\"alpha :\", DSQ2arg$sigTreshold))\n\n#' ## Analysis summary\n#+ summary, echo = F, warning = F, error = F, message = F\nif (testType==\"Wald\") {\n    title_suffix <- paste0(DSQ2arg$factor,\": \",DSQ2arg$cond1,\" vs \",DSQ2arg$cond2)\n} else {\n    title_suffix <- DSQ2arg$factor\n}\ncat(title_suffix)\nsumm <- capture.output(summary(res, alpha = DSQ2arg$sigTreshold))\ncat(summ[c(-1,-9,-10, -11)], sep = \"\\n\")\n\n#+ writing results into csv, echo = F, warning = F, error = F, message = F\nwrite.csv(as.data.frame(res), file=paste0(DSQ2arg$title,\".out.csv\"), quote = FALSE)\n\nif (independentFiltering) {\n    threshold <- unname(metadata(res)$filterThreshold)\n} else {\n    threshold <- 0\n}\n\n#' ## p-value histogram\n#+ histogram, fig.width = 10, fig.height = 7, fig.align = \"center\", echo = F, warning = F, error = F, message = F\nuse <- res$baseMean > threshold\nif (sum(!use) == 0) {\n    h <- hist(res$pvalue, breaks=0:50/50, plot=FALSE)\n    barplot(height = h$counts,\n            col = \"powderblue\", space = 0, xlab=\"p-values\", ylab=\"frequency\",\n            main=paste(\"Histogram of p-values for\",title_suffix))\n    text(x = c(0, length(h$counts)), y = 0, label=paste(c(0,1)), adj=c(0.5,1.7), xpd=NA)\n} else {\n    h1 <- hist(res$pvalue[!use], breaks=0:50/50, plot=FALSE)\n    h2 <- hist(res$pvalue[use], breaks=0:50/50, plot=FALSE)\n    colori <- c(\"filtered (low count)\"=\"khaki\", \"not filtered\"=\"powderblue\")\n    barplot(height = rbind(h1$counts, h2$counts), beside = FALSE,\n            col = colori, space = 0, xlab=\"p-values\", ylab=\"frequency\",\n            main=paste(\"Histogram of p-values for\",title_suffix))\n    text(x = c(0, length(h1$counts)), y = 0, label=paste(c(0,1)), adj=c(0.5,1.7), xpd=NA)\n    legend(\"topright\", fill=rev(colori), legend=rev(names(colori)), bg=\"white\")\n}\n\n#' ## MA plot\n#+ ma plot, fig.width = 10, fig.height = 7, fig.align = \"center\", echo = F, warning = F, error = F, message = F\nplotMA(res, main= paste(\"MA-plot for\",title_suffix), ylim=range(res$log2FoldChange, na.rm=TRUE), alpha = DSQ2arg$sigTreshold)\n\nsave.image(paste0(DSQ2arg$title,\".env.RData\"))\n\n#' # Session info\n#+ session, echo = F, warning = F, error = F, message = F\nsessionInfo()",
                "writable": false
              },
              {
                "entryname": "sbg_html_to_b64.py",
                "entry": "\"\"\"\nUsage:\n    sbg_html_to_b64.py --input FILE [--select FILE]\n\nDescription:\n    This tool is used for conversion of html file to b64 html file so it can be easily displayed in browsers.\n\nOptions:\n    -h, --help      Show this help message and exit. (For third class of tools it's required to put\n                    this option).\n\n    -v, --version   Show version and exit.\n\n    --input FILE    Input file is archive containing html and all other files included in the html file(images, etc).\n\n    --select FILE If we wish to select specific html file from folder that we wish to parse.\n\nExamples:\n    python sbg_html_to_b64.py --input sample_fastqc.zip\n\"\"\"\n\nimport os\nfrom docopt import docopt\nimport os.path\nimport base64\nimport mimetypes\nfrom bs4 import BeautifulSoup\nimport lxml\nfrom path import Path\nfrom subprocess import call, check_output\nimport re\n\n\ndef dataurl(data, img=False, mime=None):\n    isfile = os.path.isfile(data)\n    if not isfile and not mime:\n        raise Exception('Mimetype must be provided when encoding data is not a valid file path.')\n    if not mime:\n        mimetypes.init()\n        mime, enc = mimetypes.guess_type(os.path.join('file://', data))\n        if mime is None:\n            raise Exception('rfc2397: failed to determine file type')\n    if isfile:\n        with open(data, 'r') as fpp:\n            data = fpp.read()\n    if not img:\n        return 'data:%s;base64,%s' % (mime, base64.b64encode(data))\n    else:\n        return 'data:%s;base64,%s' % (mime, base64.b64encode(data))\n\n\ndef compact_html(html_file):\n    with open(html_file) as f:\n        html = f.read()\n\n    if 'snpEff_summary' in html_file:\n        for l in html.split('\\n'):\n            if str(l).startswith('<a name'):\n                html = html.replace(str(l), str(l) + '</a>')\n        html = html.replace('<p>', '<p></p>')\n        html = html[:-358]\n        soup = BeautifulSoup(html, \"lxml\")\n\n        js = \"javascript: void(0); document.getElementById('%s').scrollIntoView(true);\"\n        for anchor in soup.findAll('a'):\n            if 'href' in str(anchor):\n                if anchor['href'].startswith('#'):\n                    anchor['href'] = js % anchor['href'][1:]\n                else:\n                    anchor.decompose()\n            else:\n                anchor['id'] = anchor['name']\n\n        return soup.prettify()\n\n    else:\n        html = html.replace('&middot;', '.')\n        html = html.replace('&raquo;', '>>')\n        html = html.replace('&ge;', '>=')\n        html = html.replace('&gt;', '>')\n        html = html.replace('&lt;', '<')\n        html = html.replace('\\xab', '<<')\n        html = html.replace('\\xbb', '>>')\n        html = html.replace('\\xc2', '')\n        html = html.replace('&le;', '<=')\n        html = html.replace('&mdash;', '--')\n        #html = re.sub(re.compile(\"/\\*.*?\\*/\", re.DOTALL), \"\", html)\n        base_dir = os.path.split(html_file)[0]\n        soup = BeautifulSoup(html, \"lxml\")\n        for img in soup.findAll('img'):\n            if img['src'].find('data:') == 0:\n                durl_img = img['src']\n            else:\n                durl_img = dataurl(os.path.join(base_dir, img['src']), img=True)\n            img['src'] = durl_img\n        return soup.prettify().encode(soup.original_encoding)\n\n\ndef html_to_dataurl(html_file):\n    return dataurl(compact_html(html_file), img=False, mime='text/html')\n\nif __name__ == \"__main__\":\n    args = docopt(__doc__, version='1.0')\n    filename = args.get('--input')\n\n    # unzipping the archive\n    if Path(filename).ext == '.zip':\n        cmd = [\"unzip\", filename, \"-d\", \"./unzip\"]\n        call(cmd)\n\n        if args.get('--select'):\n            selected_file = args.get('--select')\n            filepath = \"./unzip/\" + args.get('--select')\n            if os.path.isfile(filepath):\n                html_file = filepath\n                b64_html = selected_file + '.b64html'\n            else:\n                raise Exception(\"File not present!\")\n        else:\n            html_file = check_output([\"find\", \"./unzip\", \"-iname\", \"*.html\"]).split('\\n')[:-1]\n            if len(html_file) == 1:\n                b64_html = Path(filename).namebase + '.b64html'\n                html_file = html_file[0]                                # conversion from list to string\n            else:\n                b64_html = [x.split('/')[-1] + '.b64html' for x in html_file]\n    else:\n        html_file = filename\n        b64_html = Path(filename).namebase + '.b64html'\n\n    # check if we need to process single or list of html files. if it is a single file then html_file is type string\n    if type(html_file) is str:\n        with open(b64_html, 'w') as fp:\n            fp.write(html_to_dataurl(html_file))\n    elif type(b64_html) is list:\n        for i, elem in enumerate(b64_html):\n            with open(elem, 'w') as fp:\n                print(html_file[i])\n                fp.write(html_to_dataurl(html_file[i]))\n    else:\n        raise Exception('This is not good.')",
                "writable": false
              },
              {
                "entryname": "pheno_data.csv",
                "entry": "${\n    if (!inputs.pheno_data) {\n\n        var expression_data = [].concat(inputs.abundances)\n        var covariate = inputs.factor\n\n        // phenotype data - header\n        var pheno_data = 'sample_id,'.concat(covariate)\n\n        if (inputs.control_for) {\n            var adjustvars = [].concat(inputs.control_for)\n            for (var i = 0; i < adjustvars.length; i++) {\n                var adjustvar = adjustvars[i]\n                pheno_data = pheno_data.concat(\",\", adjustvar)\n            }\n        }\n\n        // phenotype data\n        for (var j = 0; j < expression_data.length; j++) {\n\n            var new_sample = expression_data[j].metadata['sample_id']\n            new_sample = new_sample.concat(',', expression_data[j].metadata[covariate])\n\n            if (inputs.control_for) {\n                for (i = 0; i < adjustvars.length; i++) {\n                    new_sample = new_sample.concat(',', expression_data[j].metadata[adjustvars[i]])\n                }\n            }\n\n            pheno_data = pheno_data.concat('\\n', new_sample)\n        }\n\n        return pheno_data + '\\n'\n    } else return 'Phenotype data is provided as an input file - ignore this one.'\n\n}",
                "writable": false
              },
              {
                "entryname": "counts.txt",
                "entry": "${\n    var everyInput = [];\n    for (var i = 0; i < inputs.abundances.length; i++) {\n        everyInput.push(inputs.abundances[i].metadata.sample_id.concat(\"\\t\").concat(inputs.abundances[i].path.slice(1)));\n    }\n    return everyInput.join('\\n');\n}",
                "writable": false
              }
            ]
          },
          {
            "class": "InlineJavascriptRequirement",
            "expressionLib": [
              "var updateMetadata = function(file, key, value) {\n    file['metadata'][key] = value;\n    return file;\n};\n\n\nvar setMetadata = function(file, metadata) {\n    if (!('metadata' in file)) {\n        file['metadata'] = {}\n    }\n    for (var key in metadata) {\n        file['metadata'][key] = metadata[key];\n    }\n    return file\n};\n\nvar inheritMetadata = function(o1, o2) {\n    var commonMetadata = {};\n    if (!Array.isArray(o2)) {\n        o2 = [o2]\n    }\n    for (var i = 0; i < o2.length; i++) {\n        var example = o2[i]['metadata'];\n        for (var key in example) {\n            if (i == 0)\n                commonMetadata[key] = example[key];\n            else {\n                if (!(commonMetadata[key] == example[key])) {\n                    delete commonMetadata[key]\n                }\n            }\n        }\n    }\n    if (!Array.isArray(o1)) {\n        o1 = setMetadata(o1, commonMetadata)\n    } else {\n        for (var i = 0; i < o1.length; i++) {\n            o1[i] = setMetadata(o1[i], commonMetadata)\n        }\n    }\n    return o1;\n};\n\nvar toArray = function(file) {\n    return [].concat(file);\n};\n\nvar groupBy = function(files, key) {\n    var groupedFiles = [];\n    var tempDict = {};\n    for (var i = 0; i < files.length; i++) {\n        var value = files[i]['metadata'][key];\n        if (value in tempDict)\n            tempDict[value].push(files[i]);\n        else tempDict[value] = [files[i]];\n    }\n    for (var key in tempDict) {\n        groupedFiles.push(tempDict[key]);\n    }\n    return groupedFiles;\n};\n\nvar orderBy = function(files, key, order) {\n    var compareFunction = function(a, b) {\n        if (a['metadata'][key].constructor === Number) {\n            return a['metadata'][key] - b['metadata'][key];\n        } else {\n            var nameA = a['metadata'][key].toUpperCase();\n            var nameB = b['metadata'][key].toUpperCase();\n            if (nameA < nameB) {\n                return -1;\n            }\n            if (nameA > nameB) {\n                return 1;\n            }\n            return 0;\n        }\n    };\n\n    files = files.sort(compareFunction);\n    if (order == undefined || order == \"asc\")\n        return files;\n    else\n        return files.reverse();\n};\n\n\n"
            ]
          }
        ],
        "hints": [
          {
            "class": "sbg:AWSInstanceType",
            "value": "r5.2xlarge;ebs-gp2;160"
          },
          {
            "class": "sbg:AlibabaCloudInstanceType",
            "value": "ecs.r5.2xlarge;cloud_ssd;160"
          },
          {
            "class": "sbg:SaveLogs",
            "value": "{*.log,pheno_data.csv,counts.txt}"
          }
        ],
        "sbg:toolAuthor": "Michael Love (HSPH Boston), Simon Anders, Wolfgang Huber (EMBL Heidelberg)",
        "sbg:links": [
          {
            "id": "https://www.bioconductor.org/packages/3.10/bioc/html/DESeq2.html",
            "label": "Homepage"
          },
          {
            "id": "http://www.genomebiology.com/2014/15/12/550",
            "label": "Publication"
          },
          {
            "id": "https://bioconductor.org/packages/release/bioc/manuals/DESeq2/man/DESeq2.pdf",
            "label": "Manual"
          },
          {
            "id": "https://github.com/mikelove/DESeq2",
            "label": "Source code"
          }
        ],
        "sbg:toolkit": "DESeq2",
        "sbg:categories": [
          "Transcriptomics",
          "Differential Expression",
          "CWL1.0"
        ],
        "sbg:toolkitVersion": "1.26.0",
        "sbg:license": "GNU Lesser General Public License (LGPL) >= 3",
        "sbg:cmdPreview": "Rscript -e \"knitr::spin('dsq2.R', knit=F)\" && Rscript -e \"rmarkdown::render('dsq2.Rmd', 'html_document')\" --args --factor=sample_group --quant=htseq  --title=demo_1 && mv dsq2.html demo_1.deseq2.1.18.0.summary_report.html && python sbg_html_to_b64.py --input demo_1.deseq2.1.18.0.summary_report.html",
        "sbg:image_url": null,
        "sbg:projectName": "SBG Public data",
        "sbg:revisionsInfo": [
          {
            "sbg:revision": 0,
            "sbg:modifiedBy": "admin",
            "sbg:modifiedOn": 1584385095,
            "sbg:revisionNotes": null
          },
          {
            "sbg:revision": 1,
            "sbg:modifiedBy": "admin",
            "sbg:modifiedOn": 1584385243,
            "sbg:revisionNotes": "first"
          },
          {
            "sbg:revision": 2,
            "sbg:modifiedBy": "admin",
            "sbg:modifiedOn": 1584385243,
            "sbg:revisionNotes": "app labels updated and gtf input id changed to \"in_gene_annotation\""
          },
          {
            "sbg:revision": 3,
            "sbg:modifiedBy": "admin",
            "sbg:modifiedOn": 1584385243,
            "sbg:revisionNotes": "output id changed from \"output\" to \"results\""
          },
          {
            "sbg:revision": 4,
            "sbg:modifiedBy": "admin",
            "sbg:modifiedOn": 1594025245,
            "sbg:revisionNotes": "Fix DESeq2 version in the report filename."
          },
          {
            "sbg:revision": 5,
            "sbg:modifiedBy": "admin",
            "sbg:modifiedOn": 1594025246,
            "sbg:revisionNotes": "normalized boxplot labels fixed"
          },
          {
            "sbg:revision": 6,
            "sbg:modifiedBy": "admin",
            "sbg:modifiedOn": 1597733862,
            "sbg:revisionNotes": "Instance hint updated and alpha converted to numeric."
          },
          {
            "sbg:revision": 7,
            "sbg:modifiedBy": "admin",
            "sbg:modifiedOn": 1611959864,
            "sbg:revisionNotes": "fixed ignoreTxVersion parameter"
          }
        ],
        "sbg:expand_workflow": false,
        "sbg:appVersion": [
          "v1.1"
        ],
        "sbg:id": "admin/sbg-public-data/deseq2-1-26-0/7",
        "sbg:revision": 7,
        "sbg:revisionNotes": "fixed ignoreTxVersion parameter",
        "sbg:modifiedOn": 1611959864,
        "sbg:modifiedBy": "admin",
        "sbg:createdOn": 1584385095,
        "sbg:createdBy": "admin",
        "sbg:project": "admin/sbg-public-data",
        "sbg:sbgMaintained": false,
        "sbg:validationErrors": [],
        "sbg:contributors": [
          "admin"
        ],
        "sbg:latestRevision": 7,
        "sbg:publisher": "sbg",
        "sbg:content_hash": "a05c9527722f06f279764b3b3f2e7927afbd3e7811ac0fbd6144d8ae3ca40aae5",
        "sbg:workflowLanguage": "CWL"
      },
      "label": "DESeq2",
      "scatter": [
        "fdr_cutoff"
      ],
      "sbg:x": -72.54818725585938,
      "sbg:y": -123.75333404541016
    }
  ],
  "requirements": [
    {
      "class": "SubworkflowFeatureRequirement"
    },
    {
      "class": "ScatterFeatureRequirement"
    },
    {
      "class": "InlineJavascriptRequirement"
    },
    {
      "class": "StepInputExpressionRequirement"
    }
  ],
  "sbg:projectName": "BCO-CWL Examples",
  "sbg:revisionsInfo": [
    {
      "sbg:revision": 0,
      "sbg:modifiedBy": "phil_webster",
      "sbg:modifiedOn": 1663013303,
      "sbg:revisionNotes": "Copy of phil_webster/purdue-workflow/rnaseq-test/19"
    }
  ],
  "sbg:image_url": "https://cgc.sbgenomics.com/ns/brood/images/phil_webster/bco-cwl-examples/rnaseq-test/0.png",
  "sbg:toolAuthor": "Phillip Webster and Jeffrey Grover",
  "sbg:appVersion": [
    "v1.2",
    "v1.0",
    "v1.1"
  ],
  "id": "https://cgc-api.sbgenomics.com/v2/apps/phil_webster/bco-cwl-examples/rnaseq-test/0/raw/",
  "sbg:id": "phil_webster/bco-cwl-examples/rnaseq-test/0",
  "sbg:revision": 0,
  "sbg:revisionNotes": "Copy of phil_webster/purdue-workflow/rnaseq-test/19",
  "sbg:modifiedOn": 1663013303,
  "sbg:modifiedBy": "phil_webster",
  "sbg:createdOn": 1663013303,
  "sbg:createdBy": "phil_webster",
  "sbg:project": "phil_webster/bco-cwl-examples",
  "sbg:sbgMaintained": false,
  "sbg:validationErrors": [],
  "sbg:contributors": [
    "phil_webster"
  ],
  "sbg:latestRevision": 0,
  "sbg:publisher": "sbg",
  "sbg:content_hash": "a76b6a3c8590fa056d65f60db9794ec5d930377b61e11b87a91dfcea5dad6e3f1",
  "sbg:workflowLanguage": "CWL",
  "sbg:copyOf": "phil_webster/purdue-workflow/rnaseq-test/19"
}