{
  "spec_version": "https://w3id.org/biocompute/1.4.2/",
  "object_id": "https://biocompute.sbgenomics.com/bco/ab53156b-6bc3-40c5-9b43-358091e182e6",
  "etag": "1604cfa95432577c597d9cc02a9870d85a7e8f9eb83967916797062f990aad02",
  "provenance_domain": {
    "name": "BROAD Best Practices RNA-Seq Variant Calling 4.1.0.0",
    "version": "1.0.0",
    "review": [],
    "derived_from": "https://cgc-api.sbgenomics.com/v2/apps/phil_webster/bco-cwl-examples/broad-best-practices-rna-seq-variant-calling-4-1-0-0/0/raw/",
    "obsolete_after": "2023-01-23T00:00:00+0000",
    "embargo": ["2023-01-23T00:00:00+0000", "2023-01-23T00:00:00+0000"],
    "created": "2023-01-23T00:00:00+0000",
    "modified": "2023-01-23T00:00:00+0000",
    "contributors": [],
    "license": "https://spdx.org/licenses/CC-BY-4.0.html"
  },
  "usability_domain": "This Whole Exome Sequencing (WES) tumor-normal workflow first uses the [Broad Institute's](https://software.broadinstitute.org/gatk/best-practices/) best-practices workflow for read alignment, and then analyzes those data in several ways. \n\n1.  Identifies variants from a human exome experiment with GATK-4 [Mutect2](https://software.broadinstitute.org/gatk/documentation/tooldocs/4.0.5.1/org_broadinstitute_hellbender_tools_walkers_mutect_Mutect2.php) for variant calling. \n2. Calculates microsatellite instability (MSI) status using [MSIsensor2](https://github.com/niu-lab/msisensor2)\n3. Calculates tumor mutation burden (TMB) score using filtered variants.\n\nNote: This workflow utilizes the tool `Xenome` to removed mouse-reads from the raw-read data. `Xenome` uses host and graft reference sequences to characterize the set of all possible k-mers according to whether they belong to: only the graft (and not the host), only the host (and not the graft), both references, neither reference, and marginal asignments. This workflow uses those reads classified as 'human-only'.\n\n---\n\n### Essential Requirements\n\n  The following metadata fields are required and should be assigned to input read files:\n\n1. **Sample ID**: Any string. As this is the biospecimen identifier, it should be different for PDX (tumor) and normal sample. The workflow will run even if the strings are the same, however, please consider using different identifiers.\n2. **Paired-end**: 1 or 2\n3. **Sample type**: Any string. Please make sure to also provide the chosen tumor and normal sample types to the *SBG Split Pair by Metadata* tool (see below for details), if this is the field you wish to use to separate tumor and normal files. \n\n#### Optional (but recommended) Metadata\n1. **Case ID**. Any string. This allows for the tracking of case's. It is not required for running; however, the workflow will append this metadata field to the front of most outputs if present. \n\nThe workflow will process both uncompressed and compressed FASTQ files.\n\n---\n\n#### The following output files will be generated:\n\n       ---VARIANT OUTPUTS---\n       Tumor FASTQC reports\n       Tumor BAM file\n       Tumor QC reports - integrated and exome\n       Final annotated VCF and TAB files\n\n       ---MSI---\n       MSI Score File\n\n       ---TMB---\n      TMB Score File\n\n---\n\n### Reference Files and Workflow Details\n\nRequired reference input files:\n\n1. **Xenome** is used to classify reads as human or mouse. Xenome indices are built on hg38 and pseudoNOD genome (based on SNP incorporation into mm10 genome from Sanger [ftp://ftp-mouse.sanger.ac.uk/REL-1505-SNPs_Indels/]). The default value of k=25 is used during the indices preparation. \nDefault file input: Xenome_WGS_indices.tar.gz\n\n2. Reference FASTA file and secondary files (.FAI, .DICT). \nDefault file input: Homo_sapiens_assembly38.fasta (Homo_sapiens_assembly38.fasta.fai, Homo_sapiens_assembly38.dict)\nChromosome naming in the default input file: chr1, chr2... chrX, chrY, chrM.\n\n3. BWA indices were prepared using bwakit/0.7.15. GRCh38 files from the Broad GATK resource bundle (hg38_201601) were used. \nDefault file input: BWA_ref_files.tar.gz\n\n4. Additional reference input files (hg38-specific) from the Broad Institute GATK resource bundle (known indels):               \n     Homo_sapiens_assembly38.known_indels.vcf.gz (Homo_sapiens_assembly38.known_indels.vcf.gz.tbi)     \n     Mills_and_1000G_gold_standard.indels.hg38.vcf.gz (Mills_and_1000G_gold_standard.indels.hg38.vcf.gz.tbi)\n     hsa\\_dbSNP\\_v151\\_20170710chr\\_renamed.vcf.gz (hsa\\_dbSNP\\_v151\\_20170710chr\\_renamed.vcf.gz.tbi)     \n\n5. dbSNP v151 (20170710) file:\nDefault file input: hsa_dbSNP_v151_20170710chr_renamed.vcf.gz (hsa_dbSNP_v151_20170710chr_renamed.vcf.gz.tbi)\n\n6. ExAC sites (lifted over to GRCh38 and formatted to match the reference FASTA). Original file was downloaded from Ensembl (http://ftp.ensembl.org/pub/data_files/homo_sapiens/GRCh38/variation_genotype/ExAC.0.3.GRCh38.vcf.gz).\nDefault file input: ExAC.0.3.GRCh38_chr_added_bad_lift_over_removed_reorder_primary_only.vcf.gz (ExAC.0.3.GRCh38_chr_added_bad_lift_over_removed_reorder_primary_only.vcf.gz.tbi)\n\n7. Annotation database file for SnpEff (v4.3):\n     snpEff_v4_3_hg38.zip\n\n8. Annotation database files for SnpSift:\n     a) dbNSFP (by default, the workflow uses dbNSFP v3.2 (academic release): dbNSFP3.2a.txt.gz)\n     b) COSMIC (default file: Sorted_Cosmicv80_Coding_Noncoding.processed.vcf.gz, with .tbi index)\n\n9. Target region files (BED and INTERVALS_LIST format)\nThe BED file should correspond to the data being processed. Chromosome naming in the BED file should match the reference FASTA used to process the data (if using the default input FASTA file, please ensure that chromosome names begin with 'chr'). If a corresponding INTERVALS_LIST file (as used by Picard toolkit) is not available, it can easily be generated using the **GATK BedToIntervalList** tool.\n\n---\n\n### Workflow Steps\n\n#### Step 1: **SBG Split Pair by Metadata**\n\nThe first step separates tumor and normal FASTQ files for downstream processing, based on specified metadata criteria which should be supplied as inputs (normal_metadata and tumor_metadata) in the format metadata_key:value. For example, for files to be classified based on Sample type metadata (sample_type field), with the values \"tumor\" and \"normal\", the inputs should read: sample_type:normal and sample_type:tumor (for normal_metadata and tumor_metadata parameters, respectively).\n\n#### Step 2:  **Tumor Alignment and Target Coverage** \n\nIn the next step, tumor (PDX model) and normal FASTQ files are QC-checked (**FASTQC**), trimmed (**Trimmomatic**), aligned (**BWA**, alt-aware), sorted (**Picard SortSam**) and prepared for variant calling (**Picard MarkDuplicates**, **GATK BaseRecalibrator**, **GATK ApplyBQSR**). QC is also performed on the processed BAM files using **Exome Coverage QC 1.0** tool and **Picard CalculateHsMetrics**. QC reports are aggregated with the **QC Integrate** script. The two workflows are similar, except for **Xenome** preprocessing of PDX-derived data (tumor workflow) to remove mouse reads. \n\nImportant notes: \n\n1. **BWA** is set to hardcode \"tumor\" and \"normal\" as sample names (SM) in the output BAMs. This can be changed via the read group input parameter. However, please make sure to also adjust **Mutect** parameters tumor and normal sample name, if this is changed.\n\n2. By default, QC checks require that at least 75 % of every target region be covered at 20X. If this requirement is not met, the task will fail (See Target Coverage parameter of the **QC Integrate** tool to adjust this).\n\n#### Step 3: Indexing BAM files (**Samtools Index BAM**)  \n\n\n#### Step 4: Variant calling (**GATK 4 Mutect2** and **FilterMutectCalls**)\n\nThe variant calling step has been paralellized by invoking **Mutect2** and **GATK FilterMutectCalls** on smaller intervals in paralel (scatter). The intervals are prepared with the **SBG Prepare Intervals** tool and the output VCFs are collected and merged with the **SnpSift split** tool before annotation. \n\nImportant default parameter: As ExAC 0.3 is used, AF Of Alleles Not In Resource was set to (ExAC 0.3 ~ 60,706Exomes) = 1/(2* 60706)= 0.0000082364  \n\n#### Step 5: Annotation \n\nThe somatic VCF is annotated with SnpEff/SnpSift tools (v4.3). Basic annotation is done with **SnpEff** (using hg38 database). If you wish to use a different database, please make sure to also alter the Assembly input parameter of the SnpEff tool. \n**SnpSift dbNSFP** and **SnpSift annotate** tools are used to add additional annotations from the [dbNSFP](https://sites.google.com/site/jpopgen/dbNSFP) and [COSMIC](https://cancer.sanger.ac.uk/cosmic). Please note that these two annotation sources should be provided as tabix-indexed VCF.GZ files (with their .TBI index present)\n\nFor convenience, the output VCF is reformatted to hold one effect per line (script vcfEffOnePerLine.pl from SnpEff toolkit) and also converted to a tab-separated file (**SnpSift extractFields tool**).\n\n**Please note:** the GT genotype field present in the final VCF does not represent genotype in the traditional sense. Per [GATK documentation](https://software.broadinstitute.org/gatk/documentation/article?id=11127): A somatic caller should detect low fraction alleles, can make no explicit ploidy assumption and omits genotyping in the traditional sense. Mutect2 adheres to all of these criteria. A number of cancer sample characteristics necessitate such caller features. For one, biopsied tumor samples are commonly contaminated with normal cells, and the normal fraction can be much higher than the tumor fraction of a sample. Second, a tumor can be heterogeneous in its mutations. Third, these mutations not uncommonly include aneuploid events that change the copy number of a cell's genome in patchwork fashion.  \n\n#### Step 6: Microsattelite instability (MSI) Status Calculation\n\nThe tumor BAM file are passed to MSIsensor2 which calls MSI status. Note that the threshold as determined by the authors of the tool is- MSI high: msi score >= 20%.\n\n#### Step 7: Tumor Mutation Burden (TMB) Calculation\nTMB is calucated as the number of coding mutations per Mb of the genome. This is assessed using variants that meet all quality criteria (coverage, AF, mapping quality, strand bias etc.), are somatic and non-polymorphic, and are defined in SnpEff as 'high' or 'moderate' functional impact. As only a porition of the genome was sequenced, genome coverage (Mb) is calculated from the input target coverage BED file.",
  "extension_domain": {
    "fhir_extension": {
      "fhir_endpoint": "",
      "fhir_version": "",
      "fhir_resources": {}
    },
    "scm_extension": {
      "scm_repository": "",
      "scm_type": "git",
      "scm_commit": "",
      "scm_path": "",
      "scm_preview": ""
    }
  },
  "description_domain": {
    "keywords": ["test", "right", "now"],
    "xref": [],
    "platform": [
      "Seven Bridges Platform"
    ],
    "pipeline_steps": [
      {
        "step_number": "1",
        "name": "gatk_revertsam_4_1_0_0",
        "description": "The **GATK RevertSam** tool reverts SAM, BAM or CRAM files to the previous state. \n\nThis tool removes or restores certain properties of the SAM records, including alignment information, which can be used to produce an unmapped BAM (uBAM) from a previously aligned BAM. It is also capable of restoring the original quality scores of the BAM file that has already undergone base quality score recalibration (BQSR) if the original qualities were retained during the calibration (OQ tag) [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n* The **GATK RevertSam** tool requires a BAM/SAM/CRAM file on its **Input BAM/SAM/CRAM file** (`--INPUT`) input. The tool generates a single BAM file on its output by default, or SAM or CRAM if the input file is SAM or CRAM, respectively.\n\n* The **GATK RevertSam** tool supports an optional parameter  **Output by readgroup** (`--OUTPUT_BY_READGROUP`) which, when true, outputs each read group in a separate file. The output file format will be equal to the input file format. This behaviour can be overridden with the **Output by readgroup file format** (`--OUTPUT_BY_READGROUP_FILE_FORMAT`) argument. Outputting by read group can optionally be done by adding an output map on the **Output map** (`--OUTPUT_MAP`) input. The output map is a tab separated file which provides file mapping with two columns, READ_GROUP_ID and OUTPUT.\n\n* Usage Example - Output to a single file:\n```\n gatk RevertSam \\\\\n      --INPUT input.bam \\\\\n      --OUTPUT reverted.bam\n\n```\n\n* Usage Example - Output by read group into multiple files with sample map:\n\n```\n\n gatk RevertSam \\\\\n      --INPUT input.bam \\\\\n      --OUTPUT_BY_READGROUP true\\\\\n      --OUTPUT_MAP reverted_bam_paths.tsv\n\n```\n\n* Usage Example - Output by read group with no output map:\n\n```\n\ngatk RevertSam \\\\\n      --INPUT input.bam \\\\\n      --OUTPUT_BY_READGROUP true \\\\\n      --OUTPUT /write/reverted/read/group/bams/in/this/dir\n\n```\n\n###Changes Introduced by Seven Bridges\n\n* All output files will be prefixed using the **Output prefix** parameter. In case **Output prefix** is not provided, output prefix will be the same as the Sample ID metadata from the **Input SAM/BAM/CRAM file**, if the Sample ID metadata exists. Otherwise, output prefix will be inferred from the **Input SAM/BAM/CRAM file** filename. This way, having identical names of the output files between runs is avoided. Moreover,  **reverted** will be added before the extension of the output file name. \n\n* The user has a possibility to specify the output file format using the **Output file format** option. Otherwise, the output file format will be the same as the format of the input file.\n\n###Common Issues and Important Notes\n\n* Note: If the program fails due to a SAM validation error, consider setting the **Validation stringency** (`--VALIDATION_STRINGENCY`) option to LENIENT or SILENT if the failures are expected to be obviated by the reversion process (e.g. invalid alignment information will be obviated when the **Remove alignment information** (`--REMOVE_ALIGNMENT_INFORMATION`) option is used).\n\n###Performance Benchmarking\n\nBelow is a table describing runtimes and task costs of **GATK RevertSam** for a couple of different samples, executed on the AWS cloud instances:\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost | Instance (AWS) | \n|:--------------:|:------------:|:--------:|:-------:|:---------:|:----------:|:------:|:------:|\n|     RNA-Seq     |  1.3 GB |     Yes    |     16M     |     101     |   4min   | ~0.03$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     |  3.9 GB |     Yes    |     50M     |     101     |   6min   | ~0.04$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     | 6.5 GB |     Yes    |     82M    |     101     |  9min  | ~0.06$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     | 12.9 GB |     Yes    |     164M    |     101     |  16min  | ~0.11$ | c4.2xlarge (8 CPUs) |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n###References\n\n[1] [GATK RevertSam](https://software.broadinstitute.org/gatk/documentation/tooldocs/4.1.0.0/picard_sam_SamToFastq.php)",
        "version": "4.1.0.0",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "2",
        "name": "gatk_samtofastq_4_1_0_0",
        "description": "The **GATK SamToFastq** tool converts a SAM or BAM file to FASTQ.\n\nThis tool extracts read sequences and qualities from the input SAM/BAM file and writes them into the output file in Sanger FASTQ format.\n\nIn the RC mode (default is True), if the read is aligned and the alignment is to the reverse strand on the genome, the read sequence from input SAM file will be reverse-complemented prior to writing it to FASTQ in order to correctly restore the original read sequence as it was generated by the sequencer [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n* The **GATK SamToFastq** tool requires a BAM/SAM file on its **Input BAM/SAM file** (`--INPUT`) input. The tool generates a single-end FASTQ file on its **Output FASTQ file(s)** output if the input BAM/SAM file is single end. In case the input file is paired end, the tool outputs the first end of the pair FASTQ and the second end of the pair FASTQ on its **Output FASTQ file(s)** output, except when the **Interleave** (`--INTERLEAVE`) option is set to True. If the output is an interleaved FASTQ file, if paired, each line will have /1 or /2 to describe which end it came from.\n\n* The **GATK SamToFastq** tool supports an optional parameter  **Output by readgroup** (`--OUTPUT_BY_READGROUP`) which, when true, outputs a FASTQ file per read group (two FASTQ files per read group if the group is paired).\n\n* Usage example (input BAM file is single-end):\n\n```\ngatk SamToFastq \n     --INPUT input.bam\n     --FASTQ output.fastq\n```\n\n\n\n\n\n* Usage example (input BAM file is paired-end):\n\n```\ngatk SamToFastq \n     --INPUT input.bam\n     --FASTQ output.pe_1.fastq\n     --SECOND_END_FASTQ output.pe_2.fastq\n     --UNPAIRED_FASTQ unpaired.fastq\n\n```\n\n###Changes Introduced by Seven Bridges\n\n* The GATK SamToFastq tool is implemented to check if the input alignments file contains single-end or paired-end data and according to that generates different command lines for these two modes and thus produces appropriate output files on its **Output FASTQ file(s)** output (one FASTQ file in single-end mode and two FASTQ files if the input alignment file contains paired-end data). \n\n* All output files will be prefixed using the **Output prefix** parameter. In case the **Output prefix** is not provided, the output prefix will be the same as the Sample ID metadata from the **input SAM/BAM file**, if the Sample ID metadata exists. Otherwise, the output prefix will be inferred from the **Input SAM/BAM** filename. This way, having identical names of the output files between runs is avoided.\n\n* For paired-end read files, in order to successfully run alignment with STAR, this tool adds the appropriate **paired-end** metadata field in the output FASTQ files.\n\n###Common Issues and Important Notes\n\n* None\n\n###Performance Benchmarking\n\nBelow is a table describing runtimes and task costs of **GATK SamToFastq** for a couple of different samples, executed on the AWS cloud instances:\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost | Instance (AWS) | \n|:--------------:|:------------:|:--------:|:-------:|:---------:|:----------:|:------:|:------:|\n|     RNA-Seq     |  1.9 GB |     Yes    |     16M     |     101     |   4min   | ~0.03$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     |  5.7 GB |     Yes    |     50M     |     101     |   7min   | ~0.04$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     | 9.5 GB |     Yes    |     82M    |     101     |  10min  | ~0.07$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     | 19 GB |     Yes    |     164M    |     101     |  20min  | ~0.13$ | c4.2xlarge (8 CPUs) |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n###References\n\n[1] [GATK SamToFastq](https://software.broadinstitute.org/gatk/documentation/tooldocs/4.0.12.0/picard_sam_SamToFastq)",
        "version": "4.1.0.0",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "3",
        "name": "sbg_extract_basename",
        "description": "This tool is extracting basename root from a single file provided to the **Input file** port. This is performed using nameroot File property built into Common Workflow Language.",
        "version": "v1.0",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "4",
        "name": "gatk_mergebamalignment_4_1_0_0",
        "description": "The **GATK MergeBamAlignment** tool is used for merging BAM/SAM alignment info from a third-party aligner with the data in an unmapped BAM file, producing a third BAM file that has alignment data (from the aligner) and all the remaining data from the unmapped BAM.\n\nMany alignment tools still require FASTQ format input. The unmapped BAM may contain useful information that will be lost in the conversion to FASTQ (meta-data like sample alias, library, barcodes, etc... as well as read-level tags.) This tool takes an unaligned BAM with meta-data, and the aligned BAM produced by calling [SamToFastq](https://software.broadinstitute.org/gatk/documentation/tooldocs/4.1.0.0/picard_sam_SamToFastq.php) and then passing the result to an aligner. It produces a new SAM file that includes all aligned and unaligned reads and also carries forward additional read attributes from the unmapped BAM (attributes that are otherwise lost in the process of converting to FASTQ). The resulting file will be valid for use by Picard and GATK tools. The output may be coordinate-sorted, in which case the tags, NM, MD, and UQ will be calculated and populated, or query-name sorted, in which case the tags will not be calculated or populated [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n* The **GATK MergeBamAlignment** tool requires a SAM or BAM file on its **Aligned BAM/SAM file** (`--ALIGNED_BAM`) input, original SAM or BAM file of unmapped reads, which must be in queryname order on its **Unmapped BAM/SAM file** (`--UNMAPPED_BAM`) input and a reference sequence on its **Reference** (`--REFERENCE_SEQUENCE`) input. The tool generates a single BAM/SAM file on its **Output merged BAM/SAM file** output.\n\n* Usage example:\n\n```\ngatk MergeBamAlignment \\\\\n      --ALIGNED_BAM aligned.bam \\\\\n      --UNMAPPED_BAM unmapped.bam \\\\\n      --OUTPUT merged.bam \\\\\n      --REFERENCE_SEQUENCE reference_sequence.fasta\n```\n\n###Changes Introduced by Seven Bridges\n\n* The output file name will be prefixed using the **Output prefix** parameter. In case **Output prefix** is not provided, output prefix will be the same as the Sample ID metadata from **Input SAM/BAM file**, if the Sample ID metadata exists. Otherwise, output prefix will be inferred from the **Input SAM/BAM file** filename. This way, having identical names of the output files between runs is avoided. Moreover,  **merged** will be added before the extension of the output file name. \n\n* The user has a possibility to specify the output file format using the **Output file format** argument. Otherwise, the output file format will be the same as the format of the input aligned file.\n\n###Common Issues and Important Notes\n\n* Note:  This is not a tool for taking multiple BAM/SAM files and creating a bigger file by merging them. For that use-case, see [MergeSamFiles](https://software.broadinstitute.org/gatk/documentation/tooldocs/4.1.0.0/picard_sam_MergeSamFiles.php).\n\n###Performance Benchmarking\n\nBelow is a table describing runtimes and task costs of **GATK MergeBamAlignment** for a couple of different samples, executed on the AWS cloud instances:\n\n| Experiment type |  Aligned BAM/SAM size |  Unmapped BAM/SAM size | Duration |  Cost | Instance (AWS) | \n|:--------------:|:------------:|:--------:|:-------:|:---------:|:----------:|:------:|:------:|------:|\n|     RNA-Seq     |  1.4 GB |  1.9 GB |   9min   | ~0.06$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     |  4.0 GB |  5.7 GB |   20min   | ~0.13$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     | 6.6 GB | 9.5 GB |  32min  | ~0.21$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     | 13 GB | 19 GB |  1h 4min  | ~0.42$ | c4.2xlarge (8 CPUs) |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n###References\n\n[1] [GATK MergeBamAlignment](https://software.broadinstitute.org/gatk/documentation/tooldocs/4.1.0.0/picard_sam_MergeBamAlignment.php)",
        "version": "4.1.0.0",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "5",
        "name": "gatk_markduplicates_4_1_0_0",
        "description": "The **GATK  MarkDuplicates** tool identifies duplicate reads in a BAM or SAM file.\n\nThis tool locates and tags duplicate reads in a BAM or SAM file, where duplicate reads are defined as originating from a single fragment of DNA. Duplicates can arise during sample preparation e.g. library construction using PCR. Duplicate reads can also result from a single amplification cluster, incorrectly detected as multiple clusters by the optical sensor of the sequencing instrument. These duplication artifacts are referred to as optical duplicates [1].\n\nThe MarkDuplicates tool works by comparing sequences in the 5 prime positions of both reads and read-pairs in the SAM/BAM file. The **Barcode tag** (`--BARCODE_TAG`) option is available to facilitate duplicate marking using molecular barcodes. After duplicate reads are collected, the tool differentiates the primary and duplicate reads using an algorithm that ranks reads by the sums of their base-quality scores (default method).\n\n\n###Common Use Cases\n\n* The **GATK MarkDuplicates** tool requires the BAM or SAM file on its **Input BAM/SAM file** (`--INPUT`) input. The tool generates a new SAM or BAM file on its **Output BAM/SAM** output, in which duplicates have been identified in the SAM flags field for each read. Duplicates are marked with the hexadecimal value of 0x0400, which corresponds to a decimal value of 1024. If you are not familiar with this type of annotation, please see the following [blog post](https://software.broadinstitute.org/gatk/blog?id=7019) for additional information. **MarkDuplicates** also produces a metrics file on its **Output metrics file** output, indicating the numbers of duplicates for both single and paired end reads.\n\n* The program can take either coordinate-sorted or query-sorted inputs, however the behavior is slightly different. When the input is coordinate-sorted, unmapped mates of mapped records and supplementary/secondary alignments are not marked as duplicates. However, when the input is query-sorted (actually query-grouped), then unmapped mates and secondary/supplementary reads are not excluded from the duplication test and can be marked as duplicate reads.\n\n* If desired, duplicates can be removed using the **Remove duplicates** (`--REMOVE_DUPLICATES`) and **Remove sequencing duplicates** ( `--REMOVE_SEQUENCING_DUPLICATES`) options.\n\n* Although the bitwise flag annotation indicates whether a read was marked as a duplicate, it does not identify the type of duplicate. To do this, a new tag called the duplicate type (DT) tag was recently added as an optional output of a SAM/BAM file. Invoking the **Tagging policy** ( `--TAGGING_POLICY`) option, you can instruct the program to mark all the duplicates (All), only the optical duplicates (OpticalOnly), or no duplicates (DontTag). The records within the output SAM/BAM file will have values for the 'DT' tag (depending on the invoked **TAGGING_POLICY** option), as either library/PCR-generated duplicates (LB), or sequencing-platform artifact duplicates (SQ). \n\n* This tool uses the **Read name regex** (`--READ_NAME_REGEX`) and the **Optical duplicate pixel distance** (`--OPTICAL_DUPLICATE_PIXEL_DISTANCE`) options as the primary methods to identify and differentiate duplicate types. Set **READ_NAME_REGEX** to null to skip optical duplicate detection, e.g. for RNA-seq or other data where duplicate sets are extremely large and estimating library complexity is not an aim. Note that without optical duplicate counts, library size estimation will be inaccurate.\n\n* Usage example:\n\n```\ngatk MarkDuplicates \\\n      --INPUT input.bam \\\n      --OUTPUT marked_duplicates.bam \\\n      --METRICS_FILE marked_dup_metrics.txt\n```\n\n###Changes Introduced by Seven Bridges\n\n* All output files will be prefixed using the **Output prefix** parameter. In case **Output prefix** is not provided, output prefix will be the same as the Sample ID metadata from the **Input SAM/BAM file**, if the Sample ID metadata exists. Otherwise, output prefix will be inferred from the **Input SAM/BAM** filename. This way, having identical names of the output files between runs is avoided. Moreover,  **dedupped** will be added before the extension of the output file name. \n\n* The user has a possibility to specify the output file format using the **Output file format** option. Otherwise, the output file format will be the same as the format of the input file.\n\n###Common Issues and Important Notes\n\n* None\n\n###Performance Benchmarking\n\nBelow is a table describing runtimes and task costs of **GATK MarkDuplicates** for a couple of different samples, executed on the AWS cloud instances:\n\n| Experiment type |  Input size | Duration |  Cost | Instance (AWS) | \n|:--------------:|:------------:|:--------:|:-------:|:---------:|\n|     RNA-Seq     |  1.8 GB |   3min   | ~0.02$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     |  5.3 GB |   9min   | ~0.06$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     | 8.8 GB |  16min  | ~0.11$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     | 17 GB |  30min  | ~0.20$ | c4.2xlarge (8 CPUs) |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n###References\n\n[1] [GATK MarkDuplicates](https://software.broadinstitute.org/gatk/documentation/tooldocs/4.1.0.0/picard_sam_markduplicates_MarkDuplicates.php)",
        "version": "4.1.0.0",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "6",
        "name": "gatk_splitncigarreads_4_1_0_0",
        "description": "The **GATK SplitNCigarReads** tool splits reads that contain Ns in their CIGAR string (e.g. spanning splicing events in RNAseq data) from the input alignment file and outputs an alignment file with reads split at N CIGAR elements and CIGAR strings updated. \n\nThis tool identifies all N cigar elements in sequence reads, and creates k+1 new reads (where k is the number of N cigar elements) that correspond to the segments of the original read beside/between the splicing events represented by the Ns in the original CIGAR. The first read includes the bases that are to the left of the first N element, while the part of the read that is to the right of the N (including the Ns) is hard clipped, and so on for the rest of the new reads [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n* The **GATK SplitNCigarReads** tool requires a BAM/SAM/CRAM file on its **Input BAM/SAM/CRAM file** (`--input`) input and a reference file on its **Reference** (`--reference`) input. The tool generates a single BAM file on its **Output split BAM/SAM/CRAM file** output by default, or SAM or CRAM if the input file is SAM or CRAM, respectively.\n\n* Usage example:\n\n```\n    gatk SplitNCigarReads \\\n      --reference Homo_sapiens_assembly38.fasta \\\n      --input input.bam \\\n      --output output.bam\n```\n\n###Changes Introduced by Seven Bridges\n\n* The output file name will be prefixed using the **Output prefix** parameter. In case **Output prefix** is not provided, output prefix will be the same as the Sample ID metadata from **Input SAM/BAM/CRAM file**, if the Sample ID metadata exists. Otherwise, output prefix will be inferred from the **Input SAM/BAM/CRAM file** filename. This way, having identical names of the output files between runs is avoided. Moreover,  **split** will be added right before the extension of the output file name. \n\n* The user has a possibility to specify the output file format using the **Output file format** argument. Otherwise, the output file format will be the same as the format of the input file.\n\n* **Include intervals** (`--intervals`) option is divided into **Include intervals string** and **Include intervals file** options.\n\n* **Exclude intervals** (`--exclude-intervals`) option is divided into **Exclude intervals string** and **Exclude intervals file** options.\n\n###Common Issues and Important Notes\n\n* Note: The **AllowAllReadsReadFilter** read filter (do not filter out any read) is automatically applied to the data by the Engine before processing by SplitNCigarReads.\n* Note: If **Read filter** (`--read-filter`) option is set to \"LibraryReadFilter\", **Library** (`--library`) option must be set to some value.\n* Note: If **Read filter** (`--read-filter`) option is set to \"PlatformReadFilter\", **Platform filter name** (`--platform-filter-name`) option must be set to some value.\n* Note: If **Read filter** (`--read-filter`) option is set to\"PlatformUnitReadFilter\", **Black listed lanes** (`--black-listed-lanes`) option must be set to some value. \n* Note: If **Read filter** (`--read-filter`) option is set to \"ReadGroupBlackListReadFilter\", **Read group black list** (`--read-group-black-list`) option must be set to some value.\n* Note: If **Read filter** (`--read-filter`) option is set to \"ReadGroupReadFilter\", **Keep read group** (`--keep-read-group`) option must be set to some value.\n* Note: If **Read filter** (`--read-filter`) option is set to \"ReadLengthReadFilter\", **Max read length** (`--max-read-length`) option must be set to some value.\n* Note: If **Read filter** (`--read-filter`) option is set to \"ReadNameReadFilter\", **Read name** (`--read-name`) option must be set to some value.\n* Note: If **Read filter** (`--read-filter`) option is set to \"ReadStrandFilter\", **Keep reverse strand only** (`--keep-reverse-strand-only`) option must be set to some value.\n* Note: If **Read filter** (`--read-filter`) option is set to \"SampleReadFilter\", **Sample** (`--sample`) option must be set to some value.\n\n###Performance Benchmarking\n\nBelow is a table describing runtimes and task costs of **GATK SplitNCigarReads** for a couple of different samples, executed on the AWS cloud instances:\n\n| Experiment type |  Input size | Duration |  Cost | Instance (AWS) | \n|:--------------:|:------------:|:--------:|:-------:|:---------:|\n|     RNA-Seq     |  1.9 GB |   29min   | ~0.19$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     |  5.4 GB |   1h 10min   | ~0.46$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     | 8.8 GB |  2h 2min  | ~0.80$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     | 17 GB |  4h 15min  | ~1.69$ | c4.2xlarge (8 CPUs) |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n###References\n\n[1] [GATK SplitNCigarReads](https://software.broadinstitute.org/gatk/documentation/tooldocs/4.1.0.0/org_broadinstitute_hellbender_tools_walkers_rnaseq_SplitNCigarReads.php)",
        "version": "4.1.0.0",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "7",
        "name": "gatk_baserecalibrator_4_1_0_0",
        "description": "The **GATK BaseRecalibrator** tool performs first pass of Base Quality Score Recalibration (BQSR) based on various covariates. The default covariates are read group, reported quality score, machine cycle, and nucleotide context. The tool generates on a recalibration table on its output.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n* The **GATK BaseRecalibrator** tool requires the input read data whose quality scores need to be assessed on its **Input BAM/SAM/CRAM file** (`--input`) input and the database of known polymorphic sites to skip over on its **Known SNPs** and **Known INDELs** (`--known-sites`) inputs. The tool generates on its **Output recalibration report** output a GATK report file with many tables: the list of arguments, the quantized qualities table, the recalibration table by read group, the recalibration table by quality score,\nthe recalibration table for all the optional covariates.\n\n* Usage example:\n\n```\ngatk BaseRecalibrator \\\n   --input my_reads.bam \\\n   --reference reference.fasta \\\n   --known-sites sites_of_variation.vcf \\\n   --known-sites another/optional/setOfSitesToMask.vcf \\\n   --output recal_data.table\n\n```\n\n###Changes Introduced by Seven Bridges\n\n* All output files will be prefixed using the **Output prefix** parameter. In case **Output prefix** is not provided, output prefix will be the same as the Sample ID metadata from **Input SAM/BAM/CRAM file**, if the Sample ID metadata exists. Otherwise, output prefix will be inferred from the **Input SAM/BAM/CRAM** filename. This way, having identical names of the output files between runs is avoided. Moreover,  **recal_data** will be added before the extension of the output file name which is CSV by default. \n\n* **Include intervals** (`--intervals`) option is divided into **Include intervals string** and **Include intervals file** options.\n\n* **Exclude intervals** (`--exclude-intervals`) option is divided into **Exclude intervals string** and **Exclude intervals file** options.\n\n###Common Issues and Important Notes\n\n* Note: If **Read filter** (`--read-filter`) option is set to \"LibraryReadFilter\", **Library** (`--library`) option must be set to some value.\n* Note: If **Read filter** (`--read-filter`) option is set to \"PlatformReadFilter\", **Platform filter name** (`--platform-filter-name`) option must be set to some value.\n* Note: If **Read filter** (`--read-filter`) option is set to\"PlatformUnitReadFilter\", **Black listed lanes** (`--black-listed-lanes`) option must be set to some value. \n* Note: If **Read filter** (`--read-filter`) option is set to \"ReadGroupBlackListReadFilter\", **Read group black list** (`--read-group-black-list`) option must be set to some value.\n* Note: If **Read filter** (`--read-filter`) option is set to \"ReadGroupReadFilter\", **Keep read group** (`--keep-read-group`) option must be set to some value.\n* Note: If **Read filter** (`--read-filter`) option is set to \"ReadLengthReadFilter\", **Max read length** (`--max-read-length`) option must be set to some value.\n* Note: If **Read filter** (`--read-filter`) option is set to \"ReadNameReadFilter\", **Read name** (`--read-name`) option must be set to some value.\n* Note: If **Read filter** (`--read-filter`) option is set to \"ReadStrandFilter\", **Keep reverse strand only** (`--keep-reverse-strand-only`) option must be set to some value.\n* Note: If **Read filter** (`--read-filter`) option is set to \"SampleReadFilter\", **Sample** (`--sample`) option must be set to some value.\n\n###Performance Benchmarking\n\nBelow is a table describing runtimes and task costs of **GATK BaseRecalibrator** for a couple of different samples, executed on the AWS cloud instances:\n\n| Experiment type |  Input size | Duration |  Cost | Instance (AWS) | \n|:--------------:|:------------:|:--------:|:-------:|:---------:|\n|     RNA-Seq     |  2.2 GB |   8min   | ~0.05$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     |  6.6 GB |   18min   | ~0.12$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     | 11 GB |  26min  | ~0.17$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     | 22 GB |  45min  | ~0.29$ | c4.2xlarge (8 CPUs) |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n###References\n\n[1] [GATK BaseRecalibrator](https://software.broadinstitute.org/gatk/documentation/tooldocs/4.1.0.0/org_broadinstitute_hellbender_tools_walkers_bqsr_BaseRecalibrator.php)",
        "version": "4.1.0.0",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "8",
        "name": "gatk_applybqsr_4_1_0_0",
        "description": "The **GATK ApplyBQSR** tool recalibrates the base qualities score of the input reads and outputs a recalibrated BAM, SAM or CRAM file. \n\nThis tool performs the second pass in a two-stage process called Base Quality Score Recalibration (BQSR). Specifically, it recalibrates the base qualities of the input reads based on the recalibration table produced by the **BaseRecalibrator** tool. The goal of this procedure is to correct systematic biasses that affect the assignment of base quality scores by the sequencer. The first pass consists of calculating error empirically and finding patterns in how error varies with basecall features over all bases. The relevant observations are written to the recalibration table. The second pass consists of applying numerical corrections to each individual basecall based on the patterns identified in the first step (recorded in the recalibration table) and write out the recalibrated data to a new BAM, SAM or CRAM file [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n* The **GATK ApplyBQSR** tool requires the BAM, SAM or CRAM file on its **Input BAM/SAM/CRAM file** (`--input`) input and the covariates table (= recalibration file) generated by the **BaseRecalibrator** tool on its **BQSR recal file** input (`--bqsr-recal-file`). The tool generates on its **Output BAM/SAM/CRAM** output a new alignments file which contains recalibrated read data.\n\n* Usage example\n\n```\n gatk ApplyBQSR \\\n   --reference reference.fasta \\\n   --input input.bam \\\n   --bqsr-recal-file recalibration.table \\\n   --output output.bam\n\n```\n\n* If the input alignments file is in CRAM format, the reference sequence is required on the **Reference sequence** (`--reference`) input of the tool.\n\n* Original qualities can be retained in the output file under the \"OQ\" tag if desired. See the **Emit original quals** (`--emit-original-quals`) argument for details.\n\n###Changes Introduced by Seven Bridges\n\n* All output files will be prefixed using the **Output prefix** parameter. In case **Output prefix** is not provided, output prefix will be the same as the Sample ID metadata from **Input SAM/BAM/CRAM file**, if the Sample ID metadata exists. Otherwise, output prefix will be inferred from the **Input SAM/BAM/CRAM** filename. This way, having identical names of the output files between runs is avoided. Moreover,  **recalibrated** will be added before the extension of the output file name. \n\n* The user has a possibility to specify the output file format using the **Output file format** argument. Otherwise, the output file format will be the same as the format of the input file.\n\n* **Include intervals** (`--intervals`) option is divided into **Include intervals string** and **Include intervals file** options.\n\n* **Exclude intervals** (`--exclude-intervals`) option is divided into **Exclude intervals string** and **Exclude intervals file** options.\n\n###Common Issues and Important Notes\n\n* Note: This tool replaces the use of PrintReads for the application of base quality score recalibration as practiced in earlier versions of GATK (2.x and 3.x) [1].\n* Note: You should only run **ApplyBQSR** with the covariates table created from the input BAM, SAM or CRAM file [1].\nInput BAM/SAM/CRAM file\n* Note: If **Read filter** (`--read-filter`) option is set to \"LibraryReadFilter\", **Library** (`--library`) option must be set to some value.\n* Note: If **Read filter** (`--read-filter`) option is set to \"PlatformReadFilter\", **Platform filter name** (`--platform-filter-name`) option must be set to some value.\n* Note: If **Read filter** (`--read-filter`) option is set to\"PlatformUnitReadFilter\", **Black listed lanes** (`--black-listed-lanes`) option must be set to some value. \n* Note: If **Read filter** (`--read-filter`) option is set to \"ReadGroupBlackListReadFilter\", **Read group black list** (`--read-group-black-list`) option must be set to some value.\n* Note: If **Read filter** (`--read-filter`) option is set to \"ReadGroupReadFilter\", **Keep read group** (`--keep-read-group`) option must be set to some value.\n* Note: If **Read filter** (`--read-filter`) option is set to \"ReadLengthReadFilter\", **Max read length** (`--max-read-length`) option must be set to some value.\n* Note: If **Read filter** (`--read-filter`) option is set to \"ReadNameReadFilter\", **Read name** (`--read-name`) option must be set to some value.\n* Note: If **Read filter** (`--read-filter`) option is set to \"ReadStrandFilter\", **Keep reverse strand only** (`--keep-reverse-strand-only`) option must be set to some value.\n* Note: If **Read filter** (`--read-filter`) option is set to \"SampleReadFilter\", **Sample** (`--sample`) option must be set to some value.\n\n###Performance Benchmarking\n\nBelow is a table describing runtimes and task costs of **GATK ApplyBQSR** for a couple of different samples, executed on the AWS cloud instances:\n\n| Experiment type |  Input size | Duration |  Cost | Instance (AWS) | \n|:--------------:|:------------:|:--------:|:-------:|:---------:|\n|     RNA-Seq     |  2.2 GB |   7min   | ~0.05$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     |  6.6 GB |   22min   | ~0.12$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     | 11 GB |  36min  | ~0.17$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     | 22 GB |  1h 15min  | ~0.29$ | c4.2xlarge (8 CPUs) |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n###References\n\n[1] [GATK ApplyBQSR](https://software.broadinstitute.org/gatk/documentation/tooldocs/4.1.0.0/org_broadinstitute_hellbender_tools_walkers_bqsr_ApplyBQSR.php)",
        "version": "4.1.0.0",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "9",
        "name": "gatk_haplotypecaller_4_1_0_0",
        "description": "Call germline single nucleotide polymorphisms (SNPs) and indels via local re-assembly of haplotypes. To call SNPs and indels, **HaplotypeCaller** requires BAM file(s) containing reads aligned to the reference genome.\n\n**HaplotypeCaller** is capable of calling SNPs and indels simultaneously via local de-novo assembly of haplotypes in an active region. In other words, whenever the program encounters a region showing signs of variation, it discards the existing mapping information and completely reassembles the reads in that region. Reassembled reads are realigned to the reference. This allows **HaplotypeCaller** to be more accurate when calling regions that are traditionally difficult to call, for example when they contain different types of variants close to each other. It also makes **HaplotypeCaller** much better at calling indels than position-based callers like UnifiedGenotyper.\n\nIn the GVCF workflow used for scalable variant calling in DNA sequence data, **HaplotypeCaller** runs per-sample to generate an intermediate GVCF (not to be used in final analysis), which can then be used in GenotypeGVCFs for joint genotyping of multiple samples in a very efficient way. The GVCF workflow enables rapid incremental processing of samples as they roll off the sequencer, as well as scaling to very large cohort sizes. \n\nIn addition, **HaplotypeCaller** is able to handle non-diploid organisms as well as pooled experiment data. Note however that the algorithms used to calculate variant likelihoods are not well suited to extreme allele frequencies (relative to ploidy) so its use is not recommended for somatic (cancer) variant discovery. For that purpose, use **Mutect2** instead.\n\nFinally, **HaplotypeCaller** is also able to correctly handle splice junctions that make RNAseq a challenge for most variant callers.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\n- Call variants individually on each sample in GVCF mode\n\n```\n gatk --java-options \"-Xmx4g\" HaplotypeCaller  \\\n   -R Homo_sapiens_assembly38.fasta \\\n   -I input.bam \\\n   -O output.g.vcf.gz \\\n   -ERC GVCF\n```\n\n\n- Call variants individually on each sample in GVCF mode with allele-specific annotations. [Here](https://software.broadinstitute.org/gatk/documentation/article?id=9622) you can read more details about allele-specific annotation and filtering.\n\n```\ngatk --java-options \"-Xmx4g\" HaplotypeCaller  \\\n   -R Homo_sapiens_assembly38.fasta \\\n   -I input.bam \\\n   -O output.g.vcf.gz \\\n   -ERC GVCF \\\n   -G Standard \\\n   -G AS_Standard\n```\n\n\n- Call variants with [bamout](https://software.broadinstitute.org/gatk/documentation/article?id=5484) to show realigned reads. After performing a local reassembly and realignment the reads get moved to different mapping positions than what you can observe in the original BAM file. This option could be used to visualize what rearrangements **HaplotypeCaller** has made.\n\n```\n gatk --java-options \"-Xmx4g\" HaplotypeCaller  \\\n   -R Homo_sapiens_assembly38.fasta \\\n   -I input.bam \\\n   -O output.vcf.gz \\\n   -bamout bamout.bam\n```\n\n\n### Common issues and important notes\n\n- If **Read filter** (`--read-filter`) option is set to \"LibraryReadFilter\", **Library** (`--library`) option must be set to some value.\n- If **Read filter** (`--read-filter`) option is set to \"PlatformReadFilter\", **Platform filter name** (`--platform-filter-name`) option must be set to some value.\n- If **Read filter** (`--read-filter`) option is set to\"PlatformUnitReadFilter\", **Black listed lanes** (`--black-listed-lanes`) option must be set to some value. \n- If **Read filter** (`--read-filter`) option is set to \"ReadGroupBlackListReadFilter\", **Read group black list** (`--read-group-black-list`) option must be set to some value.\n- If **Read filter** (`--read-filter`) option is set to \"ReadGroupReadFilter\", **Keep read group** (`--keep-read-group`) option must be set to some value.\n- If **Read filter** (`--read-filter`) option is set to \"ReadLengthReadFilter\", **Max read length** (`--max-read-length`) option must be set to some value.\n- If **Read filter** (`--read-filter`) option is set to \"ReadNameReadFilter\", **Read name** (`--read-name`) option must be set to some value.\n- If **Read filter** (`--read-filter`) option is set to \"ReadStrandFilter\", **Keep reverse strand only** (`--keep-reverse-strand-only`) option must be set to some value.\n- If **Read filter** (`--read-filter`) option is set to \"SampleReadFilter\", **Sample** (`--sample`) option must be set to some value.\n- When working with PCR-free data, be sure to set **PCR indel model** (`--pcr_indel_model`) to NONE.\n- When running **Emit ref confidence** ( `--emit-ref-confidence`) in GVCF or in BP_RESOLUTION mode, the confidence threshold is automatically set to 0. This cannot be overridden by the command line. The threshold can be set manually to the desired level when using **GenotypeGVCFs**.\n- It is recommended to use a list of intervals to speed up the analysis. See [this document](https://software.broadinstitute.org/gatk/documentation/article?id=4133) for details.\n\n### Changes Introduced by Seven Bridges\n\n- **Include intervals** (`--intervals`) option is divided into **Include genomic intervals** and **Intervals string values** options.\n- **Exclude intervals** (`--exclude-intervals`) option is divided into **Exclude genomic intervals** and **Exclude intervals string values** options.\n- Using the **Output prefix** parameter you can set the name of the VCF output. If this value is not set the output name will be generated based on **Sample ID** metadata value from one of the input BAM files. If **Sample ID** value is not set the name will be inherited from one of the input BAM file names.\n\n### Performance Benchmarking\n\nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes.\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost (spot) | Cost (on-demand) | AWS instance type |\n|:--------------:|:------------:|:--------:|:-------:|:---------:|:----------:|:------:|:------:|:------:|\n|     RNA-Seq     | 2.6 GB |     Yes    |     16M     |     101     |   50min   | 0.22$ | 0.44$ | c4.2xlarge |\n|     RNA-Seq     | 7.7 GB |     Yes    |     50M     |     101     |   1h31min   | 0.40$ | 0.87$ | c4.2xlarge |\n|     RNA-Seq     | 12.7 GB |     Yes    |     82M    |     101     |  2h19min  | 0.61$ | 1.22$ | c4.2xlarge |\n|     RNA-Seq     | 25 GB |     Yes    |     164M    |     101     |  4h5min  | 1.07$ | 2.43 | c4.2xlarge |",
        "version": "4.1.0.0",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "10",
        "name": "gatk_intervallisttools_4_1_0_0",
        "description": "This tool offers multiple interval list file manipulation capabilities including sorting, merging, subtracting, padding, and other set-theoretic operations. \n\nThe default action is to merge and sort genomic intervals provided as the input. Compatible input files are INTERVAL_LIST and VCF files. **IntervalListTools** can also \"scatter\" the output into many interval files. This can be useful for creating multiple interval lists for scattering an analysis execution.\n\n###Common Use Cases\n\n- Combine the intervals from two interval lists:\n```\njava -jar picard.jar IntervalListTools \\\n      ACTION=CONCAT \\\n      I=input.interval_list \\\n      I=input_2.interval_list \\\n      O=new.interval_list\n```\n- Combine the intervals from two interval lists, sorting and merging overlapping and abutting intervals:\n```\n java -jar picard.jar IntervalListTools \\\n       ACTION=CONCAT \\\n       SORT=true \\\n       UNIQUE=true \\\n       I=input.interval_list \\\n       I=input_2.interval_list \\\n       O=new.interval_list \n```\n- Subtract the intervals in **second_input** (`SECOND_INPUT`) from those in **in_intervals** (`INPUT`):\n```\n java -jar picard.jar IntervalListTools \\\n       ACTION=SUBTRACT \\\n       I=input.interval_list \\\n       SI=input_2.interval_list \\\n       O=new.interval_list \n```\n- Find bases that are in either *input1.interval_list* or *input2.interval_list*, and also in *input3.interval_list*:\n```\n java -jar picard.jar IntervalListTools \\\n       ACTION=INTERSECT \\\n       I=input1.interval_list \\\n       I=input2.interval_list \\\n       SI=input3.interval_list \\\n       O=new.interval_list \n```\n- Split intervals list file using * scatter_count* (`SCATTER_COUNT`) option:\n```\n java -jar picard.jar IntervalListTools \\\n       I=input.interval_list \\\n       SCATTER_COUNT=2 \n```\n\n\n###Common Issues and Important Notes\n\n- A SAM style header must be present at the top of the *interval_list* file. After the header, the file then contains records, one per line in text format with the following tab-separated values. Example of the *interval_list* file: \n```\n@HD    VN:1.0\n@SQ    SN:chr1    LN:501\n@SQ    SN:chr2    LN:401\nchr1    1    100    +    starts at the first base of the contig and covers 100 bases\nchr2    100    100    +    interval with exactly one base\n```\n- The coordinate system is 1-based, closed-ended so that the first base in a sequence has position 1, and both the start and the end positions are included in an interval.\n-  The **Interval list** input file should be denoted with the extension INTERVAL_LIST.\n\n\n###Changes Introduced by Seven Bridges\n\nIf no additional parameter is set, the app will output the INTERVAL_LIST file given on the input.\n\n\n###Performance Benchmarking\nThe execution time takes several minutes on the default instance. Unless specified otherwise, the default AWS instance used to run the **IntervalListTools** will be c4.2xlarge (8CPUs and 16GB RAM).",
        "version": "4.1.0.0",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "11",
        "name": "gatk_mergevcfs_4_1_0_0",
        "description": "The **GATK MergeVcfs** tool combines multiple variant files into a single variant file. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n* The **MergeVcfs** tool requires one or more input files in VCF format on its **Input variant files** (`--INPUT`) input. The input files can be in VCF format (can be gzipped, i.e. ending in \".vcf.gz\", or binary compressed, i.e. ending in \".bcf\"). The tool generates a VCF file on its **Output merged VCF or BCF file** output.\n\n* The **MergeVcfs** tool supports a sequence dictionary file (typically name ending in .dict) on its **Sequence dictionary** (`--SEQUENCE_DICTIONARY`) input if the input VCF does not contain a complete contig list and if the output index is to be created (true by default).\n\n* The output file is sorted (i) according to the dictionary and (ii) by coordinate.\n\n* Usage example:\n\n```\ngatk MergeVcfs \\\n          --INPUT input_variants.01.vcf \\\n          --INPUT input_variants.02.vcf.gz \\\n          --OUTPUT output_variants.vcf.gz\n```\n\n###Changes Introduced by Seven Bridges\n\n* The output file will be prefixed using the **Output prefix** parameter. In case **Output prefix** is not provided, the input files provided on the **Input variant files** input will be alphabetically sorted by name and  output prefix will be equal to the Sample ID metadata from the first element from that list, if the Sample ID metadata exists. Otherwise, output prefix will be inferred from the filename of the first element from this list. Moreover, the number of input files will be added after the output prefix as well as the tool specific extension which is **merged**. This way, having identical names of the output files between runs is avoided.\n\n* The user has a possibility to specify the output file format using the **Output file format** argument. The default output format is \"vcf.gz\".\n\n###Common Issues and Important Notes\n\n* Note 1: If running this tool on multi-sample input files (originating from e.g. some scatter-gather runs), the input files must contain the same sample names in the same column order. \n\n* Note 2: Input file headers must contain compatible declarations for common annotations (INFO, FORMAT fields) and filters.\n\n* Note 3: Input files variant records must be sorted by their contig and position following the sequence dictionary provided or the header contig list.\n\n###Performance Benchmarking\n\nThis tool is ultra fast, with a running time less than a minute on the default AWS c4.2xlarge instance.\n\n###References\n\n[1] [GATK MergeVcfs](https://software.broadinstitute.org/gatk/documentation/tooldocs/4.1.0.0/picard_vcf_MergeVcfs.php)",
        "version": "4.0.12.0",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "12",
        "name": "gatk_variantfiltration_4_1_0_0",
        "description": "The **GATK VariantFiltration** tool filters variant calls of the input VCF file based on INFO and/or FORMAT annotations and outputs a filter VCF file. \n\nThis tool is designed for hard-filtering variant calls based on certain criteria. Records are hard-filtered by changing the value in the FILTER field to something other than PASS. Filtered records will be preserved in the output unless their removal is requested in the command line [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n* The **GATK VariantFiltration** tool requires the VCF file on its **Input variants file** (`--variant`) input and a reference file on its **Reference** (`--reference`) input. The tool generates a filtered VCF file on its **Output filtered variants file** output.\n\n* Usage example:\n```\n   gatk VariantFiltration \\\n   --reference reference.fasta \\\n   --variant input.vcf.gz \\\n   --output output.vcf.gz \\\n   --filter-name \"my_filter1\" \\\n   --filter-expression \"AB < 0.2\" \\\n   --filter-name \"my_filter2\" \\\n   --filter-expression \"MQ0 > 50\"\n\n```\n\n###Changes Introduced by Seven Bridges\n\n* All output files will be prefixed using the **Output prefix** parameter. In case **Output prefix** is not provided, output prefix will be the same as the Sample ID metadata from **Input variants file**, if the Sample ID metadata exists. Otherwise, output prefix will be inferred from the **Input variants** filename. This way, having identical names of the output files between runs is avoided. Moreover,  **filtered** will be added before the extension of the output file name. \n\n* The user has a possibility to specify the output file format using the **Output file format** argument. Otherwise, the output will be in the compressed VCF file format.\n\n###Common Issues and Important Notes\n\n* Note: Composing filtering expressions can range from very simple to extremely complicated depending on what you're trying to do.\n\nCompound expressions (ones that specify multiple conditions connected by &&, AND, ||, or OR, and reference multiple attributes) require special consideration. By default, variants that are missing one or more of the attributes referenced in a compound expression are treated as PASS for the entire expression, even if the variant would satisfy the filter criteria for another part of the expression. This can lead to unexpected results if any of the attributes referenced in a compound expression are present for some variants, but missing for others.\n\nIt is strongly recommended to provide such expressions as individual arguments, each referencing a single attribute and specifying a single criteria. This ensures that all of the individual expression are applied to each variant, even if a given variant is missing values for some of the expression conditions.\n\nAs an example, multiple individual expressions provided like this:\n\n```\n   gatk VariantFiltration \\\n   --reference reference.fasta \\\n   --variant input.vcf.gz \\\n   --output output.vcf.gz \\\n   --filter-name \"my_filter1\" \\\n   --filter-expression \"AB < 0.2\" \\\n   --filter-name \"my_filter2\" \\\n   --filter-expression \"MQ0 > 50\"\n \n```\n\nare preferable to a single compound expression such as this:\n\n```\n    gatk VariantFiltration \\\n    --reference reference.fasta \\\n    --variant input.vcf.gz \\\n    --output output.vcf.gz \\\n    --filter-name \"my_filter\" \\\n    --filter-expression \"AB < 0.2 || MQ0 > 50\"\n  \n```\n\n###Performance Benchmarking\n\nThis tool is ultra fast, with a running time less than a minute on the default AWS c4.2xlarge instance.\n\n\n###References\n\n[1] [GATK VariantFiltration](https://software.broadinstitute.org/gatk/documentation/tooldocs/4.1.0.0/org_broadinstitute_hellbender_tools_walkers_filters_VariantFiltration.php)",
        "version": "4.1.0.0",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "13",
        "name": "star_align_2_5_3a_modified",
        "description": "**STAR** is an ultrafast universal RNA-seq aligner. \n\n**STAR** (Spliced Transcripts Alignment to a Reference), an ultrafast RNA-seq aligner, is capable of mapping full length RNA sequences and detecting de novo canonical junctions, non-canonical splices, and chimeric (fusion) transcripts. **STAR** employs an RNA-seq alignment algorithm that uses sequential maximum mappable seed search in uncompressed suffix arrays followed by seed clustering and stitching procedure. It is optimized for mammalian sequence reads, but fine tuning of its parameters enables customization to satisfy unique needs [1].\n\n**STAR** works with reads starting from lengths ~15 bases up to ~300 bases. In case of having longer reads, the use of **STAR Long** tool is recommended instead.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\nThe main purpose of **STAR** is to generate aligned BAM files (in genome and transcriptome coordinates) from RNA-seq data, which can later be used in further RNA studies, like gene expression analysis for example. \n\nSome important notes about this tool are: \n\n- The main input to the tool are **Reads** (`--readFilesIn`) in FASTQ format (single end or paired end), or unaligned BAM format.\n- **Genome files** in the form of a **STAR index archive**, outputted by the **STAR Genome Generate** tool, also need to be provided.\n- Its generally a good idea to always provide a GTF file to the inputs, if you want to get the **Transcriptome aligned reads** and **Reads per gene** outputs. \n- The main output of this tool is the **Aligned reads** output in coordinate sorted BAM format. The **Transcriptome aligned reads** BAM file is produced if the **Quantification mode** (`--quantMode`) parameter is set to **TranscriptomeSAM**. \n- Gene counts are produced if the **Quantification mode** (`--quantMode`) parameter is set to **GeneCounts**.  \n- **STAR** can detect chimeric transcripts, but the parameter **Min chimeric segment** (`--chimSegmentMin`) in *Chimeric Alignments*  category must be adjusted to a desired minimum chimeric segment length (12 is a good value, as recommended by the **STAR-Fusion** wiki). This output can later be used in **STAR-Fusion** for further fusion analysis. \n- If you want to use **STAR** results as an input to an RNA-seq differential expression analysis(using the **Cufflinks** app), set the parameter **Strand field flag** (`--outSAMstrandField`) to **intronMotif**.\n- Unmapped reads in FASTQ format are outputted on the **Unmapped reads** output if the **Output unmapped reads** (`--outReadsUnmapped`) parameter is set to the **Fastx** value. \n- Unmapped reads can be outputted within the main out BAM file on the **Aligned reads** and **Transcriptome aligned reads** outputs if the **Write unmapped in SAM** (`--outSAMunmapped`) parameter is set to **Within** or **Within KeepPairs**. \n- A basic **Two-pass mode** can be turned during the alignment step, which means that all the first pass junctions will be inserted into the genome indices for the second pass, by setting the **Two-pass mode** (`--twopassMode`) option to **Basic**. \n\nAdditionally, if you have long reads available and wish to map them with STAR, setting the **STARlong** option will run the **STARlong** algorithm instead, which uses a more efficient seed stitching algorithm for long reads (>200b), and also uses different array allocations. Selecting this boolean option will also automatically change the following parameters of STAR to comply with long read alignment best practices:\n`--outFilterMultimapScoreRange 20`\n`--outFilterScoreMinOverLread 0`\n`--outFilterMismatchNmax 1000`\n`--winAnchorMultimapNmax 200`\n`--seedSearchLmax 30`\n`--seedSearchStartLmax 12`\n`--seedPerReadNmax 100000`\n`--seedPerWindowNmax 100`\n`--alignTranscriptsPerReadNmax 100000`\n`--alignTranscriptsPerWindowNmax 10000`\n\n###Common issues###\n- For paired-end read files, it is important to properly set the **Paired-end** metadata field on your read files.\n- For FASTQ reads in multi-file format (i.e. two FASTQ files for paired-end 1 and two FASTQ files for paired-end2), the proper metadata needs to be set (the following hierarchy is valid: **Sample ID/Library ID/Platform Unit ID/File Segment Number**).\n\n### Changes Introduced by Seven Bridges\n\n- All output files will be prefixed by the input sample ID (inferred from the **Sample ID** metadata if existent, of from filename otherwise), unless the **Output file name prefix** option is explicitly specified.\n- **Unmapped reads** in FASTQ format are by default unsorted by read ID. This can induce problems if these files are used in subsequent analysis (i.e. downstream alignment). The option to sort unmapped reads by read ID is added to this wrapper, by setting the **Sort unmapped reads** parameter to True. The suffix for the **Unmapped reads** output can be controlled by the **Unmapped output file names** options (the default is *Unmapped*).\n- The tool can accept uncompressed FASTQ files, as well as GZ and BZ2 compressed FASTQ files, without the user having to specify anything. Also, if unaligned BAM files are used as inputs, the single-end/paired-end flag (SE/PE) needn't be specified - it will be inferred automatically using a built-in **Samtools** script. \n\n### Performance Benchmarking\n\nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes, with the following options in mind - unmapped reads are sorted by read id, output BAM is sorted by coordinate and basic two pass mode is turned on:\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost |  Instance (AWS) |\n|:---------------:|:-----------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     RNA-Seq     |  2 x 230 MB |     Yes    |     1M     |     101     |   18min   | $0.40 | c4.8xlarge |\n|     RNA-Seq     |  2 x 4.5 GB |     Yes    |     20M     |     101     |   30min   | $0.60 | c4.8xlarge |\n|     RNA-Seq     | 2 x 17.4 GB |     Yes    |     76M    |     101     |   64min  | $1.20 | c4.8xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [STAR paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3530905/)",
        "version": "2.5.3a_modified",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "14",
        "name": "star_genome_generate_2_5_3a_modified",
        "description": "**STAR Genome Generate** produces the necessary index files for successful **STAR** alignment, from an input FASTA and GTF files.  \n\n**STAR** (Spliced Transcripts Alignment to a Reference), an ultrafast RNA-seq aligner, is capable of mapping full length RNA sequences and detecting de novo canonical junctions, non-canonical splices, and chimeric (fusion) transcripts. **STAR** employs an RNA-seq alignment algorithm that uses sequential maximum mappable seed search in uncompressed suffix arrays followed by seed clustering and stitching procedure. It is optimized for mammalian sequence reads, but fine tuning of its parameters enables customization to satisfy unique needs [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\n**STAR Genome Generate** is a tool that generates genome index files. One set of files should be generated per each FASTA/GTF combination. Once produced, these files could be used as long as FASTA/GTF combination stays the same. Also, **STAR Genome Generate** which produced these files and **STAR** aligner using them must be of the same toolkit version.\n\n* If the indexes for a desired FASTA/GTF pair have already been generated, make sure to supply the resulting TAR bundle to the tool input if you are using this tool in a workflow in order to skip unnecessary indexing and speed up the whole workflow process.\n* If you are providing a GFF3 file (which are usually downloaded from **NCBI's RefSeq database**) and wish to use **STAR** results for further downstream analysis, a good idea would be to set the **Exon parent name** (`--sjdbGTFtagExonParentTranscript`) option to **Parent**.\n* If you wish to run **STAR** in **multiple samples two-pass mode**, you need to provide the resulting **splice junction** outputs from **STAR** to the **List of annotated junctions** (`--sjdbFileChrStartEnd`) input of **STAR Genome Generate**, and generate the index archive with these, instead of supplying the GTF file [2]. \n\n###Common issues###\n\n* If the reference genome used has a bit number of contig sequences (>5000), a suggestion is to set the **Bins size** (`--genomeChrBinNbits`) parameter to the value of min(18, log2(GenomeLength/NumberOfReferences)). \n* If the reference genome used is a rather small genome, a suggestion is to set the **Pre-indexing string length** (`--genomeSAindexNbases`) parameter to the value of min(14, log2(GenomeLength)/2 - 1). \n* If **STAR Genome Generates** for some reason fails because of insufficient RAM problem, the **Limit Genome Generate RAM** (`--limitGenomeGenerateRAM`) parameter can be increased to make the RAM demands, though since the default value is 60GB, this should only be happening with extremely large reference files (for example, 30GB is enough for the human reference genome). \n* The GTF and FASTA files need to have compatible transcript IDs and chromosome names.\n\n### Changes Introduced by Seven Bridges\n\n* The directory containing the index files will be outputted as a TAR bundle (the **Genome files** output). This bundle can then be provided to the **STAR** aligner, which will automatically take care of untarring it and preparing it to run successfully without further issues. \n\n### Performance Benchmarking\n\nSince **STAR Genome Generate** is run with a FASTA/GTF combination, the runtime of this tool will be pretty much constant across a number of different genomes. For the human reference genome, the tool is expected to finish in around 30 minutes, costing around $0.75 on the c4.8xlarge AWS instance. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [STAR paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3530905/)\n[2] [STAR manual](http://labshare.cshl.edu/shares/gingeraslab/www-data/dobin/STAR/STAR.sandbox/doc/STARmanual.pdf)",
        "version": "2.5.3a_modified",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      }
    ]
  },
  "execution_domain": {
    "script": [
      "https://cgc-api.sbgenomics.com/v2/apps/phil_webster/bco-cwl-examples/broad-best-practices-rna-seq-variant-calling-4-1-0-0/0/raw/"
    ],
    "script_driver": "Seven Bridges Common Workflow Language Executor",
    "software_prerequisites": [
      {
        "name": "Seven Bridges Platform",
        "version": "2023-01-23",
        "uri": [
          {
            "uri": "https://igor.sbgenomics.com/",
            "access_time": "2023-01-23",
            "sha1_chksum": ""
          }
        ]
      },
      {
        "name": "test_sb_new",
        "version": "2",
        "uri": [
          {
            "uri": "sbgenomics.com",
            "access_time": "2023-01-23",
            "sha1_chksum": ""
          }
        ]
      }
    ],
    "external_data_endpoints": [
      [
        {
          "name": "external",
          "url": "cgc.sbgenomics.com"
        }
      ]
    ],
    "environment_variables": {
      "fog": ""
    }
  },
  "parametric_domain": [
    {
      "param": "2",
      "value": "3",
      "step": "4"
    }
  ],
  "io_domain": {
    "input_subdomain": [
      {
        "uri": [
          {
            "filename": "",
            "uri": "",
            "access_time": ""
          }
        ]
      }
    ],
    "output_subdomain": [
      {
        "mediatype": "",
        "uri": [
          {
            "uri": "",
            "access_time": ""
          }
        ]
      }
    ]
  },
  "error_domain": {
    "empirical_error": {
      "1": "test"
    },
    "algorithmic_error": {
      "1": "test"
    }
  }
}
